# ğŸ¯ è¯­éŸ³AIç«¯åˆ°ç«¯æ¨¡å‹è®¨è®º è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **37 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [20:24-20:49]

**{Yeah, I think [going this direction] (actually) something (that's interesting) [that the Deepgram founder Scott was saying] is (actually) going the direction of [how can we, instead of obsessing over larger and larger models],} {[which gives like you know open AI and large labs] are willing â€” actually I'll just say large labs are willing to spend you know very large sums of money in order to do larger and larger models,} {but these ultimately are very costly and don't productionize well [for like very simple use cases].}**

æ˜¯çš„ï¼Œè¿™ä¸ªæ–¹å‘å¾ˆæœ‰æ„æ€ã€‚**Deepgram** åˆ›å§‹äºº **Scott** ä¹Ÿåœ¨è°ˆåŒä¸€ä¸ªæ–¹å‘ï¼šæˆ‘ä»¬ä¸åº”è¯¥ä¸€å‘³è¿½æ±‚è¶Šæ¥è¶Šå¤§çš„æ¨¡å‹â€”â€”è™½ç„¶å„å¤§å®éªŒå®¤æ„¿æ„ç ¸é‡é‡‘ä¸æ–­æ‰©å¤§è§„æ¨¡ï¼Œä½†è¿™äº›æ¨¡å‹æœ€ç»ˆéå¸¸æ˜‚è´µï¼Œåœ¨ç®€å•ä½¿ç”¨åœºæ™¯ä¸‹ä¹Ÿéš¾ä»¥è½åœ°é‡äº§ã€‚

è§£æï¼š
* **obsess over** ğŸ”¥ï¼šçŸ­è¯­ï¼Œç—´è¿·äºã€ä¸€å‘³è¿½æ±‚
* **large labs**ï¼šå¤§å‹å®éªŒå®¤ï¼ˆæ³›æŒ‡ OpenAIã€Google ç­‰å¤´éƒ¨ AI å…¬å¸ï¼‰
* **productionize** ğŸ”¥ï¼šåŠ¨è¯ï¼Œæ¨å‘ç”Ÿäº§ç¯å¢ƒã€è½åœ°é‡äº§
* **ultimately**ï¼šå‰¯è¯ï¼Œæœ€ç»ˆã€å½’æ ¹ç»“åº•

---

(2) [20:49-21:18]

**{And I think something [I'm excited about] in general is how can you create smaller and smaller models [that are very targeted for these verticals].} {There's much more of the 8020 rule [than there is in text],} {where you know booking an appointment accounts for just a massive number of use cases, or being able to handle a customer inquiry and get some user information [like what's your birthday, what's your social security or your credit card (or whatever you're trying to verify)].}**

æˆ‘çœŸæ­£å…´å¥‹çš„æ˜¯ï¼šå¦‚ä½•æ‰“é€ è¶Šæ¥è¶Šå°ã€é«˜åº¦å‚ç›´åŒ–çš„æ¨¡å‹ã€‚è¯­éŸ³é¢†åŸŸçš„äºŒå…«æ³•åˆ™æ¯”æ–‡æœ¬é¢†åŸŸæ›´æ˜æ˜¾â€”â€”é¢„çº¦ã€å®¢æˆ·å’¨è¯¢å’Œèº«ä»½éªŒè¯ï¼ˆç”Ÿæ—¥ã€ç¤¾ä¿å·ã€ä¿¡ç”¨å¡å·ç­‰ï¼‰è¿™äº›åœºæ™¯å°±å äº†ç»å¤§å¤šæ•°ä½¿ç”¨æ¡ˆä¾‹ã€‚

è§£æï¼š
* **verticals** ğŸ”¥ï¼šåè¯ï¼Œå‚ç›´è¡Œä¸šã€ç»†åˆ†é¢†åŸŸ
* **8020 rule**ï¼šäºŒå…«æ³•åˆ™ï¼Œ20% çš„åœºæ™¯è¦†ç›– 80% çš„éœ€æ±‚
* **account for** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œå ï¼ˆæ¯”ä¾‹ï¼‰
* **social security**ï¼šç¤¾ä¼šå®‰å…¨å·ï¼ˆç¾å›½èº«ä»½è¯ä»¶ï¼‰

---

(3) [21:18-21:38]

**{And these account for a really large portion of things [that you really want to nail every single time],} {and having smaller models [that are really targeted at doing customer service versus doing a podcast versus creating YouTube content] makes a lot of sense for audio.}**

è¿™äº›åœºæ™¯å äº†ä½ æ¯æ¬¡éƒ½å¿…é¡»åšå¥½çš„ç»å¤§éƒ¨åˆ†éœ€æ±‚ã€‚é’ˆå¯¹å®¢æœã€æ’­å®¢ã€YouTube å†…å®¹åˆ†åˆ«æ‰“é€ å°è€Œç²¾çš„ä¸“ç”¨æ¨¡å‹ï¼Œå¯¹è¯­éŸ³æ¥è¯´éå¸¸åˆç†ã€‚

è§£æï¼š
* **nail** ğŸ”¥ï¼šåŠ¨è¯ï¼ˆä¿šè¯­ï¼‰ï¼Œå®Œç¾å®Œæˆã€æ¯æ¬¡éƒ½åšå¯¹
* **every single time**ï¼šå¼ºè°ƒï¼Œæ¯ä¸€æ¬¡éƒ½
* **targeted at**ï¼šçŸ­è¯­ï¼Œä¸“é—¨é’ˆå¯¹ã€ä¸“ä¸ºâ€¦â€¦è®¾è®¡

---

(4) [21:38-22:08]

**{Going towards smaller models â€” I think that's the right direction,} {because the very large integrated multimodal model is powerful, but it's complex to imagine [that it is a viable business model] to put it into every interaction,} {right, because in some context like customer care, some phone calls are going to create a lot of value [like if you make a claim to an airline and you get reimbursed (or whatever)]} {â€” you know that creates a lot of value.}**

æœæ›´å°æ¨¡å‹çš„æ–¹å‘èµ°ï¼Œæˆ‘è®¤ä¸ºæ˜¯æ­£ç¡®çš„ã€‚å› ä¸ºå¤§å‹ä¸€ä½“åŒ–å¤šæ¨¡æ€æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†æŠŠå®ƒç”¨åœ¨æ¯ä¸€æ¬¡äº¤äº’ä¸Šï¼Œå¾ˆéš¾è¯´æ˜¯å¯è¡Œçš„å•†ä¸šæ¨¡å¼ã€‚æ¯”å¦‚å®¢æœåœºæ™¯ï¼Œæœ‰äº›ç”µè¯å¾ˆæœ‰ä»·å€¼â€”â€”æ¯”å¦‚ä½ å‘èˆªç©ºå…¬å¸ç´¢èµ”å¹¶è·å¾—èµ”å¿â€”â€”è¿™ç¡®å®åˆ›é€ äº†å¾ˆå¤§ä»·å€¼ã€‚

è§£æï¼š
* **integrated multimodal model**ï¼šä¸€ä½“åŒ–å¤šæ¨¡æ€æ¨¡å‹
* **viable** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œå¯è¡Œçš„ã€æœ‰ç”Ÿå‘½åŠ›çš„
* **make a claim**ï¼šæå‡ºç´¢èµ”
* **get reimbursed** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œè·å¾—æŠ¥é”€/èµ”å¿

---

(5) [22:08-22:25]

**{But when people just call to know how to reboot their computer or whatever, the value is extremely marginal.} {So you don't want to put a gigantic mixture of experts [that can solve international or mathematical olympiads] just to do that.}**

ä½†å½“äººä»¬åªæ˜¯æ‰“ç”µè¯é—®å¦‚ä½•é‡å¯ç”µè„‘ï¼Œä»·å€¼å°±æå…¶å¾®è–„ã€‚ä½ ä¸éœ€è¦ä¸ºæ­¤éƒ¨ç½²ä¸€ä¸ªèƒ½è§£å†³å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹é¢˜ç›®çš„åºå¤§æ··åˆä¸“å®¶æ¨¡å‹ã€‚

è§£æï¼š
* **marginal** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œå¾®ä¹å…¶å¾®çš„ã€è¾¹é™…æ€§çš„
* **mixture of experts (MoE)**ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆAI æ¶æ„æœ¯è¯­ï¼‰
* **gigantic**ï¼šå½¢å®¹è¯ï¼Œå·¨å¤§çš„

---

(6) [22:19-22:46]

**{So for voice we are rather trying to go towards miniaturization.} {So for example at Koutat last week we open-sourced **Pocket TTS**, which is like the first **TTS** [that can run on **CPU**].} {So it's not only on-device, it's on-device on **CPU**.} {We are now thinking about the best way to advertise it â€” and that's taking like very old smartphones and just having the thing running on it.}**

å¯¹è¯­éŸ³æ¥è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨æœå°å‹åŒ–æ–¹å‘åŠªåŠ›ã€‚æ¯”å¦‚ **Koutat** ä¸Šå‘¨å¼€æºäº† **Pocket TTS**â€”â€”è¿™æ˜¯é¦–ä¸ªå¯ä»¥åœ¨ **CPU** ä¸Šè¿è¡Œçš„è¯­éŸ³ **TTS** æ¨¡å‹ï¼Œä¸åªæ˜¯ç«¯ä¾§éƒ¨ç½²ï¼Œè€Œæ˜¯åœ¨ **CPU** ä¸Šè¿è¡Œã€‚æˆ‘ä»¬ç°åœ¨åœ¨æƒ³æ€ä¹ˆæ¨å¹¿å®ƒï¼Œè®¡åˆ’æ˜¯æ‹¿ä¸€äº›è€æ—§æ‰‹æœºç›´æ¥è·‘ç»™å¤§å®¶çœ‹ã€‚

è§£æï¼š
* **miniaturization** ğŸ”¥ï¼šåè¯ï¼Œå°å‹åŒ–
* **open-source**ï¼šåŠ¨è¯ï¼Œå¼€æºï¼ˆæ­¤å¤„ä½œåŠ¨è¯ç”¨ï¼‰
* **on-device**ï¼šå½¢å®¹è¯/å‰¯è¯ï¼Œç«¯ä¾§çš„ã€åœ¨è®¾å¤‡æœ¬åœ°è¿è¡Œ
* **CPU**ï¼šä¸­å¤®å¤„ç†å™¨ï¼ˆç›¸å¯¹äº GPU æ›´æ™®é€šã€ä½åŠŸè€—ï¼‰

---

(7) [22:46-23:14]

**{And this is going to have natural voice in a lot of contexts.} {Right now I think there is a big split [between the premium experience of voice and the low-end scalable experience of voice],} {and you can have like robotic but very affordable, or high quality but very expensive.} {Our goal is rather that in all use cases you can have access to human-like level of interaction and speech.}**

è¿™æ ·å°±èƒ½åœ¨å¾ˆå¤šåœºæ™¯ä¸‹å®ç°è‡ªç„¶çš„è¯­éŸ³ä½“éªŒã€‚ç›®å‰è¯­éŸ³é¢†åŸŸå­˜åœ¨æ˜æ˜¾åˆ†è£‚ï¼šè¦ä¹ˆæ˜‚è´µçš„é«˜ç«¯ä½“éªŒï¼Œè¦ä¹ˆå»‰ä»·ä½†æœºæ¢°çš„ä½ç«¯ä½“éªŒã€‚è€Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©æ‰€æœ‰åœºæ™¯éƒ½èƒ½è·å¾—æ¥è¿‘äººç±»æ°´å¹³çš„è¯­éŸ³äº¤äº’ã€‚

è§£æï¼š
* **premium**ï¼šå½¢å®¹è¯ï¼Œé«˜ç«¯çš„ã€ä¼˜è´¨çš„
* **scalable**ï¼šå½¢å®¹è¯ï¼Œå¯æ‰©å±•çš„ã€å¯è§„æ¨¡åŒ–çš„
* **robotic**ï¼šå½¢å®¹è¯ï¼Œæœºæ¢°çš„ã€åƒæœºå™¨äººçš„
* **human-like** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œæ¥è¿‘äººç±»çš„ã€ç±»äººçš„

---

(8) [23:14-23:44]

**{Yeah, I think especially because the voice models are so expensive, it's a really non-trivial expense to be running these models,} {and we're still seeing in production today [running up against the fact] [that it might not be cheaper than outsourcing this task somewhere].} {So that part is really difficult in terms of [how you guys are combining **LLMs** and everything [that we've learned from **TTS** tokenization]].}**

æ˜¯çš„ï¼Œè¯­éŸ³æ¨¡å‹éå¸¸æ˜‚è´µï¼Œè¿è¥æˆæœ¬ç»éå°æ•°ç›®ã€‚æˆ‘ä»¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä»ç„¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹ç”šè‡³ä¸ä¸€å®šæ¯”å¤–åŒ…è¿™é¡¹ä»»åŠ¡æ›´ä¾¿å®œã€‚æ‰€ä»¥å¦‚ä½•å°† **LLM** ä¸ **TTS** åˆ†è¯æŠ€æœ¯çš„ç»éªŒç›¸ç»“åˆï¼Œè¿™éƒ¨åˆ†ç¡®å®å¾ˆéš¾ã€‚

è§£æï¼š
* **non-trivial** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œä¸å¯å¿½è§†çš„ã€ç›¸å½“å¤§çš„ï¼ˆtrivial = å¾®ä¸è¶³é“çš„ï¼‰
* **run up against** ğŸ”¥ï¼šçŸ­è¯­åŠ¨è¯ï¼Œé­é‡ã€ç¢°åˆ°ï¼ˆéšœç¢/é—®é¢˜ï¼‰
* **outsourcing**ï¼šå¤–åŒ…
* **tokenization**ï¼šåˆ†è¯åŒ–ï¼ˆå°†è¯­éŸ³/æ–‡æœ¬åˆ‡åˆ†æˆ token çš„è¿‡ç¨‹ï¼‰

---

(9) [23:44-24:07]

**{What are some of the techniques [where you're improving speech-to-speech models] and what's kind of on the frontier?} {I know to a lot of people they just know [that speech-to-speech today maybe isn't there in terms of instruction following],} {but what does the kind of road map of advancements look like?}**

ä½ ä»¬åœ¨å“ªäº›æ–¹é¢æ”¹è¿›äº†è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ï¼Ÿå‰æ²¿åœ¨å“ªé‡Œï¼Ÿå¾ˆå¤šäººçŸ¥é“å½“å‰è¯­éŸ³åˆ°è¯­éŸ³åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢è¿˜ä¸å¤ªè¡Œï¼Œä½†å…·ä½“çš„è¿›æ­¥è·¯çº¿å›¾æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ

è§£æï¼š
* **on the frontier**ï¼šåœ¨å‰æ²¿ã€å¤„äºæœ€å…ˆè¿›é¢†åŸŸ
* **instruction following** ğŸ”¥ï¼šæŒ‡ä»¤éµå¾ªï¼ˆAI æ¨¡å‹æŒ‰ç…§ç”¨æˆ·æŒ‡ä»¤è¡Œäº‹çš„èƒ½åŠ›ï¼‰
* **road map of advancements**ï¼šè¿›æ­¥è·¯çº¿å›¾

---

(10) [24:07-24:38]

**{So I would say one of the biggest challenges is the intelligence gap [between the original text model and the speech-to-speech model].} {Then you can tackle this from a lot of angles.} {There is recycling as much as possible from the original architecture â€” you know, freezing some of it so [that you have a guarantee (that it is not going to change)], but changing enough so [that it can still learn to speak], or adding parameters in the right way.}**

æˆ‘è®¤ä¸ºæœ€å¤§çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯åŸå§‹æ–‡æœ¬æ¨¡å‹ä¸è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ä¹‹é—´çš„**æ™ºèƒ½å·®è·**ã€‚åº”å¯¹è¿™ä¸€é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦å…¥æ‰‹ï¼šå°½å¯èƒ½å¤ç”¨åŸå§‹æ¶æ„â€”â€”å†»ç»“éƒ¨åˆ†å‚æ•°ä»¥ä¿è¯ä¸å˜ï¼ŒåŒæ—¶è°ƒæ•´è¶³å¤Ÿå¤šçš„éƒ¨åˆ†è®©å®ƒå­¦ä¼šè¯´è¯ï¼Œæˆ–ä»¥æ­£ç¡®çš„æ–¹å¼æ·»åŠ æ–°å‚æ•°ã€‚

è§£æï¼š
* **intelligence gap** ğŸ”¥ï¼šæ™ºèƒ½å·®è·ï¼ˆæ–‡æœ¬æ¨¡å‹å’Œè¯­éŸ³æ¨¡å‹èƒ½åŠ›çš„è½å·®ï¼‰
* **tackle** ğŸ”¥ï¼šåŠ¨è¯ï¼Œç€æ‰‹è§£å†³ã€åº”å¯¹
* **recycling**ï¼šå¤ç”¨ã€é‡ç”¨ï¼ˆæ­¤å¤„æŒ‡é‡ç”¨æ–‡æœ¬æ¨¡å‹æ¶æ„ï¼‰
* **freezing parameters**ï¼šå†»ç»“å‚æ•°ï¼ˆè®­ç»ƒæ—¶ä¸æ›´æ–°æŸäº›å±‚çš„æƒé‡ï¼‰

---

(11) [24:38-25:05]

**{So you don't change your text model, but you add things [that turn it into a speech model], with guarantees [that you didn't forget anything].} {Data reinforcement learning â€” so it's really you can take it from all sides.} {And then I think there is a big part about it [that is rather about the kind of data you use to train this model].} {So for example **Moshi** when we released it in 2024, technically everything was in the architecture to have the richest emotional conversations â€” it didn't throw away anything from your voice.}**

ä½ ä¸æ”¹å˜æ–‡æœ¬æ¨¡å‹ï¼Œè€Œæ˜¯åœ¨ä¸Šé¢æ·»åŠ ç»„ä»¶ï¼Œä¿è¯ä¸ä¼šé—å¿˜åŸæœ‰çŸ¥è¯†ï¼Œä½¿å…¶å˜æˆè¯­éŸ³æ¨¡å‹ã€‚æ•°æ®å¼ºåŒ–å­¦ä¹ ä¹Ÿæ˜¯ä¸€ä¸ªæ–¹å‘â€”â€”æ€»çš„æ¥è¯´å¯ä»¥å…¨æ–¹ä½å…¥æ‰‹ã€‚å…¶ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†åœ¨äºç”¨ä»€ä¹ˆæ•°æ®æ¥è®­ç»ƒã€‚æ¯”å¦‚ **Moshi** åœ¨ 2024 å¹´å‘å¸ƒæ—¶ï¼Œæ¶æ„å±‚é¢å®Œå…¨å…·å¤‡äº†è¿›è¡Œæœ€ä¸°å¯Œæƒ…æ„Ÿå¯¹è¯çš„èƒ½åŠ›â€”â€”å®ƒä¸ä¼šä¸¢å¼ƒä½ å£°éŸ³ä¸­çš„ä»»ä½•ä¿¡æ¯ã€‚

è§£æï¼š
* **reinforcement learning**ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨¡å‹ï¼‰
* **throw away**ï¼šä¸¢å¼ƒã€æŠ›å¼ƒ
* **richest emotional conversations**ï¼šæœ€ä¸°å¯Œçš„æƒ…æ„Ÿå¯¹è¯

---

(12) [25:05-25:31]

**{So if you were being ironic or confused or bored or whatever, technically it could capture it.} {The reason why it didn't is [that it was not in its training data].} {So it was never trained to use this kind of what we call paralinguistic information â€” [which is everything [that is in what we say] but is not what we say] â€” so it's how we say things.}**

å¦‚æœä½ åœ¨è¯´åè¯ã€è¡¨ç°å›°æƒ‘æˆ–æ— èŠï¼Œæ¨¡å‹æŠ€æœ¯ä¸Šæ˜¯å¯ä»¥æ•æ‰åˆ°çš„ã€‚ä½†ä¹‹æ‰€ä»¥æ²¡æœ‰ï¼Œæ˜¯å› ä¸ºè®­ç»ƒæ•°æ®é‡Œæ²¡æœ‰è¿™äº›ã€‚æ¨¡å‹ä»æ¥æ²¡è¢«è®­ç»ƒå»ä½¿ç”¨æ‰€è°“çš„**å‰¯è¯­è¨€ä¿¡æ¯**â€”â€”å³æˆ‘ä»¬è¯´è¯æ–¹å¼ä¸­é™¤äº†å­—é¢æ„æ€ä¹‹å¤–çš„æ‰€æœ‰ä¿¡æ¯ï¼Œæ˜¯æˆ‘ä»¬**æ€ä¹ˆè¯´**è€Œé**è¯´ä»€ä¹ˆ**ã€‚

è§£æï¼š
* **ironic**ï¼šå½¢å®¹è¯ï¼Œè®½åˆºçš„ã€åè®½çš„
* **paralinguistic information** ğŸ”¥ï¼šå‰¯è¯­è¨€ä¿¡æ¯ï¼ˆè¯­è°ƒã€æƒ…ç»ªã€åœé¡¿ç­‰éå­—é¢ä¿¡æ¯ï¼‰
* **capture**ï¼šåŠ¨è¯ï¼Œæ•æ‰ã€æ„ŸçŸ¥

---

(13) [25:31-25:55]

**{It was never trained to do this kind of thing.} {And then it's really about the data and the way you do instruction tuning for **LLM** â€” and here it is exactly the same.} {You train your model â€” for example, you would ask the model: "describe my voice." And the model would have a ground truth [that will correspond to your demographic characteristics, your speaking style, and so on].} {And then the only way for the model to answer about that is to extract the relevant information.}**

å®ƒä»æ¥æ²¡è¢«è®­ç»ƒå»åšè¿™ç±»äº‹æƒ…ã€‚è¿™å°±åƒ **LLM** çš„æŒ‡ä»¤å¾®è°ƒä¸€æ ·â€”â€”å®Œå…¨ç›¸åŒçš„é€»è¾‘ã€‚ä½ è®­ç»ƒæ¨¡å‹ï¼šè®©å®ƒæè¿°ä½ çš„å£°éŸ³ï¼Œç„¶åæä¾›å¯¹åº”çš„æ ‡å‡†ç­”æ¡ˆï¼ˆåŒ…å«äººå£ç‰¹å¾ã€è¯´è¯é£æ ¼ç­‰ï¼‰ï¼Œæ¨¡å‹è¦å›ç­”è¿™ä¸ªé—®é¢˜å°±å¿…é¡»ä»è¯­éŸ³ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚

è§£æï¼š
* **instruction tuning** ğŸ”¥ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆé€šè¿‡æœ‰æ ‡æ³¨çš„é—®ç­”å¯¹æ¥ä¼˜åŒ– LLM çš„å…³é”®æŠ€æœ¯ï¼‰
* **ground truth**ï¼šæ ‡å‡†ç­”æ¡ˆã€çœŸå®æ ‡ç­¾ï¼ˆæœºå™¨å­¦ä¹ æœ¯è¯­ï¼‰
* **demographic characteristics**ï¼šäººå£ç‰¹å¾ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ç­‰ï¼‰
* **extract**ï¼šåŠ¨è¯ï¼Œæå–

---

(14) [25:55-26:27]

**{Otherwise it's just going to predict random tokens.} {And so by training the model to do all these kinds of tasks, it will be naturally able to find these cues in speech.} {So there are challenges on the architecture, and there are some really about what we call instrument data â€” [which is the kind of synthetic data (you specifically design) to teach specific characteristics and abilities to your model] â€” and both are necessary to tackle the issue.}**

å¦åˆ™æ¨¡å‹åªä¼šéšæœºé¢„æµ‹ tokenã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹å®Œæˆè¿™äº›ä»»åŠ¡ï¼Œå®ƒå°±èƒ½è‡ªç„¶åœ°ä»è¯­éŸ³ä¸­æ‰¾åˆ°çº¿ç´¢ã€‚æŒ‘æˆ˜æ¥è‡ªä¸¤æ–¹é¢ï¼šæ¶æ„è®¾è®¡ï¼Œä»¥åŠ**å·¥å…·æ•°æ®**â€”â€”å³ä¸“é—¨è®¾è®¡çš„åˆæˆæ•°æ®ï¼Œç”¨äºæ•™æ¨¡å‹å­¦ä¹ ç‰¹å®šç‰¹å¾å’Œèƒ½åŠ›ã€‚ä¸¤è€…ç›¸äº’ç‹¬ç«‹ä½†éƒ½ä¸å¯æˆ–ç¼ºã€‚

è§£æï¼š
* **predict random tokens**ï¼šéšæœºé¢„æµ‹ tokenï¼ˆæ¨¡å‹æ²¡æœ‰çœŸæ­£ç†è§£æ—¶çš„è¡Œä¸ºï¼‰
* **cues** ğŸ”¥ï¼šåè¯ï¼Œçº¿ç´¢ã€æç¤ºä¿¡å·
* **instrument data / synthetic data** ğŸ”¥ï¼šå·¥å…·æ•°æ®/åˆæˆæ•°æ®ï¼ˆäººå·¥è®¾è®¡çš„è®­ç»ƒæ•°æ®ï¼‰

---

(15) [26:27-27:00]

**{That's really exciting to hear â€” how it's not only just a data problem on the audio side but also bringing in how can we merge the learnings from a lot of these **LLMs**.} {I think that's one of the hard parts too â€” we still haven't solved a lot of the instruction following pieces at the **LLM** layer, and so â€” today I think we still see a lot, even with cascading architectures â€” how do you get an agent to follow the instructions continuously in order to get to the end of a task?}**

è¿™çœŸçš„å¾ˆä»¤äººå…´å¥‹â€”â€”ä¸ä»…ä»…æ˜¯éŸ³é¢‘æ•°æ®é—®é¢˜ï¼Œè¿˜åœ¨äºå¦‚ä½•èåˆ **LLM** çš„å­¦ä¹ æˆæœã€‚è¿™ä¹Ÿæ˜¯éš¾ç‚¹ä¹‹ä¸€ï¼š**LLM** å±‚é¢çš„æŒ‡ä»¤éµå¾ªé—®é¢˜è¿˜æ²¡å®Œå…¨è§£å†³ã€‚å³ä½¿ç”¨äº†çº§è”æ¶æ„ï¼Œå¦‚ä½•è®© agent æŒç»­éµå¾ªæŒ‡ä»¤ç›´åˆ°å®Œæˆä»»åŠ¡ï¼Œä¾ç„¶æ˜¯ä¸ªå¤§é—®é¢˜ã€‚

è§£æï¼š
* **merge the learnings** ğŸ”¥ï¼šèåˆå­¦åˆ°çš„ç»éªŒ/çŸ¥è¯†
* **cascading architectures** ğŸ”¥ï¼šçº§è”æ¶æ„ï¼ˆSTT â†’ LLM â†’ TTS çš„åˆ†é˜¶æ®µå¤„ç†æ–¹å¼ï¼‰
* **continuously**ï¼šå‰¯è¯ï¼ŒæŒç»­åœ°ã€å§‹ç»ˆåœ°

---

(16) [27:00-27:38]

**{And so that is really what unlocked voice, I think â€” was when **LLMs** came out and was actually at the level of reasoning capabilities [that you could autonomously navigate a task].} {But today there are still challenges even at the **LLM** layer.} {The speech-to-speech model will always be derived from a text model originally, so they will keep up with the text model.} {One thing [that is very interesting] for voice is â€” we are all fighting around latency of **STT**, **TTS** â€” but with agents solving more and more complex tasks, now in a lot of cases the bottleneck in latency is not the speech part, it's the **LLM** just producing a meaningful answer.}**

çœŸæ­£è®©è¯­éŸ³å®ç”¨çš„ï¼Œæ˜¯ **LLM** æ¨ç†èƒ½åŠ›è¾¾åˆ°äº†èƒ½è‡ªä¸»å®Œæˆä»»åŠ¡çš„æ°´å¹³ã€‚ä½†å³ä¾¿å¦‚æ­¤ï¼Œ**LLM** å±‚é¢ä»æœ‰æŒ‘æˆ˜ã€‚è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹å§‹ç»ˆåŸºäºæ–‡æœ¬æ¨¡å‹è¡ç”Ÿï¼Œå› æ­¤äºŒè€…ä¼šåŒæ­¥è¿›æ­¥ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç°åœ¨å»¶è¿Ÿç“¶é¢ˆå·²ä¸å†æ˜¯ **STT**/**TTS**ï¼Œè€Œæ˜¯ **LLM** ç”Ÿæˆæœ‰æ„ä¹‰ç­”æ¡ˆçš„é€Ÿåº¦ã€‚

è§£æï¼š
* **unlock**ï¼šåŠ¨è¯ï¼Œè§£é”ã€ä½¿æŸäº‹æˆä¸ºå¯èƒ½
* **autonomously navigate a task**ï¼šè‡ªä¸»å¯¼èˆª/å®Œæˆä¸€é¡¹ä»»åŠ¡
* **derived from**ï¼šè¡ç”Ÿè‡ªã€åŸºäº
* **bottleneck** ğŸ”¥ï¼šåè¯ï¼Œç“¶é¢ˆ

---

(17) [27:38-28:10]

**{And I think it's interesting because it creates challenges [to keep a natural conversation], despite the fact [that the **LLM** is going to have an arbitrary amount of time necessary to answer the user].} {So in some cases, if it's like one or two seconds, you can have fake keyboard clicks â€” [that works pretty well] because it sounds like you have a human on the other side of the phone.}**

è¿™å¾ˆæœ‰æ„æ€â€”â€”**LLM** éœ€è¦ä»»æ„é•¿çš„æ—¶é—´æ¥å›ç­”ï¼Œè¿™ç»™ç»´æŒè‡ªç„¶å¯¹è¯å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å¯¹äºä¸€ä¸¤ç§’çš„å»¶è¿Ÿï¼Œå¯ä»¥æ’­æ”¾è™šå‡çš„é”®ç›˜æ•²å‡»å£°â€”â€”å¬èµ·æ¥å°±åƒç”µè¯å¦ä¸€å¤´æœ‰ä¸ªçœŸäººåœ¨æ‰“å­—ï¼Œæ•ˆæœå‡ºå¥‡åœ°å¥½ã€‚

è§£æï¼š
* **arbitrary** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œä»»æ„çš„ã€ä¸ç¡®å®šçš„ï¼ˆè¿™é‡ŒæŒ‡å»¶è¿Ÿæ—¶é—´é•¿çŸ­ä¸å¯é¢„æµ‹ï¼‰
* **fake keyboard clicks** ğŸ”¥ï¼šè™šå‡é”®ç›˜ç‚¹å‡»éŸ³ï¼ˆç”¨æ¥æ©ç›– AI å¤„ç†å»¶è¿Ÿçš„å°æŠ€å·§ï¼‰
* **the other side of the phone**ï¼šç”µè¯çš„å¦ä¸€å¤´

---

(18) [28:10-28:50]

**{But what is more interesting is â€” eventually, if the reasoning is, let's say, 15 seconds long â€” you will also want your model to get the user through [what is happening in the background], so [that they are not wondering [whether the whole thing broke and they should hang up]].} {And I like this one because it's also about [how you can turn the thinking trace into something [that can be meaningfully inserted in the conversation]].}**

ä½†æ›´æœ‰è¶£çš„æ˜¯ï¼Œå¦‚æœæ¨ç†æ—¶é—´è¾¾åˆ° 15 ç§’ï¼Œä½ éœ€è¦è®©æ¨¡å‹å‘ŠçŸ¥ç”¨æˆ·åå°æ­£åœ¨å‘ç”Ÿä»€ä¹ˆï¼Œé˜²æ­¢ç”¨æˆ·ä»¥ä¸ºç³»ç»Ÿå´©æºƒè€ŒæŒ‚æ–­ç”µè¯ã€‚æˆ‘å–œæ¬¢è¿™ä¸ªæ–¹å‘ï¼Œå› ä¸ºå®ƒå…³ä¹å¦‚ä½•å°†æ¨¡å‹çš„**æ€è€ƒè½¨è¿¹**è½¬åŒ–ä¸ºå¯ä»¥è‡ªç„¶æ’å…¥å¯¹è¯çš„å†…å®¹ã€‚

è§£æï¼š
* **hang up**ï¼šåŠ¨è¯çŸ­è¯­ï¼ŒæŒ‚æ–­ç”µè¯
* **thinking trace** ğŸ”¥ï¼šæ€è€ƒè½¨è¿¹ã€æ¨ç†é“¾ï¼ˆæ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„è®°å½•ï¼‰
* **meaningfully inserted**ï¼šæœ‰æ„ä¹‰åœ°æ’å…¥ï¼ˆè€Œéç”Ÿç¡¬æ‰“æ–­ï¼‰

---

(19) [28:50-29:30]

**{So interestingly, even the mode of reasoning of the text models â€” [which is fine [when it's a text model] because you look at it, it's reasoning, you just do something else and then you come back] â€” how do you make it not annoying in a conversation?} {I think it's very interesting.} {And then the second thing [that is going to be critical] is: the bigger and the more expensive the text models are â€” I know now there is like a new one every few days â€” what is the most effective way of turning them into speech-to-speech models?} {And here we see that we are very inefficient.}**

æ–‡æœ¬æ¨¡å‹çš„æ¨ç†æ–¹å¼åœ¨æ–‡æœ¬åœºæ™¯æ²¡é—®é¢˜â€”â€”å› ä¸ºä½ çœ‹åˆ°å®ƒåœ¨æ€è€ƒï¼Œå¯ä»¥å»å¹²åˆ«çš„ï¼›ä½†åœ¨è¯­éŸ³å¯¹è¯ä¸­æ€ä¹ˆä¸è®©å®ƒæ˜¾å¾—çƒ¦äººï¼Ÿè¿™å¾ˆæœ‰è¶£ã€‚ç¬¬äºŒä¸ªå…³é”®é—®é¢˜æ˜¯ï¼šéšç€æ–‡æœ¬æ¨¡å‹è¶Šæ¥è¶Šå¤§ã€è¶Šæ¥è¶Šè´µï¼ˆç°åœ¨å‡ ä¹æ¯éš”å‡ å¤©å°±æœ‰æ–°æ¨¡å‹ï¼‰ï¼Œå¦‚ä½•æœ€æœ‰æ•ˆåœ°æŠŠå®ƒä»¬è½¬åŒ–ä¸ºè¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ï¼Ÿç›®å‰æˆ‘ä»¬çš„è½¬åŒ–æ–¹å¼éå¸¸ä½æ•ˆã€‚

è§£æï¼š
* **mode of reasoning**ï¼šæ¨ç†æ–¹å¼
* **annoying**ï¼šå½¢å®¹è¯ï¼Œçƒ¦äººçš„
* **critical**ï¼šå½¢å®¹è¯ï¼Œå…³é”®çš„ã€è‡³å…³é‡è¦çš„
* **inefficient**ï¼šå½¢å®¹è¯ï¼Œä½æ•ˆçš„

---

(20) [29:30-30:00]

**{So I did my PhD in a lab [that studies language acquisition in babies].} {So every weekend we received babies. We did experiments â€” you track their eyes.} {There are even people [who spent days in families just writing everything [that is said to a baby]] and they just computed the statistics.} {And at most, in the most favorable conditions, at 5 years old, a baby has heard less than 5,000 hours of speech.}**

æˆ‘è¯»åšå£«æ—¶åœ¨ä¸€ä¸ªç ”ç©¶å©´å„¿è¯­è¨€ä¹ å¾—çš„å®éªŒå®¤ã€‚æˆ‘ä»¬æ¯ä¸ªå‘¨æœ«éƒ½æ¥å¾…å©´å„¿åšå®éªŒâ€”â€”æ¯”å¦‚è¿½è¸ªä»–ä»¬çš„çœ¼ç›ã€‚ç”šè‡³æœ‰ç ”ç©¶è€…åœ¨å®¶åº­ä¸­å¾…äº†å¥½å‡ å¤©ï¼Œè®°å½•æ‰€æœ‰è¯´ç»™å©´å„¿å¬çš„è¯å¹¶åšç»Ÿè®¡åˆ†æã€‚ç»“è®ºæ˜¯ï¼šåœ¨æœ€ç†æƒ³çš„æ¡ä»¶ä¸‹ï¼Œä¸€ä¸ª 5 å²å­©å­å¬åˆ°çš„è¯­éŸ³ä¸è¶…è¿‡ 5000 å°æ—¶ã€‚

è§£æï¼š
* **language acquisition** ğŸ”¥ï¼šè¯­è¨€ä¹ å¾—ï¼ˆå­©å­å­¦ä¹ è¯­è¨€çš„è¿‡ç¨‹ï¼‰
* **track their eyes**ï¼šè¿½è¸ªçœ¼ç›ï¼ˆçœ¼åŠ¨è¿½è¸ªå®éªŒï¼‰
* **compute the statistics**ï¼šè®¡ç®—ç»Ÿè®¡æ•°æ®
* **favorable conditions**ï¼šæœ‰åˆ©æ¡ä»¶ã€æœ€ä½³æƒ…å†µ

---

(21) [30:00-30:26]

**{And you know they can speak, they are very resilient, they can learn new words and so on.} {5,000 hours â€” I mean, we train, everybody trains models on millions of hours â€” and so there is also the efficiency of human learning relative to machine learning.} {It's a known problem, but in speech it's really crazy, because in particular as humans we learn to speak way before we learn to read.}**

ä½†é‚£æ—¶ä»–ä»¬å·²ç»èƒ½è¯´è¯ã€å¾ˆæœ‰éŸ§æ€§ã€èƒ½å­¦æ–°è¯äº†ã€‚5000 å°æ—¶â€”â€”è€Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹åŠ¨è¾„ç”¨æ•°ç™¾ä¸‡å°æ—¶çš„æ•°æ®ã€‚äººç±»å­¦ä¹ ä¸æœºå™¨å­¦ä¹ çš„æ•ˆç‡å·®è·æ˜¯ä¸ªä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ï¼Œä½†åœ¨è¯­éŸ³é¢†åŸŸå°¤ä¸ºæƒŠäººâ€”â€”å› ä¸ºäººç±»å­¦è¯´è¯è¿œæ—©äºå­¦é˜…è¯»ã€‚

è§£æï¼š
* **resilient** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œæœ‰éŸ§æ€§çš„ã€é€‚åº”åŠ›å¼ºçš„
* **relative to**ï¼šç›¸å¯¹äºã€ä¸â€¦â€¦ç›¸æ¯”
* **way before**ï¼šè¿œè¿œæ—©äºï¼ˆway åœ¨å£è¯­ä¸­ä½œå¼ºè°ƒï¼‰

---

(22) [30:26-30:56]

**{So all the tactic of "I start from a text and then I learn to speak" â€” you don't even have it, right.} {So you learn everything â€” the semantics, the segmentation of words, the morphology of words and so on â€” in a few hundred hours, you already master it quite reasonably.} {So yeah, I mean this is very inspiring because we know there is a way to learn much more efficiently, and I think that's a very interesting research problem.}**

æ‰€ä»¥"å…ˆæœ‰æ–‡æœ¬å†å­¦è¯´è¯"è¿™ç§ç­–ç•¥æ ¹æœ¬ä¸å­˜åœ¨ã€‚æˆ‘ä»¬åœ¨å‡ ç™¾å°æ—¶å†…å°±ä¹ å¾—äº†è¯­ä¹‰ã€åˆ†è¯å’Œè¯å½¢å˜åŒ–ç­‰ä¸€åˆ‡ã€‚è¿™å¾ˆé¼“èˆäººå¿ƒâ€”â€”æˆ‘ä»¬çŸ¥é“å­˜åœ¨ä¸€ç§æ›´é«˜æ•ˆçš„å­¦ä¹ æ–¹å¼ï¼Œè¿™æ˜¯ä¸ªéå¸¸æœ‰æ„æ€çš„ç ”ç©¶é—®é¢˜ã€‚

è§£æï¼š
* **semantics**ï¼šè¯­ä¹‰å­¦ã€è¯­ä¹‰
* **segmentation of words**ï¼šåˆ†è¯
* **morphology** ğŸ”¥ï¼šè¯å½¢å­¦ï¼ˆç ”ç©¶è¯çš„æ„æˆå’Œå˜åŒ–å½¢å¼ï¼‰
* **master**ï¼šåŠ¨è¯ï¼ŒæŒæ¡ã€ç²¾é€š

---

(23) [30:56-31:20]

**{Yeah, and I think that's the most exciting piece too â€” there's so much more signal with voice [that it captures the expressiveness of everything [that you are trying to express as a human]], [that so much of that is lost in text], and to be able to recapture that and train that into models â€” yeah, is super exciting.}**

å¯¹æˆ‘æ¥è¯´ï¼Œæœ€ä»¤äººå…´å¥‹çš„ä¸€ç‚¹æ˜¯ï¼šå£°éŸ³åŒ…å«æ›´ä¸°å¯Œçš„ä¿¡å·ï¼Œèƒ½æ•æ‰äººç±»è¡¨è¾¾çš„æ‰€æœ‰æƒ…æ„Ÿï¼Œè€Œè¿™äº›åœ¨æ–‡å­—ä¸­å¤§é‡æµå¤±ã€‚èƒ½é‡æ–°æ•è·è¿™äº›ä¿¡æ¯å¹¶è®­ç»ƒè¿›æ¨¡å‹é‡Œâ€”â€”å¤ªæ¿€åŠ¨äººå¿ƒäº†ã€‚

è§£æï¼š
* **signal**ï¼šåè¯ï¼Œä¿¡å·ï¼ˆæ­¤å¤„æŒ‡å£°éŸ³ä¸­è•´å«çš„ä¿¡æ¯é‡ï¼‰
* **expressiveness** ğŸ”¥ï¼šåè¯ï¼Œè¡¨è¾¾åŠ›ã€è¡¨ç°åŠ›
* **recapture** ğŸ”¥ï¼šåŠ¨è¯ï¼Œé‡æ–°æ•è·ã€æ‰¾å›

---

(24) [31:01-31:32]

**{I mean even like my husband speaks Slovak, and even listening to Slovak you can hear like "oh that sentence was wrong" or I could complete the word, even though I can't speak, even though there's so much [that I don't understand].} {So that's super interesting in terms of â€” you guys have worked a lot on non-speech-to-speech models as well, you also have a **TTS** model and a speech-to-text model. What's the reasoning there? Why spend time on **TTS** and **STT** if the future is speech-to-speech models?}**

ç”šè‡³å°±åƒæˆ‘ä¸ˆå¤«è¯´æ–¯æ´›ä¼å…‹è¯­ï¼Œæˆ‘è™½ç„¶å¬ä¸æ‡‚ï¼Œå´èƒ½æ„Ÿè§‰åˆ°"è¿™å¥è¯ä¸å¯¹"æˆ–è€…é¢„æµ‹å‡ºä¸‹ä¸€ä¸ªè¯â€”â€”è¯­éŸ³å°±æ˜¯è¿™ä¹ˆç¥å¥‡ã€‚è¯è¯´ä½ ä»¬ä¹Ÿåœ¨éç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ä¸ŠæŠ•å…¥äº†å¾ˆå¤šï¼Œæ—¢æœ‰ **TTS** ä¹Ÿæœ‰ **STT**ã€‚å¦‚æœæœªæ¥æ˜¯è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ï¼Œä¸ºä»€ä¹ˆè¿˜è¦åœ¨è¿™ä¸Šé¢èŠ±æ—¶é—´ï¼Ÿ

è§£æï¼š
* **Slovak**ï¼šæ–¯æ´›ä¼å…‹è¯­
* **complete the word**ï¼šçŒœå‡º/è¡¥å…¨å•è¯
* **non-speech-to-speech models**ï¼šéç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ï¼ˆå³ TTS + STT çš„çº§è”æ–¹å¼ï¼‰

---

(25) [31:57-32:29]

**{Yeah, so we did **Moshi**, right, and it was very impressive, and for us in terms of scientific achievement it's still the thing [I'm the most proud of].} {But then **Alex** and I, we said: "okay, we could also train a streaming speech-to-text and streaming text-to-speech."} {So **Alex** suggested I do the speech-to-text and he does the text-to-speech, so we did like a two-person project.} {And at first we were like: "okay, just let's make something that is decent."}**

æˆ‘ä»¬åšäº† **Moshi**ï¼Œåœ¨ç§‘å­¦æˆå°±ä¸Šé‚£ä»ç„¶æ˜¯æˆ‘æœ€éª„å‚²çš„äº‹ã€‚ä½†åæ¥æˆ‘å’Œ **Alex** è¯´ï¼šå¯ä»¥é¡ºæ‰‹è®­ç»ƒä¸€ä¸ªæµå¼ **STT** å’Œæµå¼ **TTS**ã€‚**Alex** å»ºè®®æˆ‘åš **STT**ï¼Œä»–åš **TTS**ï¼Œå°±è¿™æ ·ä¿©äººå¯åŠ¨äº†é¡¹ç›®ã€‚æœ€åˆçš„æƒ³æ³•å¾ˆç®€å•ï¼šåšä¸ªè¿˜è¿‡å¾—å»çš„ä¸œè¥¿å°±è¡Œã€‚

è§£æï¼š
* **streaming STT/TTS**ï¼šæµå¼ STT/TTSï¼ˆå®æ—¶å¤„ç†ï¼Œä¸ç­‰å¾…å®Œæ•´è¾“å…¥ï¼‰
* **decent** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œè¿‡å¾—å»çš„ã€ä¸é”™çš„ï¼ˆå£è¯­ä¸­å¸¸è¡¨ç¤º"å¤Ÿå¥½äº†"ï¼‰

---

(26) [32:29-33:01]

**{We open source it and you know it's just for the community.} {But we have a very strong competitive mind.} {So I found the open **ASR** leaderboard and like yeah, I want to climb the leaderboard.} {We spent much more time than expected on it and we ended up with the best streaming models for **TTS** and speech-to-text.} {And we said: "what is a cool way of advertising that?"} {So at the time I didn't even know what a voice agent was â€” I was just thinking: "okay, we can make a **LLM** wrapper" â€” that was how I called it.}**

æˆ‘ä»¬æ‰“ç®—å¼€æºç»™ç¤¾åŒºï¼Œä½†æˆ‘ä»¬éƒ½æœ‰å¾ˆå¼ºçš„ç«äº‰å¿ƒã€‚æˆ‘å‘ç°äº† **ASR** å…¬å¼€æ’è¡Œæ¦œï¼Œäºæ˜¯ç›®æ ‡å˜æˆäº†å†²æ¦œâ€”â€”æœ€ç»ˆèŠ±çš„æ—¶é—´è¿œè¶…é¢„æœŸï¼Œç»“æœåšå‡ºäº†æœ€å¥½çš„æµå¼ **TTS** å’Œ **STT** æ¨¡å‹ã€‚å‘å¸ƒæ—¶æƒ³æ‰¾ä¸ªé…·ç‚«çš„æ¨å¹¿æ–¹å¼ï¼Œé‚£æ—¶æˆ‘ç”šè‡³ä¸çŸ¥é“"è¯­éŸ³ agent"è¿™ä¸ªæ¦‚å¿µï¼Œå°±å«å®ƒ"**LLM** å°è£…å™¨"ã€‚

è§£æï¼š
* **competitive mind** ğŸ”¥ï¼šç«äº‰æ„è¯†ã€å¥½å¼ºçš„å¿ƒ
* **ASR leaderboard**ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ’è¡Œæ¦œ
* **climb the leaderboard** ğŸ”¥ï¼šå†²æ¦œã€æå‡æ’å
* **wrapper**ï¼šåè¯ï¼Œå°è£…å™¨ï¼ˆåŒ…è£¹å¦ä¸€ä¸ªç»„ä»¶çš„å¤–å±‚ï¼‰

---

(27) [33:01-33:26]

**{So we take an **LLM**, we add the streaming stuff â€” transcription, synthesis â€” and people will have a cool website [where they can put a prompt for the personality, put a 10-second sample, and speak to a historical figure or to a president or to a cartoon character (or whatever)].} {And this attracted much more business traction than speech-to-speech, because it was usable day one â€” people were really looking for that.}**

æˆ‘ä»¬æŠŠ **LLM** åŠ ä¸Šæµå¼è½¬å†™å’Œåˆæˆï¼Œåšäº†ä¸€ä¸ªç½‘ç«™â€”â€”ç”¨æˆ·å¯ä»¥è®¾å®šè§’è‰²æç¤ºè¯ã€ä¸Šä¼  10 ç§’éŸ³é¢‘ï¼Œç„¶åå’Œå†å²äººç‰©ã€æ€»ç»Ÿæˆ–å¡é€šè§’è‰²å¯¹è¯ã€‚è¿™æ¯”è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹è·å¾—äº†æ›´å¤šå•†ä¸šå…³æ³¨ï¼Œå› ä¸ºç¬¬ä¸€å¤©å°±èƒ½ç”¨ï¼Œäººä»¬çœŸçš„å¾ˆéœ€è¦è¿™ä¸ªã€‚

è§£æï¼š
* **transcription**ï¼šè½¬å†™ï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰
* **synthesis**ï¼šåˆæˆï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰
* **traction** ğŸ”¥ï¼šåè¯ï¼Œå¸‚åœºå¸å¼•åŠ›ã€é‡‡ç”¨ç‡ï¼ˆå•†ä¸šæœ¯è¯­ï¼‰
* **day one** ğŸ”¥ï¼šçŸ­è¯­ï¼Œç¬¬ä¸€å¤©ã€ä¸Šçº¿å³å¯ç”¨

---

(28) [33:26-33:59]

**{And it was very easy for them to try because they would already have a **KKD** system, they could try independently replacing the **TTS** with our speech-to-text.} {So when we did it, it was for sure the shortest path to having people using our models.} {And then the challenge is two things â€” the two advantages of speech-to-speech: the first one is latency, and the second one is understanding the paralinguistic information.}**

è€Œä¸”ç”¨æˆ·å°è¯•èµ·æ¥å¾ˆå®¹æ˜“â€”â€”å·²æœ‰ **KKD** ç³»ç»Ÿçš„äººåªéœ€ç‹¬ç«‹æ›¿æ¢ **TTS** ç»„ä»¶å°±èƒ½è¯•ç”¨ã€‚è¿™æ¯«æ— ç–‘é—®æ˜¯è®©äººä»¬æœ€å¿«ç”¨ä¸Šæˆ‘ä»¬æ¨¡å‹çš„è·¯å¾„ã€‚æ¥ä¸‹æ¥çš„æŒ‘æˆ˜æ˜¯ä¸¤ä»¶äº‹â€”â€”è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹çš„ä¸¤å¤§ä¼˜åŠ¿ï¼šä¸€æ˜¯å»¶è¿Ÿï¼ŒäºŒæ˜¯ç†è§£å‰¯è¯­è¨€ä¿¡æ¯ã€‚

è§£æï¼š
* **shortest path** ğŸ”¥ï¼šæœ€çŸ­è·¯å¾„ã€æ·å¾„
* **independently**ï¼šç‹¬ç«‹åœ°ï¼Œä¸ä¾èµ–å…¶ä»–éƒ¨åˆ†
* **paralinguistic information**ï¼šå‰¯è¯­è¨€ä¿¡æ¯ï¼ˆåŒå‰ï¼‰

---

(29) [33:59-34:34]

**{And so we started with reducing the latency as much as possible, with the goal [that eventually I think you can have cascaded systems [that are overall below 500 milliseconds]].} {With that, it's already strong enough for having a good interaction, and it's so convenient for people [who build agents] that it's really worth it.} {But we're not giving up on speech-to-speech models, we keep doing research actively on it, and the road map is [that eventually we meet at the intersection of the advantages of both].}**

æˆ‘ä»¬ä»å°½é‡é™ä½å»¶è¿Ÿå…¥æ‰‹ï¼Œç›®æ ‡æ˜¯è®©çº§è”ç³»ç»Ÿæ•´ä½“å»¶è¿Ÿåšåˆ° 500 æ¯«ç§’ä»¥ä¸‹ã€‚è¾¾åˆ°è¿™ä¸ªæ°´å¹³ï¼Œäº¤äº’ä½“éªŒå·²ç»è¶³å¤Ÿå¥½ï¼Œå¯¹æ„å»º agent çš„äººæ¥è¯´éå¸¸æ–¹ä¾¿ï¼Œéå¸¸å€¼å¾—åšã€‚ä½†æˆ‘ä»¬æ²¡æœ‰æ”¾å¼ƒè¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹ï¼Œä»åœ¨ç§¯æç ”ç©¶â€”â€”è·¯çº¿å›¾æ˜¯æœ€ç»ˆåœ¨ä¸¤è€…çš„ä¼˜åŠ¿äº¤æ±‡å¤„ç›¸é‡ã€‚

è§£æï¼š
* **below 500 milliseconds**ï¼š500 æ¯«ç§’ä»¥ä¸‹ï¼ˆæ¥è¿‘äººç±»è‡ªç„¶å¯¹è¯å»¶è¿Ÿçš„é˜ˆå€¼ï¼‰
* **intersection** ğŸ”¥ï¼šåè¯ï¼Œäº¤æ±‡å¤„ã€èåˆç‚¹

---

(30) [34:34-35:00]

**{Definitely. I think what you said is interesting around the modularity â€” people are much more willing to swap out pieces of their system than the entire system.} {Why do you think that is? Is that because the logic [they have for the cascading system] is very complex?} {I would say first of all [what you can see] is there are few companies [that provide both a strong speech-to-text and text-to-speech, not even mentioning speech-to-speech].}**

ä½ æåˆ°çš„æ¨¡å—åŒ–å¾ˆæœ‰æ„æ€â€”â€”äººä»¬æ›´æ„¿æ„æ›¿æ¢ç³»ç»Ÿçš„æŸä¸ªéƒ¨ä»¶è€Œä¸æ˜¯æ•´ä½“æ›¿æ¢ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿæ˜¯å› ä¸ºçº§è”ç³»ç»Ÿçš„é€»è¾‘å¤ªå¤æ‚äº†å—ï¼Ÿé¦–å…ˆï¼Œèƒ½åŒæ—¶æä¾›å¼ºå¤§ **STT** å’Œ **TTS** çš„å…¬å¸æœ¬å°±ä¸å¤šï¼Œæ›´ä¸ç”¨è¯´è¯­éŸ³åˆ°è¯­éŸ³äº†ã€‚

è§£æï¼š
* **modularity** ğŸ”¥ï¼šæ¨¡å—åŒ–ï¼ˆç³»ç»Ÿå„éƒ¨åˆ†ç‹¬ç«‹å¯æ›¿æ¢çš„ç‰¹æ€§ï¼‰
* **swap out** ğŸ”¥ï¼šçŸ­è¯­åŠ¨è¯ï¼Œæ›¿æ¢ã€æ¢æ‰
* **not even mentioning**ï¼šæ›´ä¸ç”¨è¯´ã€ç”šè‡³æ²¡æœ‰æåˆ°

---

(31) [35:00-35:51]

**{So the nice thing about modularity is â€” typically people would have **TTS** from provider A, **STT** from provider B, and **LLM** from provider C, so in a way it just gives you much more options to combine the strength of each provider.} {But I think the only way of making it compelling is â€” it's really on us â€” to make it have all the features of cascaded systems plus naturalness.} {Otherwise, I don't see why people would bother doing that, right? And I think the one very nice thing about speech-to-speech eventually is [that this will be able to be much faster but also cheaper] because there is an additional compute cost [to go back and forth to text].}**

æ¨¡å—åŒ–çš„å¥½å¤„æ˜¯ï¼šç”¨æˆ·å¯ä»¥ **TTS** ç”¨ A å®¶ï¼Œ**STT** ç”¨ B å®¶ï¼Œ**LLM** ç”¨ C å®¶ï¼Œè‡ªç”±ç»„åˆå„å®¶ä¼˜åŠ¿ã€‚ä½†è¦è®©è¯­éŸ³åˆ°è¯­éŸ³çœŸæ­£æœ‰å¸å¼•åŠ›ï¼Œè´£ä»»åœ¨æˆ‘ä»¬â€”â€”å¿…é¡»åšåˆ°å…·å¤‡çº§è”ç³»ç»Ÿçš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒæ—¶è¿˜æ›´è‡ªç„¶ã€‚å¦åˆ™æ²¡äººä¼šè´¹é‚£ä¸ªåŠ²ã€‚è€Œè¯­éŸ³åˆ°è¯­éŸ³æœ€ç»ˆçš„ä¸€å¤§ä¼˜åŠ¿æ˜¯æ›´å¿«ä¹Ÿæ›´ä¾¿å®œâ€”â€”å› ä¸ºæ¥å›è½¬æˆæ–‡æœ¬æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚

è§£æï¼š
* **compelling** ğŸ”¥ï¼šå½¢å®¹è¯ï¼Œæœ‰å¸å¼•åŠ›çš„ã€ä»¤äººä¿¡æœçš„
* **on us**ï¼šçŸ­è¯­ï¼Œæ˜¯æˆ‘ä»¬çš„è´£ä»»
* **bother**ï¼šåŠ¨è¯ï¼Œè´¹å¿ƒã€è´¹åŠ²
* **back and forth**ï¼šæ¥å›

---

(32) [35:51-36:12]

**{So for example on the Koutat side, speech-to-speech translation â€” with a model [that can run on a phone] you can do real-time speech-to-speech translation. It was called **EB**, it's open source.} {If you had a 1 billion parameter budget for a speech-to-text + text translation model + **TTS**, it's ridiculous â€” it just would not work at all.} {So eventually when you integrate everything, you can compress everything [because there is a lot of redundancy [between the speech-to-text, the **LLM**, and the text-to-speech]], and you can remove this redundancy if you train the thing end-to-end.}**

**Koutat** é‚£è¾¹æˆ‘ä»¬åœ¨åšè¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼Œä¸€ä¸ªå¯ä»¥åœ¨æ‰‹æœºä¸Šè¿è¡Œçš„æ¨¡å‹å°±èƒ½å®ç°å®æ—¶ç¿»è¯‘ï¼Œé¡¹ç›®å« **EB**ï¼Œå·²å¼€æºã€‚å¦‚æœç”¨ 10 äº¿å‚æ•°çš„é¢„ç®—åš **STT** + ç¿»è¯‘ + **TTS**ï¼Œæ ¹æœ¬å°±è·‘ä¸é€šã€‚ä¸€æ—¦æŠŠæ‰€æœ‰ç»„ä»¶æ•´åˆåœ¨ä¸€èµ·ï¼Œå°±å¯ä»¥å‹ç¼©å†—ä½™â€”â€”**STT**ã€**LLM**ã€**TTS** ä¹‹é—´æœ‰å¤§é‡é‡å ï¼Œç«¯åˆ°ç«¯è®­ç»ƒå¯ä»¥æ¶ˆé™¤è¿™äº›å†—ä½™ã€‚

è§£æï¼š
* **real-time**ï¼šå®æ—¶çš„
* **parameter budget**ï¼šå‚æ•°é¢„ç®—ï¼ˆæ¨¡å‹å…è®¸ä½¿ç”¨çš„å‚æ•°æ€»é‡ï¼‰
* **redundancy** ğŸ”¥ï¼šåè¯ï¼Œå†—ä½™
* **end-to-end** ğŸ”¥ï¼šç«¯åˆ°ç«¯ï¼ˆç›´æ¥ä¼˜åŒ–æœ€ç»ˆç›®æ ‡ï¼Œä¸ç»è¿‡ä¸­é—´ç¦»æ•£æ­¥éª¤ï¼‰

---

(33) [36:10-36:48]

**{I guess going back to training audio or speech-to-speech models â€” I'm curious, something [that I've heard] is [that when you train an audio model, a lot of the information is lost from text models, and people don't really know exactly why]. Is that true?} {Yeah, so that's what I call the intelligence gap.} {It's really about the fact [that you have a model [that was very well trained to remember everything, acquire knowledge, and be able to reason]], and now it has to learn a lot of irrelevant stuff regarding knowledge.}**

å›åˆ°è®­ç»ƒéŸ³é¢‘æ¨¡å‹â€”â€”æˆ‘å¬è¯´è®­ç»ƒéŸ³é¢‘æ¨¡å‹æ—¶ä¼šæŸå¤±å¾ˆå¤šæ–‡æœ¬æ¨¡å‹çš„ä¿¡æ¯ï¼Œä½†å¤§å®¶ä¹Ÿä¸æ¸…æ¥šåŸå› ï¼Œè¿™æ˜¯çœŸçš„å—ï¼Ÿè¿™å°±æ˜¯æˆ‘è¯´çš„æ™ºèƒ½å·®è·ã€‚æœ¬æ¥æ¨¡å‹è®­ç»ƒå¾—å¾ˆå¥½â€”â€”èƒ½è®°ä½ä¸€åˆ‡ã€ä¹ å¾—çŸ¥è¯†ã€ä¼šæ¨ç†â€”â€”ä½†ç°åœ¨å®ƒå¿…é¡»å»å­¦å¤§é‡ä¸çŸ¥è¯†æ— å…³çš„ä¸œè¥¿ã€‚

è§£æï¼š
* **information loss**ï¼šä¿¡æ¯æŸå¤±
* **intelligence gap**ï¼šæ™ºèƒ½å·®è·ï¼ˆåŒå‰ï¼‰
* **acquire knowledge**ï¼šä¹ å¾—çŸ¥è¯†

---

(34) [36:48-37:31]

**{[Which are conversational dynamics, reproducing the right voice] â€” even you know [when you have a recording, it comes with specific recording conditions], the model is going to try to control perfectly the reverberation (of whatever sample you're giving it).} {So it's going to waste a lot of capacity on things [that are irrelevant to your task].} {So it's our job to give them the right supervision â€” like "okay, this is important, this is noise, you shouldn't care about that."} {So it's really about designing the right loss function and the right data to steer them towards what you want.}**

è¿™äº›æ— å…³çš„ä¸œè¥¿åŒ…æ‹¬å¯¹è¯åŠ¨æ€ã€å¤ç°æ­£ç¡®çš„éŸ³è‰²â€”â€”ç”šè‡³è¿å½•éŸ³æ—¶ç‰¹å®šçš„æ··å“æ¡ä»¶å®ƒéƒ½ä¼šå°è¯•å®Œç¾è¿˜åŸã€‚è¿™ä¼šæµªè´¹å¤§é‡èƒ½åŠ›åœ¨ä¸ä»»åŠ¡æ— å…³çš„äº‹ä¸Šã€‚æ‰€ä»¥æˆ‘ä»¬çš„å·¥ä½œæ˜¯ç»™æ¨¡å‹æ­£ç¡®çš„ç›‘ç£â€”â€”å‘Šè¯‰å®ƒ"è¿™ä¸ªé‡è¦ï¼Œé‚£ä¸ªæ˜¯å™ªå£°ï¼Œä¸ç”¨ç®¡"ã€‚å…³é”®åœ¨äºè®¾è®¡æ­£ç¡®çš„æŸå¤±å‡½æ•°å’Œæ•°æ®ï¼Œå¼•å¯¼æ¨¡å‹æœä½ å¸Œæœ›çš„æ–¹å‘èµ°ã€‚

è§£æï¼š
* **conversational dynamics**ï¼šå¯¹è¯åŠ¨æ€ï¼ˆå¯¹è¯ä¸­çš„èŠ‚å¥ã€è½®æ¬¡ç­‰ï¼‰
* **reverberation** ğŸ”¥ï¼šåè¯ï¼Œæ··å“ï¼ˆå£°éŸ³åœ¨ç©ºé—´ä¸­åå°„äº§ç”Ÿçš„å›å£°æ•ˆæœï¼‰
* **loss function** ğŸ”¥ï¼šæŸå¤±å‡½æ•°ï¼ˆè¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼å·®è·çš„å‡½æ•°ï¼‰
* **steer towards** ğŸ”¥ï¼šå¼•å¯¼æœå‘

---

(35) [37:31-38:02]

**{Because what made me completely vaccinated against doomerism is doing instruction tuning from **LLM** â€” because then you realize your models are just doing exactly what they were tuned for, and nothing much more, and nothing less.} {So that's why the job on how we supervise them is gigantic â€” that's what is called post-training today. It's most of the staff and resources and time of people [who train models].}**

åš **LLM** æŒ‡ä»¤å¾®è°ƒè®©æˆ‘å½»åº•æ‘†è„±äº†"AI æœ«æ—¥è®º"â€”â€”å› ä¸ºä½ ä¼šæ„è¯†åˆ°æ¨¡å‹å°±æ˜¯ä¸¥æ ¼æŒ‰è®­ç»ƒç›®æ ‡åšäº‹ï¼Œä¸å¤šä¹Ÿä¸å°‘ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¦‚ä½•ç›‘ç£æ¨¡å‹æ˜¯ä¸€é¡¹å·¨å¤§çš„å·¥ç¨‹â€”â€”è¿™å°±æ˜¯ä»Šå¤©æ‰€è¯´çš„**åè®­ç»ƒï¼ˆpost-trainingï¼‰**ï¼Œå æ®äº†ç»å¤§éƒ¨åˆ†äººåŠ›ã€èµ„æºå’Œæ—¶é—´ã€‚

è§£æï¼š
* **vaccinated against doomerism** ğŸ”¥ï¼šå¯¹"AI æœ«æ—¥è®º"å…ç–«ï¼ˆç”ŸåŠ¨æ¯”å–»ï¼Œdoomerism = å¯¹ AI å¤±æ§çš„æœ«æ—¥ææƒ§ï¼‰
* **tuned for**ï¼šä¸ºâ€¦â€¦è€Œè°ƒä¼˜
* **post-training** ğŸ”¥ï¼šåè®­ç»ƒï¼ˆé¢„è®­ç»ƒä¹‹åçš„å¾®è°ƒã€å¯¹é½é˜¶æ®µï¼Œç°ä»£ AI ä¸­æœ€é‡è¦çš„ç¯èŠ‚ä¹‹ä¸€ï¼‰

---

(36) [38:02-39:10]

**{Yeah. Making them not distracted by all the minutia of the world.} {Honestly, all these things about speech-to-speech models just sound like engineers â€” when they know the answer, they'll interrupt you with the answer.} {I think in particular with full duplex, right now there are two big limitations to making it very enjoyable. The first one is the models are very disciplined in the sense [that it can be boring] â€” they politely wait for their turn.} {And at the same time as a human you need to be very disciplined â€” you talk at the right pace, you avoid long pauses so [that it doesn't think you're done and start speaking].}**

è®©å®ƒä»¬ä¸è¢«ä¸–ç•Œä¸Šçš„çäº‹åˆ†å¿ƒã€‚è¯´å®è¯ï¼Œè¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹çš„è¿™äº›é—®é¢˜å¬èµ·æ¥å°±åƒå·¥ç¨‹å¸ˆâ€”â€”ä»–ä»¬çŸ¥é“ç­”æ¡ˆäº†å°±ä¼šæ‰“æ–­ä½ ã€‚å…¨åŒå·¥æ¨¡å¼ä¸‹ï¼Œç›®å‰æœ‰ä¸¤å¤§é™åˆ¶ï¼šç¬¬ä¸€ï¼Œæ¨¡å‹å¤ªå®ˆè§„çŸ©äº†â€”â€”å®ƒä»¬ç¤¼è²Œåœ°ç­‰å¾…è½®æ¬¡ï¼Œæ˜¾å¾—æœ‰äº›æ— èŠã€‚åŒæ—¶äººç±»ä¹Ÿéœ€è¦å¾ˆå®ˆè§„çŸ©â€”â€”è¯­é€Ÿåˆé€‚ï¼Œé¿å…é•¿åœé¡¿ï¼Œå¦åˆ™æ¨¡å‹ä»¥ä¸ºä½ è¯´å®Œäº†å°±å¼€å§‹è®²ã€‚

è§£æï¼š
* **minutia**ï¼šåè¯ï¼Œçç¢ç»†èŠ‚ï¼ˆå¤æ•° minutiaeï¼‰
* **full duplex** ğŸ”¥ï¼šå…¨åŒå·¥ï¼ˆåŒæ–¹å¯ä»¥åŒæ—¶è¯´è¯ï¼Œç±»ä¼¼äººç±»è‡ªç„¶å¯¹è¯ï¼‰
* **disciplined**ï¼šå½¢å®¹è¯ï¼Œæœ‰çºªå¾‹çš„ã€å®ˆè§„çŸ©çš„
* **turn-taking**ï¼šè½®æµè¯´è¯ï¼ˆå¯¹è¯ä¸­çš„è½®æ¬¡åˆ‡æ¢ï¼‰

---

(37) [39:10-41:11]

**{So as we fix all of that and allow for arbitrary conditions â€” the model just understanding [that "okay, this person is thinking right now, I should just shut up and wait"] â€” this is going to be much more fun.} {And I think honestly the best way of showcasing that will be the first contrarian [that just interrupts you and says "no no no, I disagree fundamentally"] and explains why you're wrong â€” it will create value because it creates new use cases [where you want to confront an idea (you're not sure about) with someone [who plays devil's advocate]].} {And that's what we tried doing with **Moshi** but it was too early.} {Now we will push it to a point [where it's going to be really, really fun].} {So where I see voice AI going this year â€” cascaded systems still have a good time to go; expressivity and contextualization of the expression; being able to handle the uncertainty of latency; miniaturization of models; and what I would love is releasing speech-to-speech models as the standard solution, because that's going to give the best experience.}**

ä¸€æ—¦è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œè®©æ¨¡å‹ç†è§£"è¿™ä¸ªäººåœ¨æ€è€ƒï¼Œæˆ‘åº”è¯¥é—­å˜´ç­‰ä»–è¯´å®Œ"ï¼Œå¯¹è¯å°±ä¼šæœ‰è¶£å¾—å¤šã€‚æˆ‘è®¤ä¸ºæœ€å¥½çš„å±•ç¤ºæ–¹å¼å°†æ˜¯ç¬¬ä¸€ä¸ª**åé©³å‹** agentâ€”â€”å®ƒä¼šæ‰“æ–­ä½ è¯´"ä¸ä¸ä¸ï¼Œæˆ‘å®Œå…¨ä¸åŒæ„"ï¼Œç„¶åè§£é‡Šä½ å“ªé‡Œé”™äº†ã€‚è¿™ä¼šåˆ›é€ æ–°ä»·å€¼ï¼Œäº§ç”Ÿ"æˆ‘æƒ³æ‰¾äººå”±åè°ƒæ¥æ£€éªŒæƒ³æ³•"çš„æ–°åœºæ™¯ã€‚æˆ‘ä»¬åœ¨ **Moshi** ä¸Šè¯•è¿‡ä½†å¤ªæ—©äº†ï¼Œç°åœ¨ä¼šæŠŠå®ƒæ¨åˆ°çœŸæ­£å¥½ç©çš„ç¨‹åº¦ã€‚å¯¹ä»Šå¹´è¯­éŸ³ AI çš„é¢„æµ‹ï¼šçº§è”ç³»ç»Ÿè¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ï¼›è¡¨ç°åŠ›ä¸æƒ…å¢ƒåŒ–ï¼›åº”å¯¹ **LLM** å¸¦æ¥çš„å»¶è¿Ÿä¸ç¡®å®šæ€§ï¼›æ¨¡å‹å°å‹åŒ–ï¼›è€Œæˆ‘æœ€æƒ³å®ç°çš„æ˜¯è®©è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æˆä¸ºæ ‡å‡†è§£å†³æ–¹æ¡ˆâ€”â€”é‚£æ‰æ˜¯æœ€å¥½çš„ä½“éªŒã€‚

è§£æï¼š
* **contrarian** ğŸ”¥ï¼šåè¯/å½¢å®¹è¯ï¼Œå”±åè°ƒçš„äºº/æŒåå¯¹æ„è§çš„
* **devil's advocate** ğŸ”¥ï¼šé­”é¬¼ä»£è¨€äººï¼ˆä¸“é—¨æå‡ºç›¸åæ„è§æ¥æ£€éªŒè®ºç‚¹çš„è§’è‰²ï¼‰
* **confront an idea**ï¼šæ£€éªŒã€æŒ‘æˆ˜ä¸€ä¸ªæƒ³æ³•
* **expressivity and contextualization** ğŸ”¥ï¼šè¡¨è¾¾åŠ›ä¸æƒ…å¢ƒåŒ–ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´è¡¨è¾¾æ–¹å¼ï¼‰
* **standard solution**ï¼šæ ‡å‡†è§£å†³æ–¹æ¡ˆ

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ®µæ·±åº¦è®¿è°ˆæ¥è‡ªè¯­éŸ³ AI é¢†åŸŸï¼Œå˜‰å®¾æ˜¯ **Moshi**/**Koutat** å›¢é˜Ÿç ”ç©¶è€…ï¼Œè®¨è®ºäº†ä¸‰å¤§æ ¸å¿ƒä¸»é¢˜ï¼šâ‘  è¯­éŸ³æ¨¡å‹çš„å°å‹åŒ–è¶‹åŠ¿ï¼ˆå‚ç›´åœºæ™¯ + **Pocket TTS**ï¼‰ï¼›â‘¡ è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹çš„æŠ€æœ¯æŒ‘æˆ˜ï¼ˆæ™ºèƒ½å·®è·ã€å‰¯è¯­è¨€ä¿¡æ¯ã€åè®­ç»ƒçš„é‡è¦æ€§ï¼‰ï¼›â‘¢ çº§è”ç³»ç»Ÿ vs. ç«¯åˆ°ç«¯æ¨¡å‹çš„ç°å®æƒè¡¡ã€‚æ ¸å¿ƒæ´è§æ˜¯ï¼šå©´å„¿ 5000 å°æ—¶ä¹ å¾—è¯­è¨€ vs. æ¨¡å‹ç™¾ä¸‡å°æ—¶è®­ç»ƒï¼Œè¯´æ˜å­˜åœ¨æ›´é«˜æ•ˆçš„å­¦ä¹ æ–¹å¼ï¼›è€Œåè®­ç»ƒï¼ˆpost-trainingï¼‰æ˜¯å½“å‰ AI è®­ç»ƒä¸­æœ€å…³é”®ä¹Ÿæœ€è€—èµ„æºçš„ç¯èŠ‚ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **productionize** | æ¨å‘ç”Ÿäº§ç¯å¢ƒã€è½åœ°é‡äº§ |
| **verticals** | å‚ç›´è¡Œä¸šã€ç»†åˆ†é¢†åŸŸ |
| **nail** | å®Œç¾æå®šï¼ˆå£è¯­ï¼‰ |
| **miniaturization** | å°å‹åŒ– |
| **paralinguistic information** | å‰¯è¯­è¨€ä¿¡æ¯ï¼ˆè¯­è°ƒã€æƒ…ç»ªç­‰éå­—é¢ä¿¡æ¯ï¼‰ |
| **intelligence gap** | æ™ºèƒ½å·®è·ï¼ˆæ–‡æœ¬æ¨¡å‹ vs. è¯­éŸ³æ¨¡å‹ï¼‰ |
| **synthetic / instrument data** | åˆæˆæ•°æ® |
| **cascading architecture** | çº§è”æ¶æ„ï¼ˆSTT + LLM + TTSï¼‰ |
| **post-training** | åè®­ç»ƒï¼ˆé¢„è®­ç»ƒåçš„å¾®è°ƒ/å¯¹é½é˜¶æ®µï¼‰ |
| **full duplex** | å…¨åŒå·¥ï¼ˆåŒæ–¹å¯åŒæ—¶è¯´è¯ï¼‰ |
| **contrarian** | å”±åè°ƒçš„ã€æŒåå¯¹æ„è§çš„ |
| **reverberation** | æ··å“ |
| **traction** | å¸‚åœºå¸å¼•åŠ›ã€é‡‡ç”¨ç‡ |
| **compelling** | æœ‰å¸å¼•åŠ›çš„ã€ä»¤äººä¿¡æœçš„ |
| **resilient** | æœ‰éŸ§æ€§çš„ã€é€‚åº”åŠ›å¼ºçš„ |
| **language acquisition** | è¯­è¨€ä¹ å¾— |
| **loss function** | æŸå¤±å‡½æ•° |
| **bottleneck** | ç“¶é¢ˆ |
| **arbitrary** | ä»»æ„çš„ï¼ˆæ­¤å¤„æŒ‡ä¸ç¡®å®šé•¿çŸ­çš„å»¶è¿Ÿï¼‰ |
| **steer towards** | å¼•å¯¼æœå‘ |
