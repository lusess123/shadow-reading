# ğŸ¯ è¯­éŸ³AIæŒ‡ä»¤éµå¾ªä¸åŸºå‡†æµ‹è¯•åœ†æ¡Œè®¨è®º è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **33 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:00-0:24] **Hey guys, it's great to have you guys here. Great, super excited that we made this happen. Today we're going to be talking through instruction following and benchmarks around what are people using as reasoning models behind voice AI today.**

å˜¿å¤§å®¶å¥½ï¼Œå¾ˆé«˜å…´å¤§å®¶èƒ½æ¥ã€‚å¤ªæ£’äº†ï¼Œä¿ƒæˆè¿™æ¬¡å¯¹è¯çœŸçš„è¶…å…´å¥‹ã€‚ä»Šå¤©æˆ‘ä»¬è¦èŠçš„æ˜¯æŒ‡ä»¤éµå¾ªå’ŒåŸºå‡†æµ‹è¯•â€”â€”çœ‹çœ‹äººä»¬ç°åœ¨åœ¨è¯­éŸ³ AI èƒŒåéƒ½åœ¨ç”¨å“ªäº›æ¨ç†æ¨¡å‹ã€‚

è§£æï¼š
* **instruction following**ï¼šæŒ‡ä»¤éµå¾ªï¼ŒAI æ¨¡å‹æŒ‰ç…§æŒ‡ä»¤æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›
* **benchmark** /ËˆbentÊƒmÉ‘Ërk/ï¼šåŸºå‡†æµ‹è¯•ï¼Œè¡¡é‡å’Œæ¯”è¾ƒæ¨¡å‹æ€§èƒ½çš„æ ‡å‡†åŒ–æµ‹è¯•
* **reasoning model**ï¼šæ¨ç†æ¨¡å‹ï¼Œå…·å¤‡é€»è¾‘æ¨ç†èƒ½åŠ›çš„ AI æ¨¡å‹

---

(2) [0:24-0:36] **I think as part of 2026, Cobalt is really diving into what's the state of voice AI across the board. Everything from the models people are using, the architectures people are using. And today we really want to deep dive on what does instruction following look like in 2026? What models are people using? What is the state of benchmarks and more.**

æˆ‘è§‰å¾—è¿›å…¥ 2026 å¹´ï¼Œ**Cobalt** æ­£åœ¨å…¨é¢æ·±å…¥ç ”ç©¶è¯­éŸ³ AI çš„ç°çŠ¶â€”â€”äººä»¬åœ¨ç”¨ä»€ä¹ˆæ¨¡å‹ã€ä»€ä¹ˆæ¶æ„ã€‚ä»Šå¤©æˆ‘ä»¬ç‰¹åˆ«æƒ³æ·±å…¥æ¢è®¨ï¼š2026 å¹´æŒ‡ä»¤éµå¾ªæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿå¤§å®¶ç”¨å“ªäº›æ¨¡å‹ï¼ŸåŸºå‡†æµ‹è¯•çš„ç°çŠ¶å¦‚ä½•ï¼Ÿ

è§£æï¼š
* **across the board** ğŸ”¥ï¼šå…¨é¢åœ°ã€å…¨æ–¹ä½ï¼ˆéå¸¸å®ç”¨çš„çŸ­è¯­ï¼‰
* **deep dive** ğŸ”¥ï¼šæ·±å…¥æ¢è®¨ï¼ˆç§‘æŠ€è¡Œä¸šé«˜é¢‘ç”¨è¯­ï¼Œåè¯/åŠ¨è¯çš†å¯ï¼‰
* **architecture** /ËˆÉ‘ËrkÉªtektÊƒÉ™r/ï¼šæ¶æ„ï¼ˆAI è¯­å¢ƒä¸‹æŒ‡ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆï¼‰

---

(3) [0:36-0:55] **So we can start with who's all here today. I'm Brooke. I'm the founder of Cobalt and we're building simulation and evaluation for voice agents. So my background is from Whimo. I led our evaluation infrastructure team at Whimo that was responsible for all of our simulation tooling. And now we're bringing everything we learned from robotics and self-driving and applying that to autonomy and voice agents.**

é‚£å…ˆä»‹ç»ä¸€ä¸‹ä»Šå¤©åˆ°åœºçš„å˜‰å®¾ã€‚æˆ‘æ˜¯ **Brooke**ï¼Œ**Cobalt** çš„åˆ›å§‹äººï¼Œæˆ‘ä»¬åšè¯­éŸ³ä»£ç†çš„æ¨¡æ‹Ÿå’Œè¯„ä¼°ã€‚æˆ‘ä¹‹å‰åœ¨ **Whimo** å¸¦è¯„ä¼°åŸºç¡€è®¾æ–½å›¢é˜Ÿï¼Œè´Ÿè´£æ‰€æœ‰æ¨¡æ‹Ÿå·¥å…·ã€‚ç°åœ¨æˆ‘ä»¬æŠŠä»æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶å­¦åˆ°çš„ç»éªŒï¼Œåº”ç”¨åˆ°è‡ªä¸»ç³»ç»Ÿå’Œè¯­éŸ³ä»£ç†ä¸Šã€‚

è§£æï¼š
* **simulation** /ËŒsÉªmjuËˆleÉªÊƒn/ï¼šæ¨¡æ‹Ÿã€ä»¿çœŸ
* **evaluation** /ÉªËŒvÃ¦ljuËˆeÉªÊƒn/ï¼šè¯„ä¼°
* **voice agent**ï¼šè¯­éŸ³ä»£ç†ï¼ŒåŸºäºè¯­éŸ³äº¤äº’çš„ AI æ™ºèƒ½ä½“
* **infrastructure** /ËˆÉªnfrÉ™strÊŒktÊƒÉ™r/ï¼šåŸºç¡€è®¾æ–½
* **autonomy** /É”ËËˆtÉ‘ËnÉ™mi/ï¼šè‡ªä¸»æ€§ã€è‡ªä¸»ç³»ç»Ÿ

---

(4) [0:55-1:21] **So I'm Zach. I'm one of the founders and CEO of a company called Ultravox AI. And so we train real-time speech models. So how do we take LLMs and train them to understand and produce speech natively. And then we also run dedicated inference for those models as well to try and make it fast and hopefully get to increasingly human-like conversations with AI. It's sort of like the long-term multi-year goal.**

æˆ‘æ˜¯ **Zach**ï¼Œ**Ultravox AI** çš„è”åˆåˆ›å§‹äººå…¼ CEOã€‚æˆ‘ä»¬è®­ç»ƒå®æ—¶è¯­éŸ³æ¨¡å‹â€”â€”å°±æ˜¯æ€ä¹ˆè®©å¤§è¯­è¨€æ¨¡å‹åŸç”Ÿåœ°ç†è§£å’Œç”Ÿæˆè¯­éŸ³ã€‚åŒæ—¶æˆ‘ä»¬è¿˜ä¸ºè¿™äº›æ¨¡å‹è¿è¡Œä¸“ç”¨æ¨ç†æœåŠ¡ï¼Œè®©å®ƒåˆå¿«åˆå¥½ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯å®ç°è¶Šæ¥è¶Šæ¥è¿‘äººç±»çš„ AI å¯¹è¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå¹´çš„é•¿æœŸç›®æ ‡ã€‚

è§£æï¼š
* **real-time speech model**ï¼šå®æ—¶è¯­éŸ³æ¨¡å‹
* **natively** /ËˆneÉªtÉªvli/ï¼šåŸç”Ÿåœ°ï¼ˆä¸ç»ä¸­é—´è½¬æ¢ï¼‰
* **dedicated inference**ï¼šä¸“ç”¨æ¨ç†ï¼ˆä¸ºç‰¹å®šæ¨¡å‹ä¼˜åŒ–çš„æ¨ç†æœåŠ¡ï¼‰
* **human-like**ï¼šç±»äººçš„ã€æ¥è¿‘äººç±»çš„

---

(5) [1:21-1:44] **I think we're slowly trying to climb our way there, but there's a lot of rich and interesting challenges along the journey as you both know. So that's me. And I'm Quinn. I'm co-founder of Daily. We make global infrastructure for real-time audio, video, and AI. And we also work a lot on an open source framework called Pipcat, which is the most widely used framework for building voice and real-time multimodal AI agents these days.**

æˆ‘ä»¬åœ¨æ…¢æ…¢å¾€é‚£ä¸ªæ–¹å‘æ”€ç™»ï¼Œä¸€è·¯ä¸Šæœ‰å¾ˆå¤šä¸°å¯Œæœ‰è¶£çš„æŒ‘æˆ˜ï¼Œä½ ä»¬éƒ½æ‡‚ã€‚è¿™å°±æ˜¯æˆ‘ã€‚ç„¶åæˆ‘æ˜¯ **Quinn**ï¼Œ**Daily** çš„è”åˆåˆ›å§‹äººï¼Œæˆ‘ä»¬åšå…¨çƒåŒ–çš„å®æ—¶éŸ³è§†é¢‘å’Œ AI åŸºç¡€è®¾æ–½ã€‚æˆ‘ä»¬è¿˜åšäº†ä¸€ä¸ªå« **Pipcat** çš„å¼€æºæ¡†æ¶ï¼Œæ˜¯ç›®å‰æœ€å¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³å’Œå®æ—¶å¤šæ¨¡æ€ AI ä»£ç†æ„å»ºæ¡†æ¶ã€‚

è§£æï¼š
* **climb our way there** ğŸ”¥ï¼šä¸€æ­¥æ­¥æ”€ç™»åˆ°é‚£é‡Œï¼ˆå½¢è±¡åœ°è¡¨è¾¾é€æ­¥è¾¾æˆç›®æ ‡ï¼‰
* **multimodal** /ËŒmÊŒltiËˆmoÊŠdl/ï¼šå¤šæ¨¡æ€çš„ï¼ˆæ”¯æŒæ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒç­‰å¤šç§è¾“å…¥è¾“å‡ºï¼‰
* **open source**ï¼šå¼€æºçš„

---

(6) [1:44-2:10] **Wow, what a group. I feel like all of us kind of see like are looking at the same elephant of voice AI from different angles. I think Zach's super excited to hear your take on it from the perspective of a platform, whereas we are coming at it from Eval's perspective. And I think Quinn, you get to see like super in the weeds of everything that people are doing. How do we actually â€” do you want to start by doing a recap of the benchmarks that you've been running?**

å“‡ï¼Œè¿™é˜µå®¹å¤ªå¼ºäº†ã€‚æˆ‘è§‰å¾—æˆ‘ä»¬ä¸‰ä¸ªå°±åƒä»ä¸åŒè§’åº¦åœ¨çœ‹è¯­éŸ³ AI è¿™å¤´å¤§è±¡ã€‚**Zach** è‚¯å®šå¾ˆæƒ³å¬ä½ ä»å¹³å°è§’åº¦çš„çœ‹æ³•ï¼Œè€Œæˆ‘ä»¬æ˜¯ä»è¯„ä¼°è§’åº¦å‡ºå‘ã€‚**Quinn**ï¼Œä½ èƒ½çœ‹åˆ°äººä»¬åœ¨åšçš„æ‰€æœ‰ç»†èŠ‚ã€‚é‚£ä½ è¦ä¸è¦å…ˆå›é¡¾ä¸€ä¸‹ä½ ä»¬åšçš„é‚£äº›åŸºå‡†æµ‹è¯•ï¼Ÿ

è§£æï¼š
* **the same elephant**ï¼šå¼•ç”¨"ç›²äººæ‘¸è±¡"å…¸æ•…ï¼ŒæŒ‡ä»ä¸åŒè§’åº¦çœ‹åŒä¸€äº‹ç‰© ğŸ”¥
* **your take on it** ğŸ”¥ï¼šä½ å¯¹æ­¤çš„çœ‹æ³•ï¼ˆå£è¯­ä¸­è¶…å¸¸ç”¨ï¼‰
* **in the weeds** ğŸ”¥ï¼šæ·±å…¥ç»†èŠ‚ï¼ˆä¿šè¯­ï¼ŒæŒ‡éå¸¸äº†è§£ç»†èŠ‚ï¼‰
* **recap** /ËˆriËkÃ¦p/ï¼šå›é¡¾ã€æ€»ç»“

---

(7) [2:10-2:39] **Sure. I can intro that real quick and then we can figure out how to fit it in the edit. So, I'm excited to talk to both of you because I turned one of our internal benchmarks, we have it daily, into something we could publish because thinking about how different models perform on the hard things about voice AI, instruction following, function calling reliability, turn-taking reliability is challenging and we all have kind of tests and vibes that we do, but I wanted to publish something that people could criticize and try to help make better.**

å¥½çš„ï¼Œæˆ‘å¿«é€Ÿä»‹ç»ä¸€ä¸‹ã€‚æˆ‘å¾ˆå…´å¥‹è·Ÿä½ ä»¬ä¿©èŠï¼Œå› ä¸ºæˆ‘æŠŠæˆ‘ä»¬ **Daily** å†…éƒ¨çš„ä¸€ä¸ªåŸºå‡†æµ‹è¯•å˜æˆäº†å¯ä»¥å…¬å¼€å‘å¸ƒçš„ä¸œè¥¿ã€‚æ€è€ƒä¸åŒæ¨¡å‹åœ¨è¯­éŸ³ AI çš„éš¾ç‚¹ä¸Šâ€”â€”æŒ‡ä»¤éµå¾ªã€å‡½æ•°è°ƒç”¨å¯é æ€§ã€è½®æ›¿å¯é æ€§â€”â€”è¡¨ç°å¦‚ä½•ï¼Œè¿™å¾ˆæœ‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬éƒ½æœ‰å„è‡ªçš„æµ‹è¯•å’Œç›´è§‰åˆ¤æ–­ï¼Œä½†æˆ‘æƒ³å‘å¸ƒä¸€äº›ä¸œè¥¿è®©å¤§å®¶æ¥æ‰¹è¯„å’Œæ”¹è¿›ã€‚

è§£æï¼š
* **function calling**ï¼šå‡½æ•°è°ƒç”¨ï¼ŒAI æ¨¡å‹è°ƒç”¨å¤–éƒ¨å·¥å…·/API çš„èƒ½åŠ›
* **turn-taking**ï¼šè½®æ›¿ã€è¯è½®è½¬æ¢ï¼ˆå¯¹è¯ä¸­è¯´è¯è€…äº¤æ›¿çš„æœºåˆ¶ï¼‰
* **vibes** /vaÉªbz/ï¼šç›´è§‰æ„Ÿå—ï¼ˆè¿™é‡ŒæŒ‡å‡­æ„Ÿè§‰åšçš„éæ­£å¼æµ‹è¯•ï¼‰ğŸ”¥
* **criticize** /ËˆkrÉªtÉªsaÉªz/ï¼šæ‰¹è¯„ã€æå‡ºæ‰¹åˆ¤æ€§æ„è§

---

(8) [2:39-3:04] **And Ultravox did amazingly well on that benchmark. So I think I have a couple questions for Brooke and for Zach. I'll just tee it up as â€” Brooke, what's hard about benchmarking voice AI because you've thought a lot about the whole testing end to end challenge in voice AI? And then Zach, how did Ultravox do so well on this really hard 30-turn long-context tool-calling benchmark?**

**Ultravox** åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°éå¸¸å‡ºè‰²ã€‚æ‰€ä»¥æˆ‘æœ‰å‡ ä¸ªé—®é¢˜æƒ³é—® **Brooke** å’Œ **Zach**ã€‚**Brooke**ï¼Œè¯­éŸ³ AI çš„åŸºå‡†æµ‹è¯•éš¾åœ¨å“ªå„¿ï¼Ÿä½ å¯¹ç«¯åˆ°ç«¯æµ‹è¯•çš„æŒ‘æˆ˜æƒ³äº†å¾ˆå¤šã€‚**Zach**ï¼Œ**Ultravox** æ˜¯æ€ä¹ˆåœ¨è¿™ä¸ªè¶…éš¾çš„ 30 è½®é•¿ä¸Šä¸‹æ–‡å·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è¿™ä¹ˆå¥½çš„ï¼Ÿ

è§£æï¼š
* **tee it up** ğŸ”¥ï¼šå¼•å‡ºè¯é¢˜ã€åšé“ºå«ï¼ˆæºè‡ªé«˜å°”å¤«çƒ"å¼€çƒ"ï¼‰
* **end to end**ï¼šç«¯åˆ°ç«¯ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´æµç¨‹ï¼‰
* **30-turn long-context tool-calling benchmark**ï¼š30 è½®é•¿ä¸Šä¸‹æ–‡å·¥å…·è°ƒç”¨åŸºå‡†æµ‹è¯•

---

(9) [3:04-3:34] **Yeah. Well first of all I think Quinn I'm super excited about the benchmarks you published. Benchmarking instruction following is no easy feat because kind of how you compare apples to apples and set up a test that's both hard enough to be able to capture â€” to actually show something meaningful that's not just ridiculously hard or ridiculously easy, but then also that you can compare across lots of different models.**

é¦–å…ˆ **Quinn** æˆ‘å¯¹ä½ å‘å¸ƒçš„åŸºå‡†æµ‹è¯•è¶…å…´å¥‹ã€‚åŸºå‡†æµ‹è¯•æŒ‡ä»¤éµå¾ªç»éæ˜“äº‹ï¼Œå› ä¸ºä½ å¾—è®¾è®¡ä¸€ä¸ªå…¬å¹³å¯¹æ¯”çš„æµ‹è¯•â€”â€”æ—¢è¶³å¤Ÿéš¾ã€èƒ½çœŸæ­£å±•ç¤ºæœ‰æ„ä¹‰çš„ç»“æœï¼Œåˆä¸ä¼šè’è°¬åœ°éš¾æˆ–ç®€å•ï¼Œè€Œä¸”è¿˜èƒ½è·¨ä¸åŒæ¨¡å‹åšæ¯”è¾ƒã€‚

è§£æï¼š
* **no easy feat** ğŸ”¥ï¼šç»éæ˜“äº‹ï¼ˆfeat = å£®ä¸¾ã€äº†ä¸èµ·çš„äº‹ï¼‰
* **compare apples to apples** ğŸ”¥ï¼šå…¬å¹³å¯¹æ¯”ï¼ˆåŒç±»äº‹ç‰©ä¹‹é—´æ¯”è¾ƒçš„ä¹ è¯­ï¼‰
* **ridiculously** /rÉªËˆdÉªkjÉ™lÉ™sli/ï¼šè’è°¬åœ°ã€å¯ç¬‘åœ°

---

(10) [3:34-4:02] **Especially because you start to say like, well, how hard can it be? I'll just put in the same prompt to a bunch of different systems. But one of the challenges is different prompts will do well on different systems or different prompting techniques work well. And so really what people want to know is what is the best I can get out of each of these systems. Not necessarily just what â€” it's not as useful to be able to just compare something out of the box if there's an obvious optimization.**

å°¤å…¶æ˜¯ä½ ä¼šè§‰å¾—â€”â€”èƒ½æœ‰å¤šéš¾å‘¢ï¼Ÿæˆ‘æŠŠåŒä¸€ä¸ªæç¤ºè¯æ‰”ç»™ä¸€å †ä¸åŒç³»ç»Ÿä¸å°±è¡Œäº†ã€‚ä½†æŒ‘æˆ˜åœ¨äºï¼Œä¸åŒæç¤ºè¯åœ¨ä¸åŒç³»ç»Ÿä¸Šæ•ˆæœä¸åŒï¼Œä¸åŒçš„æç¤ºå·¥ç¨‹æŠ€å·§ä¹Ÿä¸ä¸€æ ·ã€‚æ‰€ä»¥äººä»¬çœŸæ­£æƒ³çŸ¥é“çš„æ˜¯ï¼šæ¯ä¸ªç³»ç»Ÿæˆ‘èƒ½æ‹¿åˆ°çš„æœ€ä½³è¡¨ç°æ˜¯ä»€ä¹ˆï¼Ÿå¦‚æœæœ‰æ˜æ˜¾çš„ä¼˜åŒ–ç©ºé—´ï¼Œå…‰æ¯”è¾ƒå¼€ç®±å³ç”¨çš„ç»“æœæ„ä¹‰ä¸å¤§ã€‚

è§£æï¼š
* **prompting techniques**ï¼šæç¤ºå·¥ç¨‹æŠ€å·§ï¼ˆä¼˜åŒ– AI è¾“å…¥çš„æ–¹æ³•ï¼‰
* **out of the box** ğŸ”¥ï¼šå¼€ç®±å³ç”¨ï¼ˆä¸åšä»»ä½•è°ƒæ•´ç›´æ¥ä½¿ç”¨ï¼‰
* **optimization** /ËŒÉ‘ËptÉªmÉªËˆzeÉªÊƒn/ï¼šä¼˜åŒ–

---

(11) [4:02-4:30] **And so yeah, I think Quinn this is definitely the hardest benchmark. We have benchmarks on TTS and STT as well as VAD, but I think instruction following is by far the hardest one to benchmark because of these problems. And then I also think that there's just such hunger for this. So, I'm really excited to hear what Zach is hearing in terms of like why they think they did so well on these benchmarks because this is by far the biggest thing that people are focused on.**

æ‰€ä»¥ **Quinn** è¿™ç»å¯¹æ˜¯æœ€éš¾çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬æœ‰ **TTS** å’Œ **STT** ä»¥åŠ **VAD** çš„åŸºå‡†æµ‹è¯•ï¼Œä½†æŒ‡ä»¤éµå¾ªå› ä¸ºè¿™äº›é—®é¢˜æ˜¯æœ€éš¾åšåŸºå‡†æµ‹è¯•çš„ã€‚è€Œä¸”å¤§å®¶å¯¹è¿™æ–¹é¢çš„éœ€æ±‚å¤ªè¿«åˆ‡äº†ã€‚æˆ‘è¶…æœŸå¾…å¬ **Zach** è¯´ä»–ä»¬ä¸ºä»€ä¹ˆåœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è¿™ä¹ˆå¥½ï¼Œå› ä¸ºè¿™æ˜¯ç›®å‰å¤§å®¶æœ€å…³æ³¨çš„é¢†åŸŸã€‚

è§£æï¼š
* **TTS**ï¼šText-to-Speechï¼Œæ–‡å­—è½¬è¯­éŸ³
* **STT**ï¼šSpeech-to-Textï¼Œè¯­éŸ³è½¬æ–‡å­—
* **VAD**ï¼šVoice Activity Detectionï¼Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹
* **by far** ğŸ”¥ï¼šç›®å‰ä¸ºæ­¢æœ€â€¦ã€é¥é¥é¢†å…ˆï¼ˆå¼ºè°ƒç¨‹åº¦çš„å‰¯è¯çŸ­è¯­ï¼‰
* **hunger for**ï¼šå¯¹â€¦çš„å¼ºçƒˆéœ€æ±‚/æ¸´æœ›

---

(12) [4:30-4:54] **Now latency is low enough and interruption rates and being able to do voice activity detection is good enough that people are pretty satisfied with where we are. But that instruction following especially as you keep trying to do harder and harder types of tasks is really where people spend a lot of time.**

ç°åœ¨å»¶è¿Ÿå·²ç»å¤Ÿä½äº†ï¼Œä¸­æ–­ç‡å’Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹ä¹Ÿå¤Ÿå¥½äº†ï¼Œäººä»¬å¯¹ç°çŠ¶è¿˜ç®—æ»¡æ„ã€‚ä½†æŒ‡ä»¤éµå¾ªâ€”â€”å°¤å…¶æ˜¯å½“ä½ ä¸æ–­å°è¯•è¶Šæ¥è¶Šéš¾çš„ä»»åŠ¡æ—¶â€”â€”æ‰æ˜¯äººä»¬èŠ±å¤§é‡æ—¶é—´çš„åœ°æ–¹ã€‚

è§£æï¼š
* **latency** /ËˆleÉªtÉ™nsi/ï¼šå»¶è¿Ÿï¼ˆä»è¾“å…¥åˆ°å“åº”çš„æ—¶é—´ï¼‰
* **interruption rate**ï¼šä¸­æ–­ç‡ï¼ˆAI é”™è¯¯æ‰“æ–­ç”¨æˆ·çš„é¢‘ç‡ï¼‰
* **pretty satisfied**ï¼šç›¸å½“æ»¡æ„ï¼ˆpretty åšå‰¯è¯ = ç›¸å½“ã€æŒºï¼‰

---

(13) [4:54-5:13] **Yeah, I thought it was â€” I was also really excited when I first saw â€” I don't remember how I even saw originally that you had published this eval somewhere, Quinn. Maybe you had linked to it somewhere. I remember looking at it being like, ah this is good because it's like real. There's so many evals out there and we published them as well, things like Big Bench Audio that measure interesting logical reasoning puzzles and how well do you do over audio.**

å¯¹ï¼Œæˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°çš„æ—¶å€™ä¹Ÿè¶…å…´å¥‹â€”â€”æˆ‘éƒ½ä¸è®°å¾—æ€ä¹ˆçœ‹åˆ° **Quinn** ä½ å‘å¸ƒè¿™ä¸ªè¯„ä¼°çš„äº†ã€‚æˆ‘è®°å¾—çœ‹åˆ°å°±æƒ³ï¼šå•Šè¿™ä¸ªä¸é”™ï¼Œå› ä¸ºå®ƒå¾ˆçœŸå®ã€‚ç°åœ¨æœ‰å¤ªå¤šè¯„ä¼°æ ‡å‡†äº†ï¼Œæˆ‘ä»¬ä¹Ÿå‘å¸ƒè¿‡ä¸€äº›ï¼Œæ¯”å¦‚ **Big Bench Audio** è¿™ç§è¡¡é‡é€»è¾‘æ¨ç†è°œé¢˜çš„ï¼Œçœ‹ä½ åœ¨éŸ³é¢‘ä¸Šè¡¨ç°å¦‚ä½•ã€‚

è§£æï¼š
* **eval**ï¼ševaluation çš„ç¼©å†™ï¼Œè¯„ä¼°/è¯„æµ‹
* **Big Bench Audio**ï¼šä¸€ä¸ªçŸ¥åçš„éŸ³é¢‘ AI è¯„æµ‹åŸºå‡†
* **logical reasoning puzzles**ï¼šé€»è¾‘æ¨ç†è°œé¢˜

---

(14) [5:13-5:36] **But they're all like good proxies for something. Like Big Bench Audio is a proxy for speech understanding and how you can compare that to the text-based input method. But I liked that the eval for this is like, oh it's actually a really common voice AI scenario. We've got some knowledge that we want to dump into the system prompt and then we've got some tools that need to be called through this dialogue.**

ä½†å®ƒä»¬éƒ½åªæ˜¯æŸç§ä»£ç†æŒ‡æ ‡ã€‚åƒ **Big Bench Audio** æ˜¯è¯­éŸ³ç†è§£çš„ä»£ç†æŒ‡æ ‡ï¼Œå¯ä»¥å’Œæ–‡æœ¬è¾“å…¥æ–¹å¼åšå¯¹æ¯”ã€‚ä½†æˆ‘å–œæ¬¢ä½ è¿™ä¸ªè¯„ä¼°çš„åœ°æ–¹æ˜¯â€”â€”å®ƒæ˜¯ä¸€ä¸ªçœŸå®å¸¸è§çš„è¯­éŸ³ AI åœºæ™¯ï¼šæˆ‘ä»¬æœ‰ä¸€äº›çŸ¥è¯†è¦å¡è¿›ç³»ç»Ÿæç¤ºè¯ï¼Œç„¶åæœ‰ä¸€äº›å·¥å…·éœ€è¦åœ¨å¯¹è¯ä¸­è¢«è°ƒç”¨ã€‚

è§£æï¼š
* **proxy** /ËˆprÉ‘Ëksi/ ğŸ”¥ï¼šä»£ç†æŒ‡æ ‡ã€æ›¿ä»£æŒ‡æ ‡ï¼ˆç”¨é—´æ¥æ–¹å¼è¡¡é‡æŸäº‹ç‰©ï¼‰
* **system prompt**ï¼šç³»ç»Ÿæç¤ºè¯ï¼ˆç»™ AI çš„åˆå§‹æŒ‡ä»¤å’Œä¸Šä¸‹æ–‡ï¼‰
* **dump into**ï¼šå¡è¿›ã€çŒå…¥ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰

---

(15) [5:36-5:59] **And it doesn't of course have all the properties of a hard eval â€” your audio is crystal clear, you enunciate beautifully â€” but I think it was interesting because even with that crystal clear audio you see some of the difficulties that exist out there. And so it was exciting. And I'll tell you that the number one secret that I found for doing well in these benchmarks is just hacking into Quinn's computer and then you have this little script â€” if it's Ultravox, you boost the numbers. It's super simple.**

å½“ç„¶å®ƒä¸å…·å¤‡ä¸€ä¸ªå›°éš¾è¯„æµ‹çš„å…¨éƒ¨ç‰¹æ€§â€”â€”ä½ çš„éŸ³é¢‘è´¨é‡å¾ˆå¥½ã€å‘éŸ³å¾ˆæ¸…æ™°â€”â€”ä½†æœ‰æ„æ€çš„æ˜¯ï¼Œå³ä½¿éŸ³é¢‘è¿™ä¹ˆæ¸…æ™°ï¼Œä½ ä¾ç„¶èƒ½çœ‹åˆ°å„ç§å›°éš¾ã€‚è¿™è®©äººå¾ˆå…´å¥‹ã€‚æˆ‘å‘Šè¯‰ä½ ä»¬ï¼Œæˆ‘å‘ç°åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸­å–å¾—å¥½æˆç»©çš„å¤´å·ç§˜è¯€å°±æ˜¯â€”â€”é»‘è¿› **Quinn** çš„ç”µè„‘ï¼Œç„¶åæœ‰ä¸€ä¸ªå°è„šæœ¬ï¼šå¦‚æœæ˜¯ **Ultravox**ï¼Œå°±æŠŠåˆ†æ•°è°ƒé«˜ã€‚è¶…ç®€å•çš„ã€‚

è§£æï¼š
* **crystal clear**ï¼šéå¸¸æ¸…æ™°ï¼ˆæ°´æ™¶èˆ¬æ¸…æ¾ˆï¼‰
* **enunciate** /ÉªËˆnÊŒnsieÉªt/ï¼šæ¸…æ™°åœ°å‘éŸ³ã€å’¬å­—æ¸…æ¥š
* **hacking into**ï¼šé»‘å…¥ï¼ˆæ­¤å¤„ä¸ºç©ç¬‘ï¼‰
* **boost** /buËst/ï¼šæå‡ã€å¢åŠ 

---

(16) [5:59-6:30] **You know, I joke, but there's a lot of benchmark maxing in AI and one of the challenges of writing good benchmarks I think is trying to understand first what the realistic use case is and second make the benchmark something that's not just going to map to something that people are benchmark maxing but really is going to reflect some part of the hard workloads here. And also I think having benchmarks by third parties â€” benchmarks by model companies themselves or by platform companies themselves, it's like that's awesome â€” wow, you guys are all number one on your own benchmarks, that's crazy.**

å¼€ç©ç¬‘å•¦ã€‚ä½† AI é¢†åŸŸç¡®å®æœ‰å¾ˆå¤š"åˆ·æ¦œ"è¡Œä¸ºï¼Œå†™å¥½åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜åœ¨äºï¼šé¦–å…ˆç†è§£çœŸå®çš„ä½¿ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Œå…¶æ¬¡ç¡®ä¿æµ‹è¯•ä¸ä¼šå˜æˆçº¯ç²¹çš„åˆ·åˆ†å¯¹è±¡ï¼Œè€Œæ˜¯çœŸæ­£åæ˜ å®é™…å·¥ä½œä¸­çš„éš¾ç‚¹ã€‚å¦å¤–æˆ‘è§‰å¾—ç¬¬ä¸‰æ–¹åŸºå‡†æµ‹è¯•å¾ˆé‡è¦â€”â€”æ¨¡å‹å…¬å¸æˆ–å¹³å°å…¬å¸è‡ªå·±çš„åŸºå‡†æµ‹è¯•å˜›ï¼Œ"å“‡ï¼Œä½ ä»¬åœ¨è‡ªå·±çš„åŸºå‡†æµ‹è¯•ä¸Šéƒ½æ˜¯ç¬¬ä¸€åï¼Œå¥½å‰å®³å“¦"ã€‚

è§£æï¼š
* **benchmark maxing** ğŸ”¥ï¼šåˆ·æ¦œã€ä¸“é—¨é’ˆå¯¹åŸºå‡†æµ‹è¯•åšä¼˜åŒ–ï¼ˆAI è¡Œä¸šæœ¯è¯­ï¼‰
* **realistic use case**ï¼šçœŸå®ä½¿ç”¨åœºæ™¯
* **third party**ï¼šç¬¬ä¸‰æ–¹ï¼ˆç‹¬ç«‹çš„å¤–éƒ¨æœºæ„ï¼‰
* **workload** /ËˆwÉœËrkloÊŠd/ï¼šå·¥ä½œè´Ÿè½½

---

(17) [6:30-6:57] **But sorry I interrupted you, Zach. You were going to say something profound. No, no, no. That was my limit. No. I mean I'm happy to talk about at some point what makes Ultravox interesting, but I think actually what the eval exposes when you look at the results is there's this really interesting trade-off that's now emerged, I would say, in the model selection space, which is that you have to now reason about latency and intelligence on the same graph.**

æŠ±æ­‰æ‰“æ–­ä½ äº† **Zach**ï¼Œä½ æœ¬æ¥è¦è¯´äº›æ·±åˆ»çš„ä¸œè¥¿ã€‚â€”â€”ä¸ä¸ä¸ï¼Œæˆ‘çš„æ·±åº¦åˆ°æ­¤ä¸ºæ­¢äº†ã€‚æˆ‘å¾ˆä¹æ„ä¹‹åèŠ **Ultravox** æœ‰ä»€ä¹ˆç‰¹åˆ«çš„ã€‚ä½†æˆ‘è§‰å¾—è¿™ä¸ªè¯„æµ‹æš´éœ²å‡ºçš„ä¸€ä¸ªæœ‰è¶£ç°è±¡æ˜¯ï¼Œæ¨¡å‹é€‰æ‹©é¢†åŸŸå‡ºç°äº†ä¸€ä¸ªæƒè¡¡â€”â€”ä½ ç°åœ¨å¿…é¡»åœ¨åŒä¸€å¼ å›¾è¡¨ä¸ŠåŒæ—¶è€ƒé‡å»¶è¿Ÿå’Œæ™ºèƒ½ã€‚

è§£æï¼š
* **profound** /prÉ™ËˆfaÊŠnd/ï¼šæ·±åˆ»çš„ã€æ·±å¥¥çš„
* **trade-off** ğŸ”¥ï¼šæƒè¡¡ã€å–èˆï¼ˆä¸¤ä¸ªç›®æ ‡ä¹‹é—´çš„å¹³è¡¡ï¼‰
* **model selection**ï¼šæ¨¡å‹é€‰æ‹©
* **on the same graph**ï¼šåœ¨åŒä¸€å¼ å›¾è¡¨ä¸Šï¼ˆæŒ‡åŒæ—¶è€ƒè™‘ä¸¤ä¸ªç»´åº¦ï¼‰

---

(18) [6:57-7:28] **And I think what you now see and what your benchmark is really showing is that the frontier is absolutely moving forward. The cost in latency of those frontier models is extremely high right now. I think I remember like the fastest one is like 916 milliseconds at like the median â€” I can't remember what it was. That's a text-mode LLM. So you still have to add like 500 milliseconds or 700 milliseconds of end-to-end latency if you're comparing it to Ultravox. Exactly.**

ä½ çš„åŸºå‡†æµ‹è¯•å±•ç¤ºçš„æ˜¯å‰æ²¿ç¡®å®åœ¨æ¨è¿›ï¼Œä½†é‚£äº›å‰æ²¿æ¨¡å‹çš„å»¶è¿Ÿä»£ä»·ç›®å‰æå…¶é«˜ã€‚æˆ‘è®°å¾—æœ€å¿«çš„å¤§æ¦‚ä¸­ä½æ•°æ˜¯ 916 æ¯«ç§’â€”â€”é‚£è¿˜æ˜¯çº¯æ–‡æœ¬æ¨¡å¼çš„å¤§è¯­è¨€æ¨¡å‹ã€‚æ‰€ä»¥å¦‚æœè·Ÿ **Ultravox** æ¯”ï¼Œä½ è¿˜å¾—å†åŠ  500 åˆ° 700 æ¯«ç§’çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚â€”â€”æ²¡é”™ã€‚

è§£æï¼š
* **frontier** /frÊŒnËˆtÉªr/ï¼šå‰æ²¿ï¼ˆAI è¯­å¢ƒä¸­æŒ‡æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼‰
* **median** /ËˆmiËdiÉ™n/ï¼šä¸­ä½æ•°
* **millisecond**ï¼šæ¯«ç§’ï¼ˆmsï¼‰
* **end-to-end latency**ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿï¼ˆä»ç”¨æˆ·è¯´è¯åˆ° AI å›å¤çš„æ€»å»¶è¿Ÿï¼‰

---

(19) [7:28-7:56] **And so I think that's what's really interesting â€” we saw all this explosive growth in model reasoning in 2025 and obviously thinking modes are going to be a big thing moving forward, but it's not obvious always how it translates into voice AI, which is why I think for a lot of deployments you continue to see models that are now like a year, year and a half old still as the foundation for those systems.**

è¿™å°±æ˜¯æœ‰æ„æ€çš„åœ°æ–¹â€”â€”æˆ‘ä»¬åœ¨ 2025 å¹´çœ‹åˆ°äº†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„çˆ†å‘å¼å¢é•¿ï¼Œ"æ€è€ƒæ¨¡å¼"æœªæ¥è‚¯å®šæ˜¯å¤§è¶‹åŠ¿ã€‚ä½†è¿™äº›è¿›æ­¥ä¸æ€»æ˜¯èƒ½ç›´æ¥è½¬åŒ–åˆ°è¯­éŸ³ AIï¼Œæ‰€ä»¥å¾ˆå¤šå®é™…éƒ¨ç½²ä¸­ä½ è¿˜æ˜¯çœ‹åˆ°ä¸€å¹´ã€ä¸€å¹´åŠå‰çš„è€æ¨¡å‹ä»ç„¶ä½œä¸ºç³»ç»ŸåŸºç¡€ã€‚

è§£æï¼š
* **explosive growth**ï¼šçˆ†å‘å¼å¢é•¿
* **thinking modes**ï¼šæ€è€ƒæ¨¡å¼ï¼ˆAI æ¨¡å‹çš„æ¨ç†/æ·±åº¦æ€è€ƒåŠŸèƒ½ï¼‰
* **deployment** /dÉªËˆplÉ”ÉªmÉ™nt/ï¼šéƒ¨ç½²ã€å®é™…åº”ç”¨
* **translate into** ğŸ”¥ï¼šè½¬åŒ–ä¸ºï¼ˆè¿™é‡Œä¸æ˜¯"ç¿»è¯‘"ï¼Œè€Œæ˜¯"è½¬å˜æˆ"ï¼‰

---

(20) [7:56-8:21] **And I think if you look at the instruction following and the intelligence â€” why isn't it doing very well? Well, it was doing really well probably for a late 2024 era model, but we made lots of progress and it's been hard sometimes to recognize that in voice AI because of this trade-off between intelligence and latency. I think that's been a tricky thing to navigate. I think that's what your eval does a really great job of demonstrating â€” the trade-offs that exist there.**

ä½ è¦çœ‹æŒ‡ä»¤éµå¾ªå’Œæ™ºèƒ½è¡¨ç°â€”â€”ä¸ºä»€ä¹ˆä¸å¤Ÿå¥½ï¼Ÿå…¶å®å¯¹äº 2024 å¹´æœ«çš„æ¨¡å‹æ¥è¯´å·²ç»å¾ˆå¥½äº†ï¼Œåªæ˜¯æˆ‘ä»¬è¿›æ­¥å¤ªå¿«ã€‚åœ¨è¯­éŸ³ AI é¢†åŸŸï¼Œç”±äºæ™ºèƒ½å’Œå»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ï¼Œæœ‰æ—¶å¾ˆéš¾æ„è¯†åˆ°è¿™ç§è¿›æ­¥ã€‚è¿™ç¡®å®ä¸å¥½æŠŠæ¡ã€‚æˆ‘è§‰å¾—ä½ çš„è¯„æµ‹å°±å¾ˆå¥½åœ°å±•ç¤ºäº†è¿™äº›æƒè¡¡ã€‚

è§£æï¼š
* **navigate** /ËˆnÃ¦vÉªÉ¡eÉªt/ ğŸ”¥ï¼šåº”å¯¹ã€æŠŠæ¡ï¼ˆæ¯”å–»ä¹‰ï¼Œä¸æ˜¯"å¯¼èˆª"ï¼‰
* **era model**ï¼šæŸä¸ªæ—¶ä»£çš„æ¨¡å‹
* **demonstrate** /ËˆdemÉ™nstreÉªt/ï¼šå±•ç¤ºã€è¯æ˜

---

(21) [8:21-8:46] **Well, we have these intuitions, those of us who are building this stuff every day. And I think one of the things about a benchmark like this is a really good opportunity to step back a little bit. One thing really surprised me. So, to put it in context for people who are not quite as in the voice AI space as the three of us. It really is true what you're saying that in production we mostly see GPT-4o and Gemini 2.5 Flash.**

æˆ‘ä»¬è¿™äº›æ¯å¤©åœ¨åšè¿™äº›ä¸œè¥¿çš„äººéƒ½æœ‰ç›´è§‰ã€‚ä½†è¿™ç§åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„æœºä¼šè®©ä½ é€€ä¸€æ­¥çœ‹å…¨å±€ã€‚æœ‰ä¸€ä»¶äº‹çœŸçš„è®©æˆ‘å¾ˆæƒŠè®¶ã€‚ç»™ä¸å¤ªç†Ÿæ‚‰è¯­éŸ³ AI çš„äººåšä¸ªèƒŒæ™¯ä»‹ç»ï¼šä½ è¯´çš„ç¡®å®æ²¡é”™ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­æˆ‘ä»¬ä¸»è¦çœ‹åˆ°çš„æ˜¯ **GPT-4o** å’Œ **Gemini 2.5 Flash**ã€‚

è§£æï¼š
* **intuition** /ËŒÉªntjuËˆÉªÊƒn/ï¼šç›´è§‰
* **step back** ğŸ”¥ï¼šé€€ä¸€æ­¥ã€ä»å…¨å±€è§’åº¦çœ‹
* **put it in context** ğŸ”¥ï¼šåšèƒŒæ™¯ä»‹ç»ã€æ”¾åˆ°è¯­å¢ƒä¸­ç†è§£
* **in production**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ˆå®é™…è¿è¡Œçš„ç³»ç»Ÿä¸­ï¼‰

---

(22) [8:46-9:04] **In terms of people using the models from the big labs, and those are year and a half old models â€” older than that if you think about when they were trained. But because they have the right mix of intelligence and latency and because people have gotten prompts optimized for them, they're pretty safe choices that a lot of people are still sticking with.**

è¯´çš„æ˜¯å¤§å®éªŒå®¤çš„æ¨¡å‹ï¼Œè¿™äº›éƒ½æ˜¯ä¸€å¹´åŠå‰çš„æ¨¡å‹äº†â€”â€”è€ƒè™‘åˆ°è®­ç»ƒæ—¶é—´å…¶å®æ›´è€ã€‚ä½†å› ä¸ºå®ƒä»¬åœ¨æ™ºèƒ½å’Œå»¶è¿Ÿä¹‹é—´æœ‰åˆé€‚çš„å¹³è¡¡ï¼Œè€Œä¸”å¤§å®¶å·²ç»æŠŠæç¤ºè¯é’ˆå¯¹å®ƒä»¬ä¼˜åŒ–å¥½äº†ï¼Œæ‰€ä»¥å®ƒä»¬æ˜¯å¾ˆå¤šäººä»ç„¶åšå®ˆçš„å®‰å…¨é€‰æ‹©ã€‚

è§£æï¼š
* **the right mix**ï¼šåˆé€‚çš„æ­é…/å¹³è¡¡
* **optimized for**ï¼šé’ˆå¯¹â€¦ä¼˜åŒ–
* **sticking with** ğŸ”¥ï¼šåšæŒä½¿ç”¨ã€ç»§ç»­ç”¨ï¼ˆstick with = ä¸æ¢ã€åšå®ˆï¼‰
* **safe choice**ï¼šå®‰å…¨çš„é€‰æ‹©ã€ç¨³å¦¥çš„é€‰æ‹©

---

(23) [9:04-9:30] **And so that's the context I live in. So when I run benchmarks, I kind of know that and I don't even spend a lot of time necessarily on a reasoning model that I know is going to be so slow. But to make this public benchmark, I ran it against all the models. And I was actually surprised at how good GPT-5.1 and the latest Claude and Gemini 3 did on this benchmark. They saturated it.**

è¿™å°±æ˜¯æˆ‘æ‰€å¤„çš„ç¯å¢ƒã€‚æ‰€ä»¥è·‘åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘äº†è§£è¿™äº›èƒŒæ™¯ï¼Œä¸ä¼šåœ¨é‚£äº›æˆ‘çŸ¥é“ä¼šå¾ˆæ…¢çš„æ¨ç†æ¨¡å‹ä¸ŠèŠ±å¤ªå¤šæ—¶é—´ã€‚ä½†ä¸ºäº†åšè¿™ä¸ªå…¬å¼€çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘è·‘äº†æ‰€æœ‰æ¨¡å‹ã€‚è®©æˆ‘æƒŠè®¶çš„æ˜¯ **GPT-5.1**ã€æœ€æ–°çš„ **Claude** å’Œ **Gemini 3** åœ¨è¿™ä¸ªæµ‹è¯•ä¸Šè¡¨ç°å¤šå¥½â€”â€”å®ƒä»¬ç›´æ¥æŠŠåˆ†æ•°æ‰“æ»¡äº†ã€‚

è§£æï¼š
* **the context I live in**ï¼šæˆ‘æ‰€å¤„çš„ç¯å¢ƒ/èƒŒæ™¯
* **saturate** /ËˆsÃ¦tÊƒÉ™reÉªt/ ğŸ”¥ï¼šé¥±å’Œã€æ‰“æ»¡ï¼ˆè¿™é‡ŒæŒ‡æ¨¡å‹æˆç»©è¾¾åˆ°åŸºå‡†æµ‹è¯•çš„ä¸Šé™ï¼‰
* **ran it against**ï¼šé’ˆå¯¹â€¦è¿è¡Œæµ‹è¯•

---

(24) [9:30-9:56] **And this was not a benchmark that any model saturated 6 months ago. On the other hand, they're all too slow to use for a voice agent. So that was like a really interesting thing. Oh okay, we actually do have this frontier-level saturation of what I thought was a really hard benchmark now, but I still can't use these models. And then when you and I were talking about it, you pointed out something that I hadn't noticed in the initial runs.**

è¿™ä¸ªåŸºå‡†æµ‹è¯• 6 ä¸ªæœˆå‰æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½æ‰“æ»¡ã€‚ä½†å¦ä¸€æ–¹é¢ï¼Œå®ƒä»¬å¯¹è¯­éŸ³ä»£ç†æ¥è¯´éƒ½å¤ªæ…¢äº†ã€‚è¿™å°±å¾ˆæœ‰æ„æ€â€”â€”å¥½å§ï¼Œæˆ‘ä»¬ç¡®å®è¾¾åˆ°äº†å‰æ²¿çº§åˆ«çš„é¥±å’Œï¼Œè¿æˆ‘è®¤ä¸ºå¾ˆéš¾çš„æµ‹è¯•éƒ½è¢«æ‰“æ»¡äº†ï¼Œä½†æˆ‘è¿˜æ˜¯ç”¨ä¸äº†è¿™äº›æ¨¡å‹ã€‚ç„¶åä½ å’Œæˆ‘è®¨è®ºæ—¶ï¼Œä½ æŒ‡å‡ºäº†ä¸€ä¸ªæˆ‘åœ¨åˆå§‹æµ‹è¯•ä¸­æ²¡æ³¨æ„åˆ°çš„ä¸œè¥¿ã€‚

è§£æï¼š
* **on the other hand** ğŸ”¥ï¼šå¦ä¸€æ–¹é¢
* **frontier-level**ï¼šå‰æ²¿çº§åˆ«çš„
* **saturation** /ËŒsÃ¦tÊƒÉ™ËˆreÉªÊƒn/ï¼šé¥±å’Œ
* **pointed out** ğŸ”¥ï¼šæŒ‡å‡ºï¼ˆpoint out = æè¯·æ³¨æ„ï¼‰
* **initial runs**ï¼šåˆå§‹æµ‹è¯•è¿è¡Œ

---

(25) [9:56-10:21] **Which was that if you look at the state-of-the-art open weights models, Ultravox being one of them, you've actually closed that gap with GPT-4o and Gemini 2.5 Flash on this production-shaped benchmark. And that's super interesting. And that gave me a lot of interesting stuff to think about â€” while washing the dishes and doing my morning run â€” about what I think 2026 is going to look like from an open weights model perspective. And I'll throw that back to Brooke. I'm actually really excited about potential progress in open weights models.**

å°±æ˜¯å¦‚æœä½ çœ‹é‚£äº›æœ€å…ˆè¿›çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼Œ**Ultravox** å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼Œä½ ä»¬å®é™…ä¸Šåœ¨è¿™ä¸ªé¢å‘ç”Ÿäº§åœºæ™¯çš„åŸºå‡†æµ‹è¯•ä¸Šå·²ç»ç¼©å°äº†å’Œ **GPT-4o** ä¸ **Gemini 2.5 Flash** çš„å·®è·ã€‚è¿™éå¸¸æœ‰è¶£ï¼Œè®©æˆ‘åœ¨æ´—ç¢—å’Œæ™¨è·‘æ—¶æƒ³äº†å¾ˆå¤šï¼š2026 å¹´ä»å¼€æ”¾æƒé‡æ¨¡å‹çš„è§’åº¦æ¥çœ‹ä¼šæ˜¯ä»€ä¹ˆæ ·ï¼ŸæŠŠè¯é¢˜æŠ›å›ç»™ **Brooke**ï¼Œæˆ‘å¯¹å¼€æ”¾æƒé‡æ¨¡å‹çš„æ½œåœ¨è¿›æ­¥çœŸçš„å¾ˆæœŸå¾…ã€‚

è§£æï¼š
* **state-of-the-art** ğŸ”¥ï¼šæœ€å…ˆè¿›çš„ï¼ˆæŠ€æœ¯é¢†åŸŸé«˜é¢‘ç”¨è¯­ï¼‰
* **open weights model**ï¼šå¼€æ”¾æƒé‡æ¨¡å‹ï¼ˆå…¬å¼€æ¨¡å‹æƒé‡ä¾›ç¤¾åŒºä½¿ç”¨çš„æ¨¡å‹ï¼‰
* **closed that gap** ğŸ”¥ï¼šç¼©å°äº†å·®è·
* **production-shaped**ï¼šé¢å‘ç”Ÿäº§åœºæ™¯çš„
* **throw that back to** ğŸ”¥ï¼šæŠŠè¯é¢˜æŠ›å›ç»™ï¼ˆå¯¹è¯ä¸­å¸¸ç”¨çš„è½¬åœºè¡¨è¾¾ï¼‰

---

(26) [10:21-10:51] **Yeah, definitely. I mean, I think that's exactly what we're seeing is it's hard to say sometimes if people are stuck just because it's so tricky to switch models. I think even more so than other agentic applications because you have so many models that are all in concert together. You're not just seeing, okay, is it going to perform as I expect with the prompts that I have or with the data that I have, but you're also then seeing how it interacts with all these other models.**

å¯¹çš„ï¼Œç¡®å®å¦‚æ­¤ã€‚æœ‰æ—¶å€™å¾ˆéš¾è¯´äººä»¬æ˜¯ä¸æ˜¯åªæ˜¯å› ä¸ºåˆ‡æ¢æ¨¡å‹å¤ªéº»çƒ¦æ‰åœç•™åœ¨åŸåœ°ã€‚æˆ‘è§‰å¾—è¯­éŸ³ AI æ¯”å…¶ä»–æ™ºèƒ½ä½“åº”ç”¨æ›´ç”šï¼Œå› ä¸ºä½ æœ‰å¤ªå¤šæ¨¡å‹éœ€è¦ååŒå·¥ä½œã€‚ä½ ä¸ä»…è¦çœ‹å®ƒåœ¨ä½ çš„æç¤ºè¯å’Œæ•°æ®ä¸‹æ˜¯å¦è¡¨ç°å¦‚é¢„æœŸï¼Œè¿˜è¦çœ‹å®ƒæ€ä¹ˆè·Ÿæ‰€æœ‰å…¶ä»–æ¨¡å‹äº’åŠ¨é…åˆã€‚

è§£æï¼š
* **stuck**ï¼šå¡ä½äº†ã€åœç•™åœ¨åŸåœ°
* **agentic application**ï¼šæ™ºèƒ½ä½“åº”ç”¨
* **in concert** ğŸ”¥ï¼šååŒã€ä¸€èµ·ï¼ˆin concert = åè°ƒé…åˆï¼‰
* **interact with**ï¼šä¸â€¦äº’åŠ¨

---

(27) [10:51-11:15] **And on top of that, I think the testing is much more expensive. The eval process that people go through, I think oftentimes is very manual. And so being able to change those models, there's a lot higher barrier to entry. And so I think that's why I'm also so excited about the benchmarks â€” being able to quantify what are those gains, especially if you're like, oh, I've heard there's so much that happens just from hearsay.**

é™¤æ­¤ä¹‹å¤–ï¼Œæµ‹è¯•æˆæœ¬é«˜å¾—å¤šã€‚äººä»¬çš„è¯„ä¼°æµç¨‹å¾€å¾€éå¸¸æ‰‹åŠ¨ã€‚æ‰€ä»¥è¦åˆ‡æ¢æ¨¡å‹ï¼Œè¿›å…¥é—¨æ§›é«˜å¾ˆå¤šã€‚è¿™ä¹Ÿæ˜¯æˆ‘å¯¹åŸºå‡†æµ‹è¯•å¦‚æ­¤å…´å¥‹çš„åŸå› â€”â€”èƒ½å¤Ÿé‡åŒ–é‚£äº›å¢ç›Šï¼Œå°¤å…¶æ˜¯å½“ä½ åªæ˜¯å¬åˆ°å„ç§é“å¬é€”è¯´çš„æ—¶å€™ã€‚

è§£æï¼š
* **on top of that** ğŸ”¥ï¼šé™¤æ­¤ä¹‹å¤–ã€è€Œä¸”
* **barrier to entry** ğŸ”¥ï¼šè¿›å…¥é—¨æ§›ã€å‡†å…¥å£å’
* **quantify** /ËˆkwÉ‘ËntÉªfaÉª/ï¼šé‡åŒ–ï¼ˆç”¨æ•°æ®è¡¡é‡ï¼‰
* **hearsay** /ËˆhÉªrseÉª/ï¼šé“å¬é€”è¯´ã€ä¼ é—»

---

(28) [11:15-11:48] **I've heard a lot of people are using this model or that model. I think this definitely happened with Gemini. A lot of people switched to Gemini mostly because of what everyone was talking about â€” in group chats or on podcasts or on LinkedIn â€” that is truly how a lot of people switched. But being able to quantify it not just in your own evals but in these public benchmarks I think is going to be really important.**

æˆ‘å¬è¯´å¾ˆå¤šäººåœ¨ç”¨è¿™ä¸ªæ¨¡å‹æˆ–é‚£ä¸ªæ¨¡å‹ã€‚æˆ‘è§‰å¾— **Gemini** å°±æ˜¯å…¸å‹â€”â€”å¾ˆå¤šäººåˆ‡æ¢åˆ° **Gemini** ä¸»è¦æ˜¯å› ä¸ºå¤§å®¶éƒ½åœ¨è®¨è®ºå®ƒï¼Œåœ¨ç¾¤èŠé‡Œã€æ’­å®¢ä¸Šã€**LinkedIn** ä¸Šâ€”â€”è¿™ç¡®å®å°±æ˜¯å¾ˆå¤šäººåˆ‡æ¢æ¨¡å‹çš„æ–¹å¼ã€‚ä½†èƒ½å¤Ÿä¸åªåœ¨è‡ªå·±çš„è¯„æµ‹ä¸­ã€è€Œæ˜¯åœ¨è¿™äº›å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­é‡åŒ–æ•ˆæœï¼Œæˆ‘è§‰å¾—ä¼šéå¸¸é‡è¦ã€‚

è§£æï¼š
* **switched to**ï¼šåˆ‡æ¢åˆ°ã€è½¬ç”¨
* **that is truly how**ï¼šè¿™ç¡®å®å°±æ˜¯â€¦çš„æ–¹å¼ï¼ˆå¼ºè°ƒçœŸå®æƒ…å†µï¼‰

---

(29) [11:48-12:19] **But yeah, I think in terms of the frontier of open weights models, creating more models that either specialize in certain areas â€” today we're still seeing just repurposing models that are made for text on the internet, repurposing that for voice AI. I think there's the whole frontier of how can we create really voice-first models, voice-first experiences, even if it's just on the text and reasoning layer. I think that's one area that I'm super excited about.**

åœ¨å¼€æ”¾æƒé‡æ¨¡å‹å‰æ²¿æ–¹é¢ï¼Œæˆ‘è§‰å¾—æ— è®ºæ˜¯åˆ›é€ ä¸“ç²¾æŸäº›é¢†åŸŸçš„æ¨¡å‹â€”â€”ä»Šå¤©æˆ‘ä»¬çœ‹åˆ°çš„è¿˜æ˜¯æŠŠä¸ºäº’è”ç½‘æ–‡æœ¬åˆ¶ä½œçš„æ¨¡å‹æ”¹é€ ç”¨äºè¯­éŸ³ AIã€‚æˆ‘è§‰å¾—çœŸæ­£çš„å‰æ²¿æ˜¯ï¼šæˆ‘ä»¬æ€ä¹ˆåˆ›é€ çœŸæ­£è¯­éŸ³ä¼˜å…ˆçš„æ¨¡å‹å’Œä½“éªŒï¼Ÿå³ä½¿åªæ˜¯åœ¨æ–‡æœ¬å’Œæ¨ç†å±‚é¢ã€‚è¿™æ˜¯æˆ‘éå¸¸æœŸå¾…çš„ä¸€ä¸ªé¢†åŸŸã€‚

è§£æï¼š
* **specialize in**ï¼šä¸“ç²¾äºã€ä¸“æ³¨äº
* **repurposing** /riËËˆpÉœËrpÉ™sÉªÅ‹/ ğŸ”¥ï¼šæ”¹é€ ç”¨é€”ã€é‡æ–°åˆ©ç”¨ï¼ˆæŠŠåŸæœ¬ä¸ºAè®¾è®¡çš„ä¸œè¥¿ç”¨äºBï¼‰
* **voice-first** ğŸ”¥ï¼šè¯­éŸ³ä¼˜å…ˆï¼ˆè®¾è®¡ç†å¿µï¼Œä»¥è¯­éŸ³äº¤äº’ä¸ºé¦–è¦è€ƒé‡ï¼‰

---

(30) [12:19-12:52] **And I do want to ask Zach, how you closed that gap which seemed so hard to work around and close to those of us building voice agents, even six months ago â€” where the open-ended conversational capabilities of the models were so great and they felt very humanlike when you're talking to them, but it was extremely hard to prompt and context-engineer them to get the instruction following and the function calling up to production quality. And I remember talking to you after the Llama 4 launch when we were sharing a car ride back to San Francisco.**

æˆ‘è¿˜æƒ³é—® **Zach**ï¼Œä½ ä»¬æ˜¯æ€ä¹ˆç¼©å°é‚£ä¸ªå·®è·çš„ï¼Ÿå¯¹äºæˆ‘ä»¬è¿™äº›åšè¯­éŸ³ä»£ç†çš„äººæ¥è¯´ï¼Œå³ä½¿å…­ä¸ªæœˆå‰è¿™çœ‹èµ·æ¥éƒ½æéš¾çªç ´â€”â€”æ¨¡å‹çš„å¼€æ”¾å¼å¯¹è¯èƒ½åŠ›å¾ˆå¼ºï¼Œè·Ÿå®ƒè¯´è¯æ„Ÿè§‰å¾ˆåƒäººç±»ï¼Œä½†è¦é€šè¿‡æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å·¥ç¨‹è®©æŒ‡ä»¤éµå¾ªå’Œå‡½æ•°è°ƒç”¨è¾¾åˆ°ç”Ÿäº§è´¨é‡ï¼Œéš¾åº¦æå¤§ã€‚æˆ‘è®°å¾— **Llama 4** å‘å¸ƒåæˆ‘ä»¬ä¸€èµ·åè½¦å›æ—§é‡‘å±±æ—¶èŠè¿‡ã€‚

è§£æï¼š
* **close the gap** ğŸ”¥ï¼šç¼©å°å·®è·
* **open-ended conversational**ï¼šå¼€æ”¾å¼å¯¹è¯çš„
* **context-engineer** ğŸ”¥ï¼šä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆé€šè¿‡ç²¾å¿ƒè®¾è®¡ä¸Šä¸‹æ–‡æ¥ä¼˜åŒ–AIè¡¨ç°ï¼‰
* **production quality**ï¼šç”Ÿäº§è´¨é‡ï¼ˆè¾¾åˆ°å¯å•†ç”¨çš„æ°´å¹³ï¼‰

---

(31) [12:52-13:21] **And saying like I really thought you just had to do a ton of context engineering and you gently pushed back and said you thought you could solve that at the model level and I was super skeptical. But I have to say I think you've solved a big chunk of that at the model level. Is that training data? Is it model architecture? Is it something else? What helped you close that gap? Well, candidly I think some of it's just also building on top of the really great work that open weight models are now doing.**

æˆ‘å½“æ—¶è§‰å¾—ä½ åªèƒ½åšå¤§é‡ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼Œä½ æ¸©å’Œåœ°åé©³è¯´ä½ è§‰å¾—èƒ½åœ¨æ¨¡å‹å±‚é¢è§£å†³ï¼Œæˆ‘å½“æ—¶è¶…çº§æ€€ç–‘ã€‚ä½†æˆ‘å¾—æ‰¿è®¤ä½ ä»¬åœ¨æ¨¡å‹å±‚é¢ç¡®å®è§£å†³äº†å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚æ˜¯è®­ç»ƒæ•°æ®çš„åŠŸåŠ³ï¼Ÿæ¨¡å‹æ¶æ„ï¼Ÿè¿˜æ˜¯å…¶ä»–ä»€ä¹ˆï¼Ÿä½ ä»¬æ€ä¹ˆç¼©å°å·®è·çš„ï¼Ÿå¦ç™½è¯´ï¼Œæˆ‘è§‰å¾—ä¸€éƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬ç«™åœ¨å¼€æ”¾æƒé‡æ¨¡å‹å‡ºè‰²å·¥ä½œçš„è‚©è†€ä¸Šã€‚

è§£æï¼š
* **pushed back** ğŸ”¥ï¼šåé©³ã€æå‡ºå¼‚è®®ï¼ˆæ¸©å’Œåœ°ä¸åŒæ„ï¼‰
* **skeptical** /ËˆskeptÉªkl/ï¼šæ€€ç–‘çš„
* **a big chunk of**ï¼šå¾ˆå¤§ä¸€éƒ¨åˆ†
* **candidly** /ËˆkÃ¦ndÉªdli/ï¼šå¦ç™½åœ°ã€å¦ç‡åœ°
* **building on top of** ğŸ”¥ï¼šåœ¨â€¦åŸºç¡€ä¸Šæ„å»º

---

(32) [13:21-13:45] **I mean, forget about the speech side for just a second. I think that we saw tremendous growth in open weight models and really the credit goes to a lot of the Chinese labs that are really pushing the frontier of open weight models. And so I think they get a lot of the credit for helping push this forward. And so what we do is we take a lot of that and then we build on top of it.**

å…ˆä¸è¯´è¯­éŸ³æ–¹é¢ï¼Œå¼€æ”¾æƒé‡æ¨¡å‹çš„å¢é•¿æ˜¯å·¨å¤§çš„ï¼ŒåŠŸåŠ³çœŸçš„è¦å½’åŠŸäºå¾ˆå¤šä¸­å›½å®éªŒå®¤â€”â€”å®ƒä»¬åœ¨æ¨åŠ¨å¼€æ”¾æƒé‡æ¨¡å‹çš„å‰æ²¿ã€‚æ‰€ä»¥æˆ‘è§‰å¾—å®ƒä»¬ä¸ºæ¨åŠ¨è¡Œä¸šè¿›æ­¥è´¡çŒ®äº†å¾ˆå¤šã€‚æˆ‘ä»¬åšçš„å°±æ˜¯åœ¨è¿™äº›åŸºç¡€ä¸Šå†å¾€ä¸Šæ„å»ºã€‚

è§£æï¼š
* **tremendous** /trÉªËˆmendÉ™s/ï¼šå·¨å¤§çš„ã€æƒŠäººçš„
* **the credit goes to** ğŸ”¥ï¼šåŠŸåŠ³å½’äºâ€¦
* **pushing the frontier**ï¼šæ¨åŠ¨å‰æ²¿

---

(33) [13:45-14:09] **And I think what we have been trying to solve at Ultravox is â€” we've long been believers in speech-native models and the importance of those architectures for achieving increasingly humanlike conversation. So we've got to remove the ASR step. We've got to remove all these things. And so what we've spent a lot of our time doing is like, okay, how do you add speech as a modality without making the model dumber? That's actually I think an extremely hard problem.**

æˆ‘ä»¬åœ¨ **Ultravox** ä¸€ç›´è‡´åŠ›äºè§£å†³çš„æ˜¯â€”â€”æˆ‘ä»¬é•¿æœŸä»¥æ¥åšä¿¡è¯­éŸ³åŸç”Ÿæ¨¡å‹ä»¥åŠè¿™ç§æ¶æ„å¯¹å®ç°è¶Šæ¥è¶Šåƒäººç±»å¯¹è¯çš„é‡è¦æ€§ã€‚æ‰€ä»¥æˆ‘ä»¬å¿…é¡»å»æ‰ **ASR** æ­¥éª¤ã€å»æ‰è¿™äº›ä¸­é—´ç¯èŠ‚ã€‚æˆ‘ä»¬èŠ±äº†å¤§é‡æ—¶é—´æ€è€ƒï¼šæ€ä¹ˆæŠŠè¯­éŸ³ä½œä¸ºä¸€ç§æ¨¡æ€åŠ è¿›å»ï¼ŒåŒæ—¶ä¸è®©æ¨¡å‹å˜ç¬¨ï¼Ÿè¿™æ˜¯ä¸€ä¸ªæå…¶å›°éš¾çš„é—®é¢˜ã€‚

è§£æï¼š
* **speech-native model** ğŸ”¥ï¼šè¯­éŸ³åŸç”Ÿæ¨¡å‹ï¼ˆç›´æ¥å¤„ç†è¯­éŸ³è€Œéå…ˆè½¬æ–‡å­—ï¼‰
* **ASR**ï¼šAutomatic Speech Recognitionï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«
* **modality** /moÊŠËˆdÃ¦lÉ™ti/ï¼šæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒç­‰ä¸åŒè¾“å…¥è¾“å‡ºå½¢å¼ï¼‰
* **making the model dumber**ï¼šè®©æ¨¡å‹å˜ç¬¨ï¼ˆå£è¯­åŒ–è¡¨è¾¾èƒ½åŠ›ä¸‹é™ï¼‰

---

(34) [14:09-14:34] **Because then you look at â€” why does GPT Realtime perform so much worse in some sense compared to the GPT-4o model? Well, it turns out that there's a lot of nuance in how you approach modality and there's trade-offs. And so I think for us a lot of it was building on top of those right foundations and then over the last year and a half solving a lot of interesting model layer and model training challenges for how we can add the speech modality without impacting the reasoning abilities of the model.**

å› ä¸ºä½ çœ‹â€”â€”ä¸ºä»€ä¹ˆ **GPT Realtime** åœ¨æŸäº›æ–¹é¢æ¯” **GPT-4o** å·®è¿™ä¹ˆå¤šï¼Ÿäº‹å®ä¸Šåœ¨å¤„ç†æ¨¡æ€çš„æ–¹å¼ä¸Šæœ‰å¾ˆå¤šç»†å¾®å·®åˆ«å’Œå–èˆã€‚æ‰€ä»¥å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œå¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯å»ºç«‹åœ¨æ­£ç¡®çš„åŸºç¡€ä¹‹ä¸Šï¼Œç„¶ååœ¨è¿‡å»ä¸€å¹´åŠé‡Œè§£å†³äº†å¾ˆå¤šæ¨¡å‹å±‚å’Œæ¨¡å‹è®­ç»ƒçš„æœ‰è¶£æŒ‘æˆ˜â€”â€”å¦‚ä½•åœ¨æ·»åŠ è¯­éŸ³æ¨¡æ€çš„åŒæ—¶ä¸å½±å“æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

è§£æï¼š
* **nuance** /ËˆnuËÉ‘Ëns/ ğŸ”¥ï¼šç»†å¾®å·®åˆ«ã€å¾®å¦™ä¹‹å¤„
* **GPT Realtime**ï¼šOpenAI çš„å®æ—¶è¯­éŸ³æ¨¡å‹
* **reasoning abilities**ï¼šæ¨ç†èƒ½åŠ›

---

(35) [14:34-14:56] **And honestly it all kind of came together at the end of Q4. It felt like we'd had Ultravox models all throughout 2025, but it wasn't until the end of the year in Q4 that I felt like we really had all the pieces put together and we finally cracked some of the nuts on the training run. And that's why I saw â€” oh, this would be a good test, this eval that Quinn has built, let's see how we do.**

è¯´å®è¯ä¸€åˆ‡åœ¨ Q4 æœ«ç»ˆäºèåˆåˆ°ä¸€èµ·äº†ã€‚æ„Ÿè§‰æˆ‘ä»¬æ•´ä¸ª 2025 å¹´ä¸€ç›´æœ‰ **Ultravox** æ¨¡å‹ï¼Œä½†ç›´åˆ°å¹´åº• Q4 æˆ‘æ‰è§‰å¾—æ‰€æœ‰æ‹¼å›¾ç»ˆäºæ‹¼å¥½äº†ï¼Œæˆ‘ä»¬ç»ˆäºåœ¨è®­ç»ƒä¸Šæ”»å…‹äº†ä¸€äº›éš¾å…³ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘çœ‹åˆ°åæƒ³â€”â€”å“¦ï¼Œ**Quinn** åšçš„è¿™ä¸ªè¯„æµ‹æ˜¯ä¸ªå¾ˆå¥½çš„æ£€éªŒï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¡¨ç°å¦‚ä½•ã€‚

è§£æï¼š
* **came together** ğŸ”¥ï¼šèåˆåˆ°ä¸€èµ·ã€å„éƒ¨åˆ†ç»ˆäºé…åˆå¥½ï¼ˆå¸¸ç”¨æ¥å½¢å®¹çªç ´æ—¶åˆ»ï¼‰
* **Q4**ï¼šç¬¬å››å­£åº¦ï¼ˆQuarter 4ï¼Œ10-12æœˆï¼‰
* **cracked some of the nuts** ğŸ”¥ï¼šæ”»å…‹äº†ä¸€äº›éš¾å…³ï¼ˆcrack the nut = è§£å†³éš¾é¢˜ï¼‰
* **training run**ï¼šè®­ç»ƒè½®æ¬¡

---

(36) [14:56-15:09] **Because honestly it's like â€” on some level you said it was a pretty hard eval. I look at it now through the lens of 2026 models â€” I'm like, that's not that hard of an eval, you know. But it's amazing that it kind of is still. And so, but yeah, I think that our big stick too has always been that â€” okay, you can't just if you want the speed, you also can't just train the model. You have to run the inference layer.**

å› ä¸ºå¦ç™½è¯´â€”â€”ä½ è¯´è¿™æ˜¯ä¸€ä¸ªç›¸å½“éš¾çš„è¯„æµ‹ã€‚æˆ‘ç°åœ¨ç”¨ 2026 å¹´æ¨¡å‹çš„çœ¼å…‰æ¥çœ‹â€”â€”å…¶å®æ²¡é‚£ä¹ˆéš¾ã€‚ä½†ä»¤äººæƒŠè®¶çš„æ˜¯å®ƒæŸç§ç¨‹åº¦ä¸Šä»ç„¶æ˜¯éš¾çš„ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸€ç›´åšæŒçš„æ ¸å¿ƒç†å¿µæ˜¯â€”â€”å¦‚æœä½ è¦é€Ÿåº¦ï¼Œä½ ä¸èƒ½åªè®­ç»ƒæ¨¡å‹ï¼Œä½ è¿˜å¾—æå®šæ¨ç†å±‚ã€‚

è§£æï¼š
* **through the lens of** ğŸ”¥ï¼šä»â€¦çš„è§’åº¦/è§†è§’çœ‹ï¼ˆéå¸¸åœ°é“çš„è¡¨è¾¾ï¼‰
* **big stick**ï¼šæ ¸å¿ƒæ­¦å™¨ã€å…³é”®ä¼˜åŠ¿
* **inference layer**ï¼šæ¨ç†å±‚ï¼ˆè¿è¡Œæ¨¡å‹è¿›è¡Œé¢„æµ‹çš„æŠ€æœ¯æ ˆï¼‰

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ˜¯ä¸€åœºå…³äº 2026 å¹´è¯­éŸ³ AI ç°çŠ¶çš„åœ†æ¡Œè®¨è®ºï¼Œä¸‰ä½å˜‰å®¾åˆ†åˆ«æ¥è‡ª **Cobalt**ï¼ˆè¯„ä¼°ï¼‰ã€**Ultravox AI**ï¼ˆè¯­éŸ³æ¨¡å‹ï¼‰å’Œ **Daily**ï¼ˆåŸºç¡€è®¾æ–½ï¼‰ã€‚æ ¸å¿ƒè¯é¢˜æ˜¯ï¼šè¯­éŸ³ AI ä¸­çš„æŒ‡ä»¤éµå¾ªå’Œå‡½æ•°è°ƒç”¨å¯é æ€§æ˜¯å½“å‰æœ€å¤§çš„æŒ‘æˆ˜ï¼›å‰æ²¿æ¨ç†æ¨¡å‹è™½ç„¶èƒ½åŠ›å¼ºä½†å»¶è¿Ÿå¤ªé«˜æ— æ³•ç”¨äºè¯­éŸ³åœºæ™¯ï¼Œå¯¼è‡´ç”Ÿäº§ç¯å¢ƒä»ä»¥ **GPT-4o** å’Œ **Gemini 2.5 Flash** ä¸ºä¸»ï¼›è€Œ **Ultravox** ç­‰å¼€æ”¾æƒé‡æ¨¡å‹é€šè¿‡è¯­éŸ³åŸç”Ÿæ¶æ„æˆåŠŸç¼©å°äº†å·®è·ï¼Œè®©äººå¯¹ 2026 å¹´çš„è¯­éŸ³ AI å‰æ™¯å……æ»¡æœŸå¾…ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **instruction following** | æŒ‡ä»¤éµå¾ªï¼ŒAIæŒ‰æŒ‡ä»¤æ‰§è¡Œçš„èƒ½åŠ› |
| **benchmark** | åŸºå‡†æµ‹è¯• |
| **deep dive** | æ·±å…¥æ¢è®¨ |
| **across the board** | å…¨é¢åœ°ã€å…¨æ–¹ä½ |
| **function calling** | å‡½æ•°è°ƒç”¨ |
| **turn-taking** | è½®æ›¿ã€è¯è½®è½¬æ¢ |
| **latency** | å»¶è¿Ÿ |
| **trade-off** | æƒè¡¡ã€å–èˆ |
| **saturate** | é¥±å’Œã€æ‰“æ»¡åˆ†æ•°ä¸Šé™ |
| **benchmark maxing** | åˆ·æ¦œ |
| **compare apples to apples** | å…¬å¹³å¯¹æ¯” |
| **no easy feat** | ç»éæ˜“äº‹ |
| **out of the box** | å¼€ç®±å³ç”¨ |
| **by far** | ç›®å‰ä¸ºæ­¢æœ€â€¦ |
| **state-of-the-art** | æœ€å…ˆè¿›çš„ |
| **open weights model** | å¼€æ”¾æƒé‡æ¨¡å‹ |
| **close the gap** | ç¼©å°å·®è· |
| **repurposing** | æ”¹é€ ç”¨é€” |
| **voice-first** | è¯­éŸ³ä¼˜å…ˆ |
| **speech-native model** | è¯­éŸ³åŸç”Ÿæ¨¡å‹ |
| **modality** | æ¨¡æ€ |
| **ASR** | è‡ªåŠ¨è¯­éŸ³è¯†åˆ« |
| **nuance** | ç»†å¾®å·®åˆ« |
| **through the lens of** | ä»â€¦çš„è§’åº¦çœ‹ |
| **candidly** | å¦ç™½åœ° |
| **hearsay** | é“å¬é€”è¯´ |
| **barrier to entry** | è¿›å…¥é—¨æ§› |
| **proxy** | ä»£ç†æŒ‡æ ‡ |
| **inference layer** | æ¨ç†å±‚ |
| **cracked some of the nuts** | æ”»å…‹äº†éš¾å…³ |
| **came together** | èåˆåˆ°ä¸€èµ· |
| **tee it up** | å¼•å‡ºè¯é¢˜ |
| **sticking with** | åšæŒä½¿ç”¨ |
| **in the weeds** | æ·±å…¥ç»†èŠ‚ |
