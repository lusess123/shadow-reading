# ğŸ¯ è¯­éŸ³AIæ¨ç†ä¸è¯„æµ‹å¯¹è°ˆ è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **63 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

> ğŸ“ è¿™æ˜¯ä¸€æœŸæ’­å®¢å¯¹è°ˆï¼Œä¸»è¦è®¨è®ºè¯­éŸ³ AI çš„æ¨ç†æœåŠ¡ã€æ¨¡å‹è¯„æµ‹ã€æ€è€ƒå‹æ¨¡å‹çš„æŒ‘æˆ˜ä»¥åŠ 2026 å¹´çš„æŠ€æœ¯è¶‹åŠ¿ã€‚å˜‰å®¾åŒ…æ‹¬ **Zach**ï¼ˆè¯­éŸ³æ¨¡å‹å…¬å¸ï¼‰ã€**Quinn**ï¼ˆ**Pipecat** æ¡†æ¶ï¼‰å’Œ **Brooke**ï¼ˆè¯„æµ‹å¹³å°ï¼‰ã€‚

---

(1) [15:09-15:22] **Okay, you can't justâ€” if you want the speed, you also can't just train the model. You have to run the inference layer. We haven't figured out how to outsource inference to someone else whose primary job is not real time inference but also tech stuff and not seen massive sort of like very widespread on latency numbers.**

å¥½çš„ï¼Œå¦‚æœä½ æƒ³è¦é€Ÿåº¦ï¼Œä½ ä¸èƒ½åªæ˜¯è®­ç»ƒæ¨¡å‹å°±å®Œäº‹äº†ã€‚ä½ è¿˜å¾—è‡ªå·±è·‘æ¨ç†å±‚ã€‚æˆ‘ä»¬è¿˜æ²¡æ‰¾åˆ°åŠæ³•æŠŠæ¨ç†å¤–åŒ…ç»™é‚£äº›ä¸»ä¸šä¸æ˜¯åšå®æ—¶æ¨ç†çš„äººâ€”â€”è™½ç„¶ä»–ä»¬ä¹ŸåšæŠ€æœ¯ç›¸å…³çš„äº‹ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°å»¶è¿Ÿæ•°æ®ä¸Šæœ‰å¤§å¹…æ”¹å–„ã€‚

è§£æï¼š
* **inference layer** /ËˆÉªnfÉ™rÉ™ns/ï¼šæ¨ç†å±‚ï¼Œæ¨¡å‹è®­ç»ƒå®Œåå®é™…æ‰§è¡Œé¢„æµ‹çš„éƒ¨åˆ† ğŸ”¥
* **outsource** /ËˆaÊŠtsÉ”Ërs/ï¼šåŠ¨è¯ï¼Œå¤–åŒ…
* **latency** /ËˆleÉªtÉ™nsi/ï¼šåè¯ï¼Œå»¶è¿Ÿï¼ˆç½‘ç»œ/ç³»ç»Ÿå“åº”æ—¶é—´ï¼‰ğŸ”¥
* **widespread**ï¼šå½¢å®¹è¯ï¼Œå¹¿æ³›çš„ã€å¤§èŒƒå›´çš„

---

(2) [15:22-15:41] **And I suspect if we were to sort of break down and graph out a lot of those requests on the text layer what we would see are very wide distributions like time to first token numbers for example just because you don't know where you sit in the queue and stuff like that. So I think for us it's a combination of the modeling work and then figuring out some of the serving techniques as well that hopefully allowed us to kind of close both the latency gaps but then also the intelligence gaps.**

æˆ‘æ€€ç–‘å¦‚æœæˆ‘ä»¬æŠŠæ–‡æœ¬å±‚çš„å¾ˆå¤šè¯·æ±‚æ‹†è§£å‡ºæ¥ç”»æˆå›¾è¡¨ï¼Œä¼šçœ‹åˆ°éå¸¸å¤§çš„åˆ†å¸ƒå·®å¼‚â€”â€”æ¯”å¦‚é¦–ä¸ª token çš„å“åº”æ—¶é—´ï¼Œå› ä¸ºä½ ä¸çŸ¥é“è‡ªå·±åœ¨é˜Ÿåˆ—é‡Œæ’åœ¨å“ªã€‚æ‰€ä»¥æˆ‘è§‰å¾—å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™æ˜¯æ¨¡å‹ç ”å‘å’Œæ¨ç†æœåŠ¡æŠ€æœ¯çš„ç»“åˆï¼Œå¸Œæœ›èƒ½åŒæ—¶ç¼©å°å»¶è¿Ÿå·®è·å’Œæ™ºèƒ½å·®è·ã€‚

è§£æï¼š
* **time to first token (TTFT)**ï¼šé¦–ä¸ª token å“åº”æ—¶é—´ï¼Œè¡¡é‡æ¨¡å‹ç”Ÿæˆç¬¬ä¸€ä¸ªå­—çš„é€Ÿåº¦ ğŸ”¥
* **distribution** /ËŒdÉªstrÉªËˆbjuËÊƒÉ™n/ï¼šåè¯ï¼Œåˆ†å¸ƒï¼ˆç»Ÿè®¡å­¦æ¦‚å¿µï¼‰
* **queue** /kjuË/ï¼šåè¯ï¼Œé˜Ÿåˆ—
* **serving techniques**ï¼šæ¨ç†æœåŠ¡æŠ€æœ¯ï¼ŒæŒ‡æ¨¡å‹éƒ¨ç½²åå¦‚ä½•é«˜æ•ˆæä¾›æœåŠ¡ ğŸ”¥

---

(3) [15:46-16:06] **Well, I don't know if you've seen this as well, Brooke, but what we saw is that for a long time, OpenAI was incredibly impressive in their not having a spread between P50 and P90 on GPT-4o latency TTFT. And I was always in awe of the ability of OpenAI to run this like global supercomputer with very tight tolerances.**

**Brooke**ï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰æ³¨æ„åˆ°ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯ï¼Œå¾ˆé•¿ä¸€æ®µæ—¶é—´é‡Œ **OpenAI** åœ¨ **GPT-4o** çš„é¦– token å»¶è¿Ÿä¸Šï¼Œ**P50** å’Œ **P90** ä¹‹é—´å‡ ä¹æ²¡æœ‰å·®è·ï¼Œè¿™çœŸçš„éå¸¸å‰å®³ã€‚æˆ‘ä¸€ç›´å¾ˆä½©æœ **OpenAI** èƒ½è¿è¡Œè¿™æ ·ä¸€ä¸ªå…¨çƒè¶…çº§è®¡ç®—æœºï¼Œè¿˜èƒ½ä¿æŒè¿™ä¹ˆç´§çš„å®¹å·®ã€‚

è§£æï¼š
* **P50 / P90**ï¼šç™¾åˆ†ä½å»¶è¿ŸæŒ‡æ ‡ï¼ŒP50 æ˜¯ä¸€åŠè¯·æ±‚çš„å»¶è¿Ÿä¸Šé™ï¼ŒP90 æ˜¯ 90% è¯·æ±‚çš„å»¶è¿Ÿä¸Šé™ ğŸ”¥
* **spread**ï¼šåè¯ï¼Œå·®è·ã€èŒƒå›´
* **in awe of**ï¼šçŸ­è¯­ï¼Œå¯¹...æ„Ÿåˆ°æ•¬ç•
* **tight tolerances**ï¼šç´§å¯†çš„å®¹å·®ï¼ŒæŒ‡æ€§èƒ½æ³¢åŠ¨å¾ˆå° ğŸ”¥

---

(4) [16:06-16:24] **But I think as AI has just grown and there's been so much demand for inference, everybody's had to make different compromises because we really don't see that consistency anymore from any of the text models, including the Gemini models, which also can be very fast, but are no longer always very fast for us in production.**

ä½†æˆ‘è§‰å¾—éšç€ AI çš„çˆ†å‘å¼å¢é•¿å’Œæ¨ç†éœ€æ±‚æ¿€å¢ï¼Œå¤§å®¶éƒ½ä¸å¾—ä¸åšå‡ºå„ç§å¦¥åï¼Œå› ä¸ºæˆ‘ä»¬ç¡®å®å·²ç»çœ‹ä¸åˆ°ä»»ä½•æ–‡æœ¬æ¨¡å‹è¿˜èƒ½ä¿æŒé‚£ç§ä¸€è‡´æ€§äº†â€”â€”åŒ…æ‹¬ **Gemini** æ¨¡å‹ï¼Œå®ƒä»¬æœ‰æ—¶ç¡®å®å¾ˆå¿«ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å·²ç»ä¸èƒ½ä¸€ç›´ä¿æŒå¾ˆå¿«äº†ã€‚

è§£æï¼š
* **compromise** /ËˆkÉ’mprÉ™maÉªz/ï¼šåè¯/åŠ¨è¯ï¼Œå¦¥å
* **consistency** /kÉ™nËˆsÉªstÉ™nsi/ï¼šåè¯ï¼Œä¸€è‡´æ€§ã€ç¨³å®šæ€§
* **in production**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ˆç›¸å¯¹äºæµ‹è¯•/å¼€å‘ç¯å¢ƒï¼‰ğŸ”¥

---

(5) [16:24-16:50] **Yeah. And I mean, I think I'm definitely betting that Google gets there though because Google is just the king of compute. I think that's been their bread and butter for the past 20 years. And so I think just the maturity of these models is so young. I think I was just talking to Scott about thisâ€” the reliability that they've been able to achieve with deep ground models is because of just how long they've been working on these models. A lot of that reliability just comes from years and years of fine-tuning and optimizing and getting them to this like hyper optimized state.**

æ˜¯çš„ï¼Œæˆ‘è‚¯å®šè¿˜æ˜¯çœ‹å¥½ **Google** èƒ½åšåˆ°çš„ï¼Œå› ä¸º **Google** å°±æ˜¯ç®—åŠ›ä¹‹ç‹å˜›ï¼Œè¿™æ˜¯ä»–ä»¬è¿‡å» 20 å¹´çš„çœ‹å®¶æœ¬é¢†ã€‚æˆ‘è§‰å¾—è¿™äº›æ¨¡å‹çš„æˆç†Ÿåº¦è¿˜å¤ªå¹´è½»äº†ã€‚æˆ‘åˆšå’Œ **Scott** èŠåˆ°è¿™ä¸ªâ€”â€”ä»–ä»¬åœ¨æ·±åº¦å­¦ä¹ åŸºç¡€æ¨¡å‹ä¸Šè¾¾åˆ°çš„å¯é æ€§ï¼Œå°±æ˜¯å› ä¸ºåœ¨è¿™äº›æ¨¡å‹ä¸ŠæŠ•å…¥äº†å¤ªä¹…ã€‚å¾ˆå¤šå¯é æ€§å°±æ¥è‡ªæ—¥å¤ä¸€æ—¥çš„å¾®è°ƒã€ä¼˜åŒ–ï¼Œè®©å®ƒä»¬è¾¾åˆ°ä¸€ç§è¶…çº§ä¼˜åŒ–çš„çŠ¶æ€ã€‚

è§£æï¼š
* **bread and butter** ğŸ”¥ï¼šçŸ­è¯­ï¼Œçœ‹å®¶æœ¬é¢†ã€ä¸»è¦æ”¶å…¥æ¥æºï¼ˆæ¯”å–»æ ¸å¿ƒä¸šåŠ¡ï¼‰
* **maturity** /mÉ™ËˆtjÊŠÉ™rÉªti/ï¼šåè¯ï¼Œæˆç†Ÿåº¦
* **hyper optimized**ï¼šè¶…çº§ä¼˜åŒ–çš„
* **fine-tuning**ï¼šå¾®è°ƒï¼ˆAI æœ¯è¯­ï¼Œåœ¨é¢„è®­ç»ƒåŸºç¡€ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼‰

---

(6) [16:56-17:18] **And I think with AI, a lot of the modelsâ€” things are just moving so fast that infra engineers haven't even had a chance to catch up. And demand is what it is. I mean you have to make choices about how you handle having too much demand. Google was really public with that when the Gemini 3 launch happened, they had to take TPUs away from Gemini 2.5 and give them to Gemini 3 for the launch.**

æˆ‘è§‰å¾— AI é¢†åŸŸé‡Œå¾ˆå¤šæ¨¡å‹çš„å‘å±•å¤ªå¿«äº†ï¼ŒåŸºç¡€è®¾æ–½å·¥ç¨‹å¸ˆéƒ½è¿˜æ¥ä¸åŠè·Ÿä¸Šã€‚è€Œéœ€æ±‚å°±æ‘†åœ¨é‚£é‡Œï¼Œä½ å¿…é¡»åšå‡ºé€‰æ‹©æ¥åº”å¯¹è¿‡å¤šçš„éœ€æ±‚ã€‚**Google** åœ¨è¿™æ–¹é¢å¾ˆå¦è¯šâ€”â€”**Gemini 3** å‘å¸ƒçš„æ—¶å€™ï¼Œä»–ä»¬ä¸å¾—ä¸ä» **Gemini 2.5** é‚£é‡ŒæŠ½è°ƒ **TPU** æ¥ç»™ **Gemini 3** åšå‘å¸ƒã€‚

è§£æï¼š
* **infra engineers**ï¼šåŸºç¡€è®¾æ–½å·¥ç¨‹å¸ˆï¼ˆinfra = infrastructure çš„ç¼©å†™ï¼‰
* **catch up**ï¼šçŸ­è¯­ï¼Œèµ¶ä¸Šã€è¿½ä¸Š
* **TPU**ï¼šå¼ é‡å¤„ç†å•å…ƒï¼ˆ**Google** è‡ªç ”çš„ AI åŠ é€ŸèŠ¯ç‰‡ï¼‰ğŸ”¥
* **be public with**ï¼šå¯¹...å…¬å¼€å¦è¯š

---

(7) [17:18-17:43] **And I was like, "No, no, don't do that. We're using Gemini 2.5 in production." But, you know, it's a good problem to have. Yeah. I'm curious, Zach, actually, like what was your guiding star as you're improving these speech models? I think obviously benchmarks have their faults if you optimize too much for benchmarks. How do you get that voice from users from like real world use cases and really know that you're optimizing for the voice AI use cases?**

æˆ‘å½“æ—¶å°±è¯´ï¼š"ä¸ä¸ä¸ï¼Œåˆ«è¿™æ ·å•Šï¼Œæˆ‘ä»¬åœ¨ç”Ÿäº§ç¯å¢ƒç”¨ç€ **Gemini 2.5** å‘¢ï¼"ä½†è¯´å®è¯ï¼Œè¿™ä¹Ÿç®—æ˜¯ä¸ªå¹¸ç¦çš„çƒ¦æ¼ã€‚**Zach**ï¼Œæˆ‘æŒºå¥½å¥‡çš„ï¼Œä½ åœ¨æ”¹è¿›è¿™äº›è¯­éŸ³æ¨¡å‹æ—¶ï¼ŒæŒ‡å¯¼æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿå¦‚æœå¯¹åŸºå‡†æµ‹è¯•è¿‡åº¦ä¼˜åŒ–æ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ã€‚ä½ æ€ä¹ˆä»çœŸå®ç”¨æˆ·çš„å®é™…ä½¿ç”¨åœºæ™¯ä¸­è·å–åé¦ˆï¼Œæ¥ç¡®ä¿ä½ æ˜¯åœ¨ä¸ºè¯­éŸ³ AI çš„åœºæ™¯åšä¼˜åŒ–ï¼Ÿ

è§£æï¼š
* **a good problem to have** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå¹¸ç¦çš„çƒ¦æ¼ï¼ˆé—®é¢˜æœ¬èº«è¯´æ˜å½¢åŠ¿ä¸é”™ï¼‰
* **guiding star**ï¼šæŒ‡å¯¼æ–¹å‘ã€åŒ—ææ˜Ÿï¼ˆæ¯”å–»æŒ‡å¼•æ–¹å‘çš„åŸåˆ™ï¼‰ğŸ”¥
* **fault** /fÉ”Ëlt/ï¼šåè¯ï¼Œç¼ºé™·ã€ä¸è¶³

---

(8) [17:49-18:09] **Yes, I mean it's a good question. On the one hand, I got to tell you as someone who loves evalâ€” like I'm a king of vibes. Like to me vibe is theâ€” I haven't figured out any benchmark that I trust fully more than like listening and having a conversation and like passing that "does this feel good" test. And you know we keep trying to play with different ways to measure it and evaluate,**

å—¯ï¼Œè¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘å¾—è·Ÿä½ è¯´ï¼Œä½œä¸ºä¸€ä¸ªçƒ­çˆ±è¯„æµ‹çš„äººâ€”â€”æˆ‘æ˜¯"æ°›å›´ä¹‹ç‹"ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œæ°›å›´æ„Ÿå°±æ˜¯ä¸€åˆ‡â€”â€”æˆ‘è¿˜æ²¡æ‰¾åˆ°ä»»ä½•ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¯”äº²è‡ªå»å¬ã€å»å¯¹è¯ã€å»é€šè¿‡é‚£ä¸ª"æ„Ÿè§‰å¯¹ä¸å¯¹"çš„æµ‹è¯•æ›´è®©æˆ‘ä¿¡æœçš„ã€‚æˆ‘ä»¬ä¸€ç›´åœ¨å°è¯•å„ç§ä¸åŒçš„æ–¹å¼å»è¡¡é‡å’Œè¯„ä¼°ï¼Œ

è§£æï¼š
* **king of vibes** ğŸ”¥ï¼šæ°›å›´ä¹‹ç‹ï¼ˆvibes = æ°›å›´æ„Ÿï¼Œè¿™é‡Œæ˜¯è‡ªå˜²å¼çš„è¡¨è¾¾ï¼‰
* **eval** /ÉªËˆvÃ¦l/ï¼šåè¯ï¼Œè¯„æµ‹ï¼ˆevaluation çš„ç¼©å†™ï¼‰ğŸ”¥
* **play with**ï¼šçŸ­è¯­ï¼Œå°è¯•ã€è¯•éªŒ

---

(9) [18:09-18:33] **but like nothing is quite as brutal as putting in my AirPods and talking to the models for 20 minutes and coming back and being like "all right well that's garbage"â€” you know, this is likeâ€” "this felt wrong, this felt wrong, this felt wrong." And I think our guiding principle has always been like how do I get to the point where it feels incredibly natural and like we'll just know it when we hear it.**

ä½†æ²¡æœ‰ä»€ä¹ˆæ¯”æˆ´ä¸Š **AirPods** è·Ÿæ¨¡å‹èŠ 20 åˆ†é’Ÿã€ç„¶åå›æ¥è¯´"å¥½å§è¿™ç®€ç›´æ˜¯åƒåœ¾"æ¥å¾—æ›´æ®‹é…·äº†â€”â€”"è¿™é‡Œæ„Ÿè§‰ä¸å¯¹ï¼Œé‚£é‡Œæ„Ÿè§‰ä¸å¯¹ï¼Œè¿™ä¸ªä¹Ÿä¸å¯¹ã€‚"æ‰€ä»¥æˆ‘ä»¬çš„æŒ‡å¯¼åŸåˆ™ä¸€ç›´æ˜¯ï¼šæ€ä¹ˆæ‰èƒ½åšåˆ°è®©å®ƒæ„Ÿè§‰æå…¶è‡ªç„¶ï¼Ÿæˆ‘ä»¬å¬åˆ°çš„æ—¶å€™è‡ªç„¶å°±ä¼šçŸ¥é“çš„ã€‚

è§£æï¼š
* **brutal** /ËˆbruËtl/ï¼šå½¢å®¹è¯ï¼Œæ®‹é…·çš„ã€ç›´æ¥çš„
* **garbage** /ËˆÉ¡É‘ËrbÉªdÊ’/ï¼šåè¯ï¼Œåƒåœ¾ï¼ˆå£è¯­ä¸­è¡¨ç¤ºè´¨é‡æå·®ï¼‰
* **guiding principle**ï¼šæŒ‡å¯¼åŸåˆ™ ğŸ”¥
* **we'll know it when we hear it**ï¼šå¬åˆ°äº†å°±çŸ¥é“äº†ï¼ˆè¡¨è¾¾ä¸€ç§ç›´è§‰æ ‡å‡†ï¼‰

---

(10) [18:33-18:57] **But of course it's a multi-year sort of ladder climb to get there and hill climb. And so a lot of it comes down toâ€” your customers are having these challenges with the models and even though I think we all have this mental state that we want to reach, there's a bunch of compromises we make in the interim. And the customers complain to us and we make these trade-offs that work for one customer but don't work for another customer and then we try to generalize that.**

å½“ç„¶äº†ï¼Œè¦è¾¾åˆ°é‚£ä¸ªçŠ¶æ€æ˜¯ä¸€ä¸ªå¤šå¹´çš„é˜¶æ¢¯å¼æ”€ç™»å’Œçˆ¬å¡è¿‡ç¨‹ã€‚æ‰€ä»¥å¾ˆå¤šæ—¶å€™å½’ç»“äºâ€”â€”ä½ ä»¬çš„å®¢æˆ·åœ¨æ¨¡å‹ä¸Šé‡åˆ°å„ç§é—®é¢˜ï¼Œè™½ç„¶æˆ‘ä»¬å¿ƒé‡Œéƒ½æœ‰ä¸€ä¸ªæƒ³è¦è¾¾åˆ°çš„ç†æƒ³çŠ¶æ€ï¼Œä½†ä¸­é—´è¿‡ç¨‹ä¸­è¦åšå¾ˆå¤šå¦¥åã€‚å®¢æˆ·æ¥æŠ±æ€¨ï¼Œæˆ‘ä»¬åšå‡ºçš„æƒè¡¡å¯èƒ½å¯¹ä¸€ä¸ªå®¢æˆ·æœ‰ç”¨ä½†å¯¹å¦ä¸€ä¸ªä¸ç®¡ç”¨ï¼Œç„¶åæˆ‘ä»¬å†è¯•ç€æ³›åŒ–ã€‚

è§£æï¼š
* **hill climb**ï¼šçˆ¬å¡ï¼ˆæœºå™¨å­¦ä¹ æœ¯è¯­ï¼Œé€æ­¥ä¼˜åŒ–çš„è¿‡ç¨‹ï¼‰ğŸ”¥
* **in the interim** /ËˆÉªntÉ™rÉªm/ï¼šçŸ­è¯­ï¼Œåœ¨æ­¤æœŸé—´ã€åœ¨è¿‡æ¸¡æœŸé—´
* **trade-off** ğŸ”¥ï¼šåè¯ï¼Œæƒè¡¡ã€å–èˆ
* **generalize** /ËˆdÊ’enÉ™rÉ™laÉªz/ï¼šåŠ¨è¯ï¼Œæ³›åŒ–ã€æ¨å¹¿

---

(11) [18:57-19:16] **So I wish I had a magical good answer there, but I feel like I don't actually. It's just a lot of gut trust and then also being yelled at by other people that things aren't good and you're like, "Yeah, you're right. That's not good. We should fix that."**

æ‰€ä»¥æˆ‘å¸Œæœ›æˆ‘èƒ½ç»™å‡ºä¸€ä¸ªå¾ˆå‰å®³çš„ç­”æ¡ˆï¼Œä½†è¯´å®è¯æˆ‘è§‰å¾—å¹¶æ²¡æœ‰ã€‚å¾ˆå¤šæ—¶å€™å°±æ˜¯é ç›´è§‰ä¿¡ä»»ï¼ŒåŠ ä¸Šè¢«åˆ«äººå¼ç€è¯´"è¿™ä¸ªä¸è¡Œ"ï¼Œç„¶åä½ å°±è¯´ï¼š"å—¯ï¼Œä½ è¯´å¾—å¯¹ï¼Œè¿™ç¡®å®ä¸å¥½ï¼Œæˆ‘ä»¬è¯¥ä¿®ä¸€ä¿®ã€‚"

è§£æï¼š
* **gut trust**ï¼šç›´è§‰ä¿¡ä»»ï¼ˆgut = è‚ é“ï¼Œå¼•ç”³ä¸ºç›´è§‰ï¼‰ğŸ”¥
* **yell at**ï¼šçŸ­è¯­ï¼Œå¯¹...å¤§å£°å–Šå«ã€è®­æ–¥

---

(12) [19:16-19:40] **I don't actually disagree that talking to the models for hours is like the very best eval. But for the purpose of the podcast, I'm going to pretend to disagree and then we'll let Brooke break the tie. So, we help so many customers get from POC to production that I definitely view a pain point asâ€” I got this prompt right and the 20 people at our company who tested it had a good experience, but then I put the model into production and people did weird things and it's not good enough.**

å…¶å®æˆ‘å¹¶ä¸åå¯¹è·Ÿæ¨¡å‹èŠå‡ ä¸ªå°æ—¶æ˜¯æœ€å¥½çš„è¯„æµ‹æ–¹å¼ã€‚ä½†ä¸ºäº†è¿™æœŸæ’­å®¢çš„æ•ˆæœï¼Œæˆ‘å…ˆå‡è£…ä¸åŒæ„ï¼Œç„¶åè®© **Brooke** æ¥åšè£åˆ¤ã€‚æˆ‘ä»¬å¸®åŠ©äº†å¾ˆå¤šå®¢æˆ·ä» **POC** èµ°åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ‰€ä»¥æˆ‘ç‰¹åˆ«èƒ½ä½“ä¼šåˆ°ä¸€ä¸ªç—›ç‚¹â€”â€”"æˆ‘æŠŠæç¤ºè¯è°ƒå¥½äº†ï¼Œå…¬å¸é‡Œ 20 ä¸ªæµ‹è¯•äººå‘˜ç”¨ç€æŒºå¥½"ï¼Œä½†ä¸€æ—¦æ”¾åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œç”¨æˆ·å„ç§å¥‡æ€ªçš„æ“ä½œå°±æ¥äº†ï¼Œæ•ˆæœå°±ä¸å¤Ÿå¥½äº†ã€‚

è§£æï¼š
* **break the tie**ï¼šçŸ­è¯­ï¼Œæ‰“ç ´å¹³å±€
* **POC (proof of concept)** ğŸ”¥ï¼šæ¦‚å¿µéªŒè¯ï¼Œäº§å“ä¸Šçº¿å‰çš„éªŒè¯é˜¶æ®µ
* **pain point** ğŸ”¥ï¼šç—›ç‚¹

---

(13) [19:40-20:01] **And from that perspective, when I wrote this benchmark or when I sort of turned our partly vibes, partly quantitative internal benchmarks into something that I felt like I could stand behind and put out there in public and say, "there's some useful information here"â€” what I was trying to do was pick a typical workflow with some hard components and make it quantitative,**

ä»è¿™ä¸ªè§’åº¦å‡ºå‘ï¼Œå½“æˆ‘å†™è¿™ä¸ªåŸºå‡†æµ‹è¯•â€”â€”æˆ–è€…è¯´å½“æˆ‘æŠŠæˆ‘ä»¬å†…éƒ¨é‚£å¥—"ä¸€åŠé æ°›å›´ã€ä¸€åŠé é‡åŒ–"çš„æµ‹è¯•å˜æˆä¸€ä¸ªæˆ‘èƒ½å…¬å¼€ç«™å°çš„ä¸œè¥¿ï¼Œå¯¹å¤–è¯´"è¿™é‡Œé¢æœ‰äº›æœ‰ç”¨çš„ä¿¡æ¯"â€”â€”æˆ‘æƒ³åšçš„å°±æ˜¯æŒ‘ä¸€ä¸ªåŒ…å«ä¸€äº›éš¾ç‚¹çš„å…¸å‹å·¥ä½œæµï¼Œç„¶åæŠŠå®ƒé‡åŒ–ï¼Œ

è§£æï¼š
* **stand behind**ï¼šçŸ­è¯­ï¼Œä¸º...ç«™å°ã€æ”¯æŒ
* **quantitative** /ËˆkwÉ’ntÉªtÉ™tÉªv/ï¼šå½¢å®¹è¯ï¼Œé‡åŒ–çš„ã€å®šé‡çš„ ğŸ”¥
* **workflow**ï¼šå·¥ä½œæµ

---

(14) [20:01-20:25] **so that I could run a bunch of models and show where the models really do differ in performance. And one of the judgment calls there isâ€” if you do some really good iterative prompting, any of the models will do better on this benchmark. But what we see in practice is that what I was trying to mimic was in practice when different people talk to the model different ways, how do they diverge?**

è¿™æ ·æˆ‘å°±èƒ½è·‘ä¸€å †æ¨¡å‹ï¼Œå±•ç¤ºå®ƒä»¬åœ¨æ€§èƒ½ä¸Šåˆ°åº•æœ‰ä»€ä¹ˆä¸åŒã€‚å…¶ä¸­ä¸€ä¸ªåˆ¤æ–­æ˜¯ï¼šå¦‚æœä½ åšäº†å¾ˆå¥½çš„è¿­ä»£å¼æç¤ºè¯ä¼˜åŒ–ï¼Œä»»ä½•æ¨¡å‹åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½ä¼šè¡¨ç°æ›´å¥½ã€‚ä½†åœ¨å®è·µä¸­æˆ‘ä»¬çœ‹åˆ°çš„â€”â€”æˆ‘æƒ³æ¨¡æ‹Ÿçš„å°±æ˜¯å½“ä¸åŒçš„äººç”¨ä¸åŒçš„æ–¹å¼è·Ÿæ¨¡å‹å¯¹è¯æ—¶ï¼Œè¡¨ç°ä¼šæ€ä¹ˆå‘æ•£ã€‚

è§£æï¼š
* **judgment call** ğŸ”¥ï¼šåè¯ï¼Œä¸»è§‚åˆ¤æ–­ï¼ˆæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆçš„å†³å®šï¼‰
* **iterative prompting**ï¼šè¿­ä»£å¼æç¤ºè¯ä¼˜åŒ– ğŸ”¥
* **mimic** /ËˆmÉªmÉªk/ï¼šåŠ¨è¯ï¼Œæ¨¡æ‹Ÿã€æ¨¡ä»¿
* **diverge** /daÉªËˆvÉœËrdÊ’/ï¼šåŠ¨è¯ï¼Œå‘æ•£ã€åˆ†åŒ–

---

(15) [20:25-20:49] **And so if you do some prompt iteration what you're doing is trying to draw a box around that divergence. But what I hope we're showing in the benchmark is what it looks like when you hit that thing that you didn't know to prompt engineer for. And that does feel like a useful quantitative exerciseâ€” even though you can't capture the whole space, if you can sort of draw a box around the space and be like, "this model's clearly in the box, this model's not in the box."**

æ‰€ä»¥å½“ä½ åšæç¤ºè¯è¿­ä»£æ—¶ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨ç»™é‚£ä¸ªå‘æ•£èŒƒå›´"ç”»ä¸ªæ¡†"ã€‚ä½†æˆ‘å¸Œæœ›æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•èƒ½å±•ç¤ºçš„æ˜¯â€”â€”å½“ä½ ç¢°åˆ°é‚£äº›ä½ ä¸çŸ¥é“è¯¥æ€ä¹ˆåšæç¤ºå·¥ç¨‹çš„åœºæ™¯æ—¶ï¼Œæ¨¡å‹è¡¨ç°æ˜¯ä»€ä¹ˆæ ·çš„ã€‚è¿™ç¡®å®æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„é‡åŒ–ç»ƒä¹ â€”â€”è™½ç„¶ä½ æ— æ³•è¦†ç›–æ•´ä¸ªç©ºé—´ï¼Œä½†å¦‚æœä½ èƒ½ç»™ç©ºé—´ç”»ä¸ªæ¡†ï¼Œè¯´"è¿™ä¸ªæ¨¡å‹æ˜æ˜¾åœ¨æ¡†é‡Œï¼Œé‚£ä¸ªæ¨¡å‹ä¸åœ¨æ¡†é‡Œ"ã€‚

è§£æï¼š
* **draw a box around** ğŸ”¥ï¼šåœˆå®šèŒƒå›´ï¼ˆæ¯”å–»ç»™é—®é¢˜åˆ’å®šè¾¹ç•Œï¼‰
* **prompt engineer**ï¼šåšæç¤ºå·¥ç¨‹ï¼ˆè¿™é‡Œç”¨ä½œåŠ¨è¯ï¼‰
* **capture the whole space**ï¼šè¦†ç›–æ•´ä¸ªç©ºé—´

---

(16) [20:49-21:08] **That's a useful point of comparison for what it feels like to deploy these things into production and have to try to get them to serve a pretty wide variety of real world user behavior. So thatâ€” and that's very much not vibes basedâ€” like it's a different angle than the vibes based thing.**

è¿™å°±æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å¯¹æ¯”ç»´åº¦â€”â€”èƒ½è®©ä½ æ„Ÿå—åˆ°æŠŠè¿™äº›ä¸œè¥¿éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€ç„¶åè¦è®©å®ƒä»¬æœåŠ¡äºå„ç§å„æ ·çœŸå®ä¸–ç•Œç”¨æˆ·è¡Œä¸ºæ˜¯ä»€ä¹ˆä½“éªŒã€‚è¿™ç§æ–¹å¼è·Ÿ"å‡­æ°›å›´æ„Ÿè§‰"å®Œå…¨ä¸åŒï¼Œæ˜¯å¦ä¸€ä¸ªè§’åº¦ã€‚

è§£æï¼š
* **point of comparison**ï¼šå¯¹æ¯”ç»´åº¦ã€æ¯”è¾ƒåŸºå‡†
* **deploy** /dÉªËˆplÉ”Éª/ï¼šåŠ¨è¯ï¼Œéƒ¨ç½² ğŸ”¥
* **vibes based**ï¼šåŸºäºæ°›å›´/ç›´è§‰çš„

---

(17) [21:08-21:33] **Yeah, I think it's interesting because I actually think doing evaluations of foundational models is so different than doing evaluations of applications and very task oriented agents. You're going to haveâ€” I think you're just looking for a much broader array of different things that your model can do. And so what's in distribution versus what's not in distributionâ€” like how does it feel across all these different use casesâ€”**

æ˜¯çš„ï¼Œæˆ‘è§‰å¾—å¾ˆæœ‰æ„æ€ï¼Œå› ä¸ºå¯¹åŸºç¡€æ¨¡å‹åšè¯„æµ‹å’Œå¯¹åº”ç”¨ç¨‹åºä»¥åŠé¢å‘ç‰¹å®šä»»åŠ¡çš„ **Agent** åšè¯„æµ‹æ˜¯éå¸¸ä¸åŒçš„ã€‚ä½ éœ€è¦è€ƒå¯Ÿæ¨¡å‹èƒ½åšåˆ°çš„æ›´å¹¿æ³›çš„èƒ½åŠ›èŒƒå›´ã€‚æ‰€ä»¥ä»€ä¹ˆæ˜¯åˆ†å¸ƒå†…çš„ã€ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¤–çš„â€”â€”åœ¨æ‰€æœ‰è¿™äº›ä¸åŒçš„ç”¨ä¾‹ä¸­æ„Ÿè§‰å¦‚ä½•â€”â€”

è§£æï¼š
* **foundational models**ï¼šåŸºç¡€æ¨¡å‹ ğŸ”¥
* **task oriented agents**ï¼šé¢å‘ä»»åŠ¡çš„æ™ºèƒ½ä½“
* **in distribution / out of distribution** ğŸ”¥ï¼šåˆ†å¸ƒå†…/åˆ†å¸ƒå¤–ï¼ˆML æœ¯è¯­ï¼ŒæŒ‡æ•°æ®æ˜¯å¦å±äºæ¨¡å‹è®­ç»ƒæ—¶è§è¿‡çš„èŒƒå›´ï¼‰

---

(18) [21:33-21:52] **ultimately is just a very different problem than setting up evals. And we primarily focus on agent evals. So that was not a trick question of this is what you should or shouldn't be doing. I think we definitely don't work as much in that space of how do you do model evals versus task evals which is why I was genuinely curious.**

è¿™è·Ÿè®¾ç½®è¯„æµ‹æ ¹æœ¬å°±æ˜¯ä¸åŒçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸»è¦èšç„¦åœ¨ **Agent** è¯„æµ‹ä¸Šã€‚æ‰€ä»¥åˆšæ‰ä¸æ˜¯æŒ–å‘é—®ä½ è¯¥ä¸è¯¥è¿™ä¹ˆåšã€‚æˆ‘è§‰å¾—æˆ‘ä»¬ç¡®å®ä¸å¤ªæ¶‰åŠæ¨¡å‹è¯„æµ‹â€”â€”ç›¸æ¯”ä»»åŠ¡è¯„æµ‹æ¥è¯´â€”â€”æ‰€ä»¥æˆ‘æ˜¯çœŸå¿ƒå¥½å¥‡æ‰é—®çš„ã€‚

è§£æï¼š
* **trick question**ï¼šé™·é˜±é¢˜ã€æ•…æ„åˆéš¾çš„é—®é¢˜
* **model evals vs task evals**ï¼šæ¨¡å‹è¯„æµ‹ vs ä»»åŠ¡è¯„æµ‹ï¼ˆå‰è€…è¯„ä¼°æ¨¡å‹åŸºç¡€èƒ½åŠ›ï¼Œåè€…è¯„ä¼°å…·ä½“ä»»åŠ¡è¡¨ç°ï¼‰ğŸ”¥
* **genuinely** /ËˆdÊ’enjuÉªnli/ï¼šå‰¯è¯ï¼ŒçœŸæ­£åœ°ã€çœŸå¿ƒåœ°

---

(19) [21:52-22:16] **I think it's a much harder problem to know what are people even going to be using this forâ€” what do people expect from a model. And I think that's where listening to customersâ€” some of the best insights we get around what models are doing well is actually just from customers saying "we're hearing jitters" or "we're seeing a lot of markdown in these cases" or "we're seeing it vocalizing punctuation in weird ways."**

æˆ‘è§‰å¾—æ›´éš¾çš„é—®é¢˜æ˜¯â€”â€”äººä»¬åˆ°åº•è¦ç”¨è¿™ä¸ªåšä»€ä¹ˆï¼Ÿä»–ä»¬å¯¹æ¨¡å‹æœ‰ä»€ä¹ˆæœŸæœ›ï¼Ÿæˆ‘è§‰å¾—è¿™å°±æ˜¯éœ€è¦å€¾å¬å®¢æˆ·å£°éŸ³çš„åœ°æ–¹ã€‚å…³äºæ¨¡å‹åšå¾—æ€ä¹ˆæ ·ï¼Œä¸€äº›æœ€å¥½çš„æ´å¯Ÿå…¶å®å°±æ¥è‡ªå®¢æˆ·è¯´"æˆ‘ä»¬å¬åˆ°äº†å¡é¡¿""æˆ‘ä»¬çœ‹åˆ°äº†å¤§é‡ **markdown** æ ¼å¼""æˆ‘ä»¬çœ‹åˆ°å®ƒåœ¨ä»¥å¥‡æ€ªçš„æ–¹å¼æŠŠæ ‡ç‚¹ç¬¦å·å¿µå‡ºæ¥"ã€‚

è§£æï¼š
* **jitters** /ËˆdÊ’ÉªtÉ™rz/ï¼šåè¯ï¼Œå¡é¡¿ã€æŠ–åŠ¨ï¼ˆéŸ³é¢‘/è§†é¢‘ä¸­çš„ä¸æµç•…ç°è±¡ï¼‰ğŸ”¥
* **vocalize** /ËˆvoÊŠkÉ™laÉªz/ï¼šåŠ¨è¯ï¼Œå‘å£°ã€å¿µå‡ºæ¥
* **punctuation** /ËŒpÊŒÅ‹ktÊƒuËˆeÉªÊƒÉ™n/ï¼šåè¯ï¼Œæ ‡ç‚¹ç¬¦å·

---

(20) [22:16-22:38] **Like that's ultimately just what you're going to hear from users. And it's hard to have said "I'm going to create evals for what happens when it accidentally starts speaking in Japanese"â€” like how did you even get there? And ultimately listening to users on that front. Yeah. I'll add that sometimes funny things we run intoâ€” we have this set of evals that we always check very closelyâ€” it's like almost on a model training run, it gets done, it goes and runs the evals.**

è¿™äº›å°±æ˜¯ä½ æœ€ç»ˆä¼šä»ç”¨æˆ·é‚£é‡Œå¬åˆ°çš„ä¸œè¥¿ã€‚è€Œä¸”å¾ˆéš¾äº‹å…ˆè¯´"æˆ‘è¦ç»™è¿™ç§æƒ…å†µå»ºä¸ªè¯„æµ‹"â€”â€”æ¯”å¦‚æ¨¡å‹æ€ä¹ˆçªç„¶å°±å¼€å§‹è¯´æ—¥è¯­äº†ï¼Ÿä½ æ€ä¹ˆé¢„æ–™åˆ°ï¼Ÿæ‰€ä»¥å½’æ ¹ç»“åº•è¿˜æ˜¯è¦å€¾å¬ç”¨æˆ·ã€‚æ˜¯çš„ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬é‡åˆ°ä¸€äº›å¾ˆæœ‰æ„æ€çš„äº‹â€”â€”æˆ‘ä»¬æœ‰ä¸€å¥—è¯„æµ‹æ˜¯æ¯æ¬¡éƒ½ä»”ç»†æ£€æŸ¥çš„ï¼ŒåŸºæœ¬ä¸Šæ¨¡å‹è®­ç»ƒå®Œäº†å°±è‡ªåŠ¨è·‘è¯„æµ‹ã€‚

è§£æï¼š
* **accidentally**ï¼šå‰¯è¯ï¼Œæ„å¤–åœ°ã€ä¸å°å¿ƒ
* **on that front**ï¼šçŸ­è¯­ï¼Œåœ¨é‚£ä¸ªæ–¹é¢
* **training run**ï¼šä¸€æ¬¡è®­ç»ƒè¿è¡Œï¼ˆæ¨¡å‹è®­ç»ƒçš„ä¸€ä¸ªå®Œæ•´å‘¨æœŸï¼‰ğŸ”¥

---

(21) [22:38-22:56] **These are things like the Big Bench audios, these are like COVOST 2 which is like BLEU. So we basically use translation as a proxy. So you know the model says something in one language and the job is to translate to another, and the premise is that if you don't understand what was being said then clearly you're going to be bad at translating.**

è¿™äº›è¯„æµ‹åŒ…æ‹¬ **Big Bench** éŸ³é¢‘æµ‹è¯•ã€**COVOST 2** è¿™ç§ç”¨ **BLEU** åˆ†æ•°è¡¡é‡çš„ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯æŠŠç¿»è¯‘å½“ä½œä¸€ç§ä»£ç†æŒ‡æ ‡â€”â€”æ¨¡å‹ç”¨ä¸€ç§è¯­è¨€è¯´äº†äº›ä»€ä¹ˆï¼Œä»»åŠ¡æ˜¯ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚å‰ææ˜¯å¦‚æœä½ è¿è¯´çš„ä»€ä¹ˆéƒ½å¬ä¸æ‡‚ï¼Œé‚£ç¿»è¯‘è‚¯å®šä¹Ÿå¥½ä¸äº†ã€‚

è§£æï¼š
* **Big Bench**ï¼šå¤§å‹åŸºå‡†æµ‹è¯•é›†ï¼ˆç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ï¼‰
* **COVOST 2**ï¼šè·¨è¯­è¨€è¯­éŸ³ç¿»è¯‘æ•°æ®é›† ğŸ”¥
* **BLEU score** ğŸ”¥ï¼šBLEU åˆ†æ•°ï¼ˆæœºå™¨ç¿»è¯‘è´¨é‡çš„è¯„ä¼°æŒ‡æ ‡ï¼‰
* **proxy** /ËˆprÉ’ksi/ï¼šåè¯ï¼Œä»£ç†ã€æ›¿ä»£æŒ‡æ ‡ ğŸ”¥
* **premise** /ËˆpremÉªs/ï¼šåè¯ï¼Œå‰æ

---

(22) [22:56-23:15] **So I think a lot of these evals in model land are like proxies for the real thing, right? We're trying to proxy speech understanding. And then what's funny is sometimes they can be quite distant from the real world. I'll give you a simple exampleâ€”**

æ‰€ä»¥æˆ‘è§‰å¾—æ¨¡å‹é¢†åŸŸé‡Œå¾ˆå¤šè¯„æµ‹å…¶å®éƒ½æ˜¯çœŸå®åœºæ™¯çš„ä»£ç†æŒ‡æ ‡å¯¹å§ï¼Ÿæˆ‘ä»¬åœ¨è¯•å›¾ç”¨ä»£ç†æ–¹å¼è¡¡é‡è¯­éŸ³ç†è§£èƒ½åŠ›ã€‚ç„¶åæœ‰æ„æ€çš„æ˜¯ï¼Œæœ‰æ—¶å€™è¿™äº›ä»£ç†æŒ‡æ ‡è·ŸçœŸå®ä¸–ç•Œçš„è·ç¦»è¿˜æŒºè¿œçš„ã€‚æˆ‘ä¸¾ä¸ªç®€å•ä¾‹å­â€”â€”

è§£æï¼š
* **model land**ï¼šæ¨¡å‹é¢†åŸŸï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰
* **distant from the real world**ï¼šè·ŸçœŸå®ä¸–ç•Œæœ‰è·ç¦»

---

(23) [23:15-23:39] **None of these evalsâ€” like a really common use case for voice agentsâ€” and I'm guessing you guys agree with thisâ€” are things like collection of some kind of data, let's say like an ID number or a phone number. And what's interesting is these are almost never in the core model speech understanding eval sets for the most part, but they're very common in voice agents and it's like a pretty hard problem actually because you have lots of variation so they're kind of out of distribution.**

è¿™äº›è¯„æµ‹é‡Œå‡ ä¹ä¸åŒ…å«â€”â€”ä½†è¯­éŸ³ **Agent** æœ€å¸¸è§çš„ä¸€ä¸ªç”¨ä¾‹å°±æ˜¯æ”¶é›†æŸç§æ•°æ®ï¼Œæ¯”å¦‚èº«ä»½è¯å·æˆ–ç”µè¯å·ç ã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œè¿™äº›åœºæ™¯å‡ ä¹ä»æ¥ä¸åœ¨æ ¸å¿ƒæ¨¡å‹çš„è¯­éŸ³ç†è§£è¯„æµ‹é›†é‡Œï¼Œä½†åœ¨è¯­éŸ³ **Agent** ä¸­éå¸¸å¸¸è§ã€‚è€Œä¸”è¿™å…¶å®æ˜¯ä¸ªæŒºéš¾çš„é—®é¢˜ï¼Œå› ä¸ºå˜åŒ–å¤ªå¤šäº†ï¼ŒåŸºæœ¬å±äºåˆ†å¸ƒå¤–çš„æ•°æ®ã€‚

è§£æï¼š
* **eval sets**ï¼šè¯„æµ‹æ•°æ®é›†
* **variation** /ËŒveriËˆeÉªÊƒÉ™n/ï¼šåè¯ï¼Œå˜åŒ–ã€å·®å¼‚
* **out of distribution** ğŸ”¥ï¼šåˆ†å¸ƒå¤–ï¼ˆæ¨¡å‹æ²¡è§è¿‡çš„æ•°æ®ç±»å‹ï¼‰

---

(24) [23:39-24:02] **So we'll give ourselves a high five on some of these model performance evalsâ€” "look at us improving our BLEU score!"â€” and then I'll throw it to a customer and they'd be like "garbage garbage garbage" and you're like "oh yeah okay there's a gap in our methodology here." And so we've had to constantly figure out how to bring those customer insights into the model evals that the modeling team can also use.**

æ‰€ä»¥æˆ‘ä»¬ä¼šå› ä¸ºæ¨¡å‹æ€§èƒ½è¯„æµ‹çš„æå‡è€Œå‡»æŒåº†ç¥â€”â€”"çœ‹çœ‹æˆ‘ä»¬ï¼Œ**BLEU** åˆ†æ•°åˆæé«˜äº†ï¼"ç„¶åä¸€ä¸¢ç»™å®¢æˆ·ï¼Œä»–ä»¬å°±è¯´"åƒåœ¾åƒåœ¾åƒåœ¾"ã€‚ç„¶åä½ å°±æ„è¯†åˆ°ï¼š"å¥½å§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®ºç¡®å®æœ‰ç¼ºå£ã€‚"æ‰€ä»¥æˆ‘ä»¬ä¸å¾—ä¸ä¸€ç›´åœ¨æƒ³åŠæ³•æŠŠå®¢æˆ·çš„è¿™äº›æ´å¯Ÿä¹Ÿèå…¥åˆ°å»ºæ¨¡å›¢é˜Ÿèƒ½ç”¨çš„æ¨¡å‹è¯„æµ‹ä¸­ã€‚

è§£æï¼š
* **give oneself a high five**ï¼šå‡»æŒåº†ç¥ï¼ˆè‡ªæˆ‘è¡¨æ‰¬ï¼‰
* **methodology** /ËŒmeÎ¸É™ËˆdÉ’lÉ™dÊ’i/ï¼šåè¯ï¼Œæ–¹æ³•è®º
* **throw it to a customer**ï¼šæ‹¿ç»™å®¢æˆ·ç”¨ï¼ˆå£è¯­ï¼‰

---

(25) [24:02-24:26] **But we weren't good at this at the beginningâ€” we would be like "look at us hill climbing this benchmark that someone has set out" but sort of missing the fact that there's a whole applied part over here that we're actually not very good at. And we made a lot of mistakes throughout 2025 in training without keeping that in mind.**

ä½†æˆ‘ä»¬ä¸€å¼€å§‹å¹¶ä¸æ“…é•¿è¿™ä¸ªâ€”â€”æˆ‘ä»¬ä¼šæ²‰æµ¸åœ¨"çœ‹æˆ‘ä»¬åœ¨åˆ«äººè®¾å®šçš„åŸºå‡†æµ‹è¯•ä¸Šä¸æ–­æ”€å‡"çš„å–œæ‚¦ä¸­ï¼Œå´å¿½è§†äº†æ—è¾¹è¿˜æœ‰ä¸€æ•´å—å®é™…åº”ç”¨éƒ¨åˆ†æˆ‘ä»¬åšå¾—å¹¶ä¸å¥½ã€‚æˆ‘è§‰å¾—åœ¨æ•´ä¸ª 2025 å¹´çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å› ä¸ºæ²¡æœ‰æŠŠè¿™äº›è®°åœ¨å¿ƒä¸Šè€ŒçŠ¯äº†å¾ˆå¤šé”™è¯¯ã€‚

è§£æï¼š
* **hill climbing** ğŸ”¥ï¼šçˆ¬å¡ï¼ˆé€æ­¥ä¼˜åŒ–ï¼Œä¹Ÿæ˜¯ä¸€ç§æœç´¢ç®—æ³•ï¼‰
* **applied part**ï¼šå®é™…åº”ç”¨éƒ¨åˆ†
* **keep in mind**ï¼šçŸ­è¯­ï¼Œè®°åœ¨å¿ƒä¸Šã€ç‰¢è®°

---

(26) [24:26-24:52] **A lot of the gaps thereâ€” I definitely had a motivation for this particular benchmark around long multi-turn conversation, because voice conversations are fundamentally long multi-turn conversation use cases and it's just really clear that that kind of data is totally underrepresented in all the training data sets that people training these foundation models have.**

é‚£äº›å·®è·â€”â€”æˆ‘åšè¿™ä¸ªåŸºå‡†æµ‹è¯•çš„ä¸€ä¸ªé‡è¦åŠ¨æœºå°±æ˜¯å›´ç»•é•¿ç¨‹å¤šè½®å¯¹è¯ã€‚å› ä¸ºè¯­éŸ³å¯¹è¯ä»æ ¹æœ¬ä¸Šå°±æ˜¯é•¿ç¨‹å¤šè½®å¯¹è¯çš„ä½¿ç”¨åœºæ™¯ï¼Œå¾ˆæ˜æ˜¾è¿™ç±»æ•°æ®åœ¨æ‰€æœ‰è®­ç»ƒåŸºç¡€æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ä¸­éƒ½æ˜¯ä¸¥é‡ä¸è¶³çš„ã€‚

è§£æï¼š
* **multi-turn conversation** ğŸ”¥ï¼šå¤šè½®å¯¹è¯
* **underrepresented** /ËŒÊŒndÉ™rËŒreprÉªËˆzentÉªd/ï¼šå½¢å®¹è¯ï¼Œä»£è¡¨æ€§ä¸è¶³çš„ã€æ•°é‡ä¸å¤Ÿçš„
* **fundamentally** /ËŒfÊŒndÉ™ËˆmentÉ™li/ï¼šå‰¯è¯ï¼Œä»æ ¹æœ¬ä¸Š

---

(27) [24:52-25:10] **And I would over and over talk to people at a foundation lab and they'd say "do you want to try this new model checkpoint? We fixed function calling." And I would be like "okay great." And they would have function calling on the first three turns of the conversation noticeably better, but function calling 20 turns into the conversation would be no better at all.**

æˆ‘ä¸€æ¬¡åˆä¸€æ¬¡åœ°è·ŸåŸºç¡€æ¨¡å‹å®éªŒå®¤çš„äººäº¤æµï¼Œä»–ä»¬ä¼šè¯´"è¦ä¸è¦è¯•è¯•è¿™ä¸ªæ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬ä¿®å¤äº†å‡½æ•°è°ƒç”¨"ï¼Œæˆ‘å°±è¯´"å¥½çš„å¤ªæ£’äº†"ã€‚ç„¶åå‘ç°å¯¹è¯å‰ä¸‰è½®çš„å‡½æ•°è°ƒç”¨ç¡®å®æ˜æ˜¾å˜å¥½äº†ï¼Œä½†åˆ°äº†å¯¹è¯ç¬¬ 20 è½®ï¼Œå‡½æ•°è°ƒç”¨å®Œå…¨æ²¡æœ‰æ”¹å–„ã€‚

è§£æï¼š
* **model checkpoint** ğŸ”¥ï¼šæ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æ¨¡å‹çŠ¶æ€ï¼‰
* **function calling** ğŸ”¥ï¼šå‡½æ•°è°ƒç”¨ï¼ˆAI æ¨¡å‹è°ƒç”¨å¤–éƒ¨å·¥å…·/API çš„èƒ½åŠ›ï¼‰
* **noticeably**ï¼šå‰¯è¯ï¼Œæ˜æ˜¾åœ°

---

(28) [25:10-25:33] **And I was like, okay, I'm going to try to put this thing out there to really demonstrate where we fall short in a real world voice AI conversation around turn 15 or turn 20. Yeah, we had to build an entire eval that we called voice agent bench which was basicallyâ€” we found that you have to simulate real world conversations.**

æ‰€ä»¥æˆ‘å°±æƒ³ï¼Œå¥½ï¼Œæˆ‘è¦æŠŠè¿™ä¸ªä¸œè¥¿å‘å‡ºå»ï¼ŒçœŸæ­£å±•ç¤ºæˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œè¯­éŸ³ AI å¯¹è¯ä¸­â€”â€”ç‰¹åˆ«æ˜¯åœ¨ç¬¬ 15 è½®æˆ–ç¬¬ 20 è½®â€”â€”çš„ä¸è¶³ã€‚å¯¹ï¼Œæˆ‘ä»¬ä¸å¾—ä¸æ„å»ºäº†ä¸€æ•´å¥—è¯„æµ‹å« **voice agent bench**â€”â€”åŸºæœ¬ä¸Šå°±æ˜¯å‘ç°ä½ å¿…é¡»æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å¯¹è¯ã€‚

è§£æï¼š
* **fall short** ğŸ”¥ï¼šçŸ­è¯­ï¼Œè¾¾ä¸åˆ°ã€æœ‰ä¸è¶³
* **voice agent bench**ï¼šè¯­éŸ³ Agent åŸºå‡†æµ‹è¯•
* **simulate** /ËˆsÉªmjuleÉªt/ï¼šåŠ¨è¯ï¼Œæ¨¡æ‹Ÿ

---

(29) [25:33-25:57] **And they have to be rapid exchange back and forth and then you can sort of watch the tool calls get worse. And we had to spend a couple of months actually building that to try and identify and make progress. And I'm sure Brooke, this is where you spend lots of timeâ€” does this resonate? I think we lost your audio Brooke. Sorryâ€” yeah, that's exactly what we're building to help people be able to do this in-house.**

è€Œä¸”å¿…é¡»æ˜¯å¿«é€Ÿçš„æ¥å›å¯¹è¯äº¤äº’ï¼Œç„¶åä½ å°±èƒ½çœ‹åˆ°å‡½æ•°è°ƒç”¨è¶Šæ¥è¶Šå·®ã€‚æˆ‘ä»¬èŠ±äº†å¥½å‡ ä¸ªæœˆæ¥æ„å»ºè¿™ä¸ªè¯„æµ‹ï¼Œå°è¯•å‘ç°é—®é¢˜å¹¶æ¨åŠ¨æ”¹è¿›ã€‚**Brooke**ï¼Œä½ åº”è¯¥ä¹Ÿåœ¨è¿™æ–¹é¢èŠ±äº†å¾ˆå¤šæ—¶é—´â€”â€”èƒ½å¼•èµ·å…±é¸£å—ï¼Ÿâ€”â€”å¥½åƒä½ å£°éŸ³æ–­äº†ã€‚æ²¡å…³ç³»â€”â€”å¯¹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬åœ¨å¸®å¤§å®¶èƒ½åœ¨å†…éƒ¨åšåˆ°çš„äº‹æƒ…ã€‚

è§£æï¼š
* **rapid exchange**ï¼šå¿«é€Ÿäº¤äº’
* **resonate** /ËˆrezÉ™neÉªt/ ğŸ”¥ï¼šåŠ¨è¯ï¼Œå¼•èµ·å…±é¸£
* **in-house**ï¼šå‰¯è¯/å½¢å®¹è¯ï¼Œå†…éƒ¨çš„ã€è‡ªè¡Œå®Œæˆçš„ ğŸ”¥

---

(30) [25:57-26:20] **I think most people don't have the luxury of being able to spend many months on this. And so what we allow you to do is run those simulations and see really how does Gemini compare with OpenAI for my particular use case. Because I think benchmarks can be incredible for helping you rough cutâ€” like what model should I even be looking at. I think of it as like coarse versus fine-tuning.**

æˆ‘è§‰å¾—å¤§å¤šæ•°äººæ²¡é‚£ä¸ªå¥¢ä¾ˆæ¡ä»¶èŠ±å¥½å‡ ä¸ªæœˆæ¥åšè¿™ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬è®©ä½ èƒ½è¿è¡Œè¿™äº›æ¨¡æ‹Ÿï¼Œçœ‹çœ‹åœ¨ä½ å…·ä½“çš„ä½¿ç”¨åœºæ™¯ä¸‹ **Gemini** å’Œ **OpenAI** æ¯”èµ·æ¥æ€ä¹ˆæ ·ã€‚åŸºå‡†æµ‹è¯•åœ¨å¸®ä½ åšç²—ç­›æ–¹é¢çœŸçš„å¾ˆç»™åŠ›â€”â€”"æˆ‘åˆ°åº•è¯¥çœ‹å“ªäº›æ¨¡å‹ï¼Ÿ"æˆ‘æŠŠå®ƒçœ‹ä½œç²—è°ƒå’Œå¾®è°ƒçš„å…³ç³»ã€‚

è§£æï¼š
* **luxury** /ËˆlÊŒkÊƒÉ™ri/ï¼šåè¯ï¼Œå¥¢ä¾ˆã€å¥¢ä¾ˆå“
* **rough cut** ğŸ”¥ï¼šç²—ç­›ã€åˆæ­¥ç­›é€‰
* **coarse vs fine-tuning**ï¼šç²—è°ƒ vs å¾®è°ƒï¼ˆè¿™é‡Œç”¨è°ƒå‚æ¦‚å¿µåšæ¯”å–»ï¼‰

---

(31) [26:20-26:45] **So being able to pick the top three or four models I might try out and then actually run these on your dataâ€” see how well the instruction following comes for an interview candidate 30 turns in might be very different than collecting medical information. And specifically on how you set up your prompt or your state machines or your workflowsâ€” there's all sorts of pieces that go into your very specific voice AI setup.**

ä¹Ÿå°±æ˜¯å…ˆæŒ‘å‡ºä¸‰å››ä¸ªå¯èƒ½è¦è¯•çš„æœ€ä½³æ¨¡å‹ï¼Œç„¶ååœ¨è‡ªå·±çš„æ•°æ®ä¸Šå®é™…è·‘ä¸€è·‘â€”â€”æ¯”å¦‚å¯¹é¢è¯•å€™é€‰äººè¿›è¡Œ 30 è½®å¯¹è¯çš„æŒ‡ä»¤éµå¾ªæ•ˆæœï¼Œå¯èƒ½å’Œæ”¶é›†åŒ»ç–—ä¿¡æ¯å®Œå…¨ä¸ä¸€æ ·ã€‚è€Œä¸”å–å†³äºä½ çš„æç¤ºè¯æ€ä¹ˆè®¾ç½®ã€çŠ¶æ€æœºæ€ä¹ˆè®¾è®¡ã€å·¥ä½œæµæ€ä¹ˆæ­å»ºâ€”â€”æœ‰å¾ˆå¤šå› ç´ éƒ½ä¼šå½±å“ä½ ç‰¹å®šçš„è¯­éŸ³ AI æ–¹æ¡ˆã€‚

è§£æï¼š
* **instruction following** ğŸ”¥ï¼šæŒ‡ä»¤éµå¾ªï¼ˆæ¨¡å‹æŒ‰æŒ‡ä»¤æ‰§è¡Œçš„èƒ½åŠ›ï¼‰
* **state machines**ï¼šçŠ¶æ€æœºï¼ˆç¨‹åºè®¾è®¡ä¸­æ§åˆ¶æµç¨‹çŠ¶æ€è½¬æ¢çš„æ¨¡å‹ï¼‰ğŸ”¥

---

(32) [26:45-27:08] **And that's something really important in benchmarks as wellâ€” capturing the nuance of there isn't one obviously best model, because otherwise that company would just have 100% market share of everyone's API calls. The question is why are some models better suited for different use cases than others, and usually if a model is obviously doing way better than everyone else in a certain area, a lot of the other ones catch up very quickly.**

æˆ‘è§‰å¾—åœ¨åŸºå‡†æµ‹è¯•ä¸­éå¸¸é‡è¦çš„ä¸€ç‚¹æ˜¯æ•æ‰è¿™ç§ç»†å¾®å·®åˆ«â€”â€”ä¸å­˜åœ¨ä¸€ä¸ªæ˜æ˜¾æœ€å¥½çš„æ¨¡å‹ï¼Œå¦åˆ™é‚£å®¶å…¬å¸æ—©å°±å äº†æ‰€æœ‰ **API** è°ƒç”¨ 100% çš„å¸‚åœºä»½é¢äº†ã€‚é—®é¢˜åœ¨äºä¸ºä»€ä¹ˆæœ‰äº›æ¨¡å‹æ›´é€‚åˆæŸäº›ä½¿ç”¨åœºæ™¯ï¼Ÿå¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨æŸä¸ªé¢†åŸŸæ˜æ˜¾é¥é¥é¢†å…ˆï¼Œé€šå¸¸å…¶ä»–æ¨¡å‹ä¼šå¾ˆå¿«è¿½èµ¶ä¸Šæ¥ã€‚

è§£æï¼š
* **nuance** /ËˆnjuËÉ‘Ëns/ ğŸ”¥ï¼šåè¯ï¼Œç»†å¾®å·®åˆ«ã€å¾®å¦™ä¹‹å¤„
* **market share**ï¼šå¸‚åœºä»½é¢
* **catch up** ğŸ”¥ï¼šè¿½èµ¶ä¸Šæ¥

---

(33) [27:08-27:37] **So how can you be constantly tracking the developments? And I think that's where benchmarks are just so incredibly valuable because most people don't have the luxury of being able to swap out different models all day every day. I think really just getting to the point where it clearly is a huge leap in instruction following or it clearly is a huge leap in latencyâ€” and that makes someone decide to switch out their models.**

æ‰€ä»¥æ€ä¹ˆæŒç»­è¿½è¸ªæœ€æ–°å‘å±•ï¼Ÿæˆ‘è§‰å¾—è¿™å°±æ˜¯åŸºå‡†æµ‹è¯•ç‰¹åˆ«æœ‰ä»·å€¼çš„åœ°æ–¹ï¼Œå› ä¸ºå¤§å¤šæ•°äººæ²¡æ¡ä»¶æ¯å¤©ä¸æ–­åˆ‡æ¢è¯•ç”¨ä¸åŒæ¨¡å‹ã€‚é€šå¸¸æ˜¯çœ‹åˆ°æŸä¸ªæ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ªæˆ–å»¶è¿Ÿä¸Šæœ‰äº†æ˜æ˜¾çš„é£è·ƒï¼Œæ‰ä¼šè®©äººä¸‹å†³å¿ƒæ¢æ¨¡å‹ã€‚

è§£æï¼š
* **swap out**ï¼šçŸ­è¯­ï¼Œæ›¿æ¢
* **leap** /liËp/ï¼šåè¯ï¼Œé£è·ƒ
* **switch out**ï¼šçŸ­è¯­ï¼Œæ¢æ‰ã€æ›¿æ¢

---

(34) [27:37-28:00] **I got a question for you guys on different tech. I'm sort of curious to get your takeâ€” maybe inspired by Brooke's state of voice AI. Something I've been thinking a lot about is this reality that state-of-the-art models are moving forward, but they're largely thinking models. They're big and they're slow.**

å¦‚æœå¯ä»¥çš„è¯ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºä¸åŒæŠ€æœ¯æ–¹å‘çš„é—®é¢˜ã€‚å—åˆ° **Brooke** ä¹‹å‰å…³äºè¯­éŸ³ AI ç°çŠ¶çš„å¯å‘â€”â€”æˆ‘ä¸€ç›´åœ¨æƒ³ä¸€ä¸ªç°å®é—®é¢˜ï¼šæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ä¸æ–­è¿›æ­¥ï¼Œä½†å®ƒä»¬å¤§å¤šæ˜¯æ€è€ƒå‹æ¨¡å‹ï¼Œåˆå¤§åˆæ…¢ã€‚

è§£æï¼š
* **state-of-the-art** ğŸ”¥ï¼šæœ€å…ˆè¿›çš„ã€æœ€å‰æ²¿çš„
* **thinking models**ï¼šæ€è€ƒå‹æ¨¡å‹ï¼ˆæŒ‡ä½¿ç”¨é“¾å¼æ€è€ƒ/æ¨ç†çš„æ¨¡å‹ï¼‰ğŸ”¥

---

(35) [28:00-28:30] **And so I'm curious how you guys are thinking about this or seeing customers think about thisâ€” at some point we can't continue to rely on models from 2024. And thinkingâ€” RL is clearly highly effective. So I'm curious maybe Quinn, at Pipecatâ€” how are you thinking about thinking models in the world of voice AI and these things that take longer? Are you guys starting to wrestle with that?**

æˆ‘æƒ³çŸ¥é“ä½ ä»¬æ€ä¹ˆçœ‹è¿™ä¸ªé—®é¢˜â€”â€”åœ¨æŸä¸ªæ—¶ç‚¹ä¸Šæˆ‘ä»¬æ€»ä¸èƒ½ä¸€ç›´ä¾èµ– 2024 å¹´çš„è€æ¨¡å‹ã€‚æ€è€ƒèƒ½åŠ›ã€å¼ºåŒ–å­¦ä¹ æ˜¾ç„¶æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚**Quinn**ï¼Œä½ ä»¬åœ¨ **Pipecat** æ˜¯æ€ä¹ˆæ€è€ƒ"æ€è€ƒå‹æ¨¡å‹åœ¨è¯­éŸ³ AI ä¸­çš„è§’è‰²"è¿™ä¸ªé—®é¢˜çš„ï¼Ÿè¿™äº›éœ€è¦æ›´é•¿æ—¶é—´çš„æ¨¡å‹â€”â€”ä½ ä»¬å¼€å§‹é¢å¯¹è¿™ä¸ªæŒ‘æˆ˜äº†å—ï¼Ÿ

è§£æï¼š
* **RL (Reinforcement Learning)** ğŸ”¥ï¼šå¼ºåŒ–å­¦ä¹ 
* **wrestle with** /ËˆresÉ™l/ï¼šçŸ­è¯­ï¼ŒåŠªåŠ›åº”å¯¹ã€çº ç»“äº
* **rely on**ï¼šçŸ­è¯­ï¼Œä¾èµ–

---

(36) [28:30-28:55] **Yeah, there's two things that feel interesting in terms of how 2026 is likely to evolve. One is we're increasingly living in a world for all agent use cases where multiple models and multiple inference loops are really valuable, and we're starting to figure out how to build those AI engineering, software engineering shapes. So a lot of the stuff we're helping customers deploy now feel like a thinking fast and slow split,**

æ˜¯çš„ï¼Œå…³äº 2026 å¹´å¯èƒ½æ€ä¹ˆå‘å±•ï¼Œæœ‰ä¸¤ä¸ªæœ‰æ„æ€çš„æ–¹å‘ã€‚ç¬¬ä¸€ä¸ªæ˜¯æˆ‘ä»¬è¶Šæ¥è¶Šç”Ÿæ´»åœ¨ä¸€ä¸ªå¤šæ¨¡å‹ã€å¤šæ¨ç†å¾ªç¯çœŸæ­£æœ‰ä»·å€¼çš„ä¸–ç•Œé‡Œâ€”â€”å¯¹æ‰€æœ‰ **Agent** ä½¿ç”¨åœºæ™¯éƒ½æ˜¯å¦‚æ­¤â€”â€”è€Œæˆ‘ä»¬æ­£åœ¨æ‘¸ç´¢æ€ä¹ˆæ„å»ºè¿™äº› AI å·¥ç¨‹å’Œè½¯ä»¶å·¥ç¨‹çš„æ¶æ„ã€‚æˆ‘ä»¬å¸®å®¢æˆ·éƒ¨ç½²çš„å¾ˆå¤šä¸œè¥¿éƒ½åƒæ˜¯ä¸€ç§"å¿«æ€è€ƒä¸æ…¢æ€è€ƒ"çš„æ‹†åˆ†ï¼Œ

è§£æï¼š
* **inference loops**ï¼šæ¨ç†å¾ªç¯ ğŸ”¥
* **thinking fast and slow** ğŸ”¥ï¼šå¿«æ€è€ƒä¸æ…¢æ€è€ƒï¼ˆå¼•ç”¨ Daniel Kahneman çš„ç†è®ºï¼ŒSystem 1 vs System 2ï¼‰

---

(37) [28:55-29:14] **where we have a fast voice loop and then various kinds of async or long running or parallel inference processesâ€” things like guard rails, things like pulling tool calling out of the fast loop because even if your tool calling is very accurateâ€” which you have crackedâ€” you still have a latency penalty at least with a traditional architecture on the tool call.**

æœ‰ä¸€ä¸ªå¿«é€Ÿçš„è¯­éŸ³å¾ªç¯ï¼Œç„¶åæœ‰å„ç§å¼‚æ­¥çš„ã€é•¿æ—¶é—´è¿è¡Œçš„æˆ–å¹¶è¡Œçš„æ¨ç†è¿‡ç¨‹â€”â€”æ¯”å¦‚æŠ¤æ æœºåˆ¶ï¼Œæ¯”å¦‚æŠŠå‡½æ•°è°ƒç”¨ä»å¿«é€Ÿå¾ªç¯ä¸­å‰¥ç¦»å‡ºæ¥ã€‚å› ä¸ºå³ä½¿ä½ çš„å‡½æ•°è°ƒç”¨éå¸¸å‡†ç¡®â€”â€”è¿™ä¸ªä½ ä»¬å·²ç»æ”»å…‹äº†â€”â€”åœ¨ä¼ ç»Ÿæ¶æ„ä¸‹ï¼Œå‡½æ•°è°ƒç”¨ä»ç„¶ä¼šå¸¦æ¥å»¶è¿Ÿä»£ä»·ã€‚

è§£æï¼š
* **guard rails** ğŸ”¥ï¼šæŠ¤æ ï¼ˆAI å®‰å…¨æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹äº§ç”Ÿä¸å½“è¾“å‡ºï¼‰
* **async**ï¼šå¼‚æ­¥çš„ï¼ˆasynchronous çš„ç¼©å†™ï¼‰
* **crack**ï¼šåŠ¨è¯ï¼Œæ”»å…‹ã€è§£å†³ï¼ˆå£è¯­ï¼‰

---

(38) [29:14-29:39] **So we can pull that outâ€” that's useful. Some tool calls are just never going to return fast because they depend on back-end systems. Sometimes you really want long-running stuff to happen and inject back into the voice context. Those are really fun and interesting software engineering problems and they feel very high leverage. So I think the multi-model thinking fast and slow, long-running processes, shared context stuff is going to be a big part of what we all think about in 2026.**

æŠŠè¿™éƒ¨åˆ†å‰¥ç¦»å‡ºæ¥æ˜¯æœ‰ç”¨çš„ã€‚æœ‰äº›å‡½æ•°è°ƒç”¨å°±æ˜¯ä¸å¯èƒ½å¿«é€Ÿè¿”å›ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–åç«¯ç³»ç»Ÿã€‚æœ‰æ—¶å€™ä½ ç¡®å®å¸Œæœ›é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡æ‰§è¡Œå®Œç„¶åæ³¨å…¥å›è¯­éŸ³ä¸Šä¸‹æ–‡ä¸­ã€‚è¿™äº›éƒ½æ˜¯éå¸¸æœ‰è¶£çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ï¼Œè€Œä¸”å¾ˆæœ‰æ æ†æ•ˆåº”ã€‚æ‰€ä»¥å¤šæ¨¡å‹çš„å¿«æ…¢æ€è€ƒã€é•¿æ—¶é—´è¿è¡Œçš„è¿›ç¨‹ã€å…±äº«ä¸Šä¸‹æ–‡ï¼Œä¼šæˆä¸º 2026 å¹´æˆ‘ä»¬éƒ½åœ¨æ€è€ƒçš„é‡è¦è¯¾é¢˜ã€‚

è§£æï¼š
* **inject back into**ï¼šæ³¨å…¥å›...ä¸­ ğŸ”¥
* **high leverage** /ËˆliËvÉ™rÉªdÊ’/ ğŸ”¥ï¼šé«˜æ æ†çš„ï¼ˆæŠ•å…¥å°ã€å½±å“å¤§ï¼‰
* **shared context**ï¼šå…±äº«ä¸Šä¸‹æ–‡

---

(39) [29:39-30:03] **The other thing I think is probably going to happen is the pendulum is going to swing a little bit back from the thinking models. It was really exciting to make that breakthrough, and DeepSeek really pushed the American Foundation Labs to invest in that direction because that was a proof point that if they didn't, they were leaving a whole bunch of capability on the table.**

å¦ä¸€ä¸ªæˆ‘è§‰å¾—å¯èƒ½ä¼šå‘ç”Ÿçš„äº‹æ˜¯ï¼Œé’Ÿæ‘†ä¼šä»æ€è€ƒå‹æ¨¡å‹é‚£è¾¹ç¨å¾®æ‘†å›æ¥ä¸€äº›ã€‚èƒ½å–å¾—é‚£ä¸ªçªç ´ç¡®å®éå¸¸ä»¤äººå…´å¥‹ï¼Œ**DeepSeek** ç¡®å®æ¨åŠ¨äº†ç¾å›½åŸºç¡€æ¨¡å‹å®éªŒå®¤å¾€é‚£ä¸ªæ–¹å‘æŠ•èµ„â€”â€”å› ä¸ºé‚£æ˜¯ä¸€ä¸ªè¯æ®ï¼Œè¯æ˜å¦‚æœä¸åšå°±ä¼šç™½ç™½æµªè´¹ä¸€å¤§å †èƒ½åŠ›ã€‚

è§£æï¼š
* **pendulum swing** ğŸ”¥ï¼šé’Ÿæ‘†æ•ˆåº”ï¼ˆäº‹ç‰©åœ¨ä¸¤ä¸ªæç«¯ä¹‹é—´æ‘‡æ‘†ï¼‰
* **proof point**ï¼šè¯æ®ç‚¹ã€è®ºæ®
* **leave on the table** ğŸ”¥ï¼šçŸ­è¯­ï¼Œç™½ç™½æµªè´¹ã€æœªèƒ½åˆ©ç”¨

---

(40) [30:03-30:28] **That was a year ago. And I think a bunch of things including that there are other interesting areas to explore and use cases like voice that really don't work well in a thinking model context are growing a lot. So there's customer pull and commercial pressure and engineering and research interest in that pendulum swinging back. And we'll see mixture of experts models in the open weights world and distillations in the big lab world**

é‚£æ˜¯ä¸€å¹´å‰çš„äº‹äº†ã€‚åŒ…æ‹¬è¿˜æœ‰å…¶ä»–æœ‰æ„æ€çš„é¢†åŸŸå¯ä»¥æ¢ç´¢ï¼Œè€Œä¸”åƒè¯­éŸ³è¿™æ ·åœ¨æ€è€ƒå‹æ¨¡å‹ä¸Šæ•ˆæœä¸å¥½çš„ä½¿ç”¨åœºæ™¯æ­£åœ¨å¿«é€Ÿå¢é•¿â€”â€”æ‰€ä»¥æœ‰å®¢æˆ·éœ€æ±‚æ‹‰åŠ¨ã€å•†ä¸šå‹åŠ›ã€å·¥ç¨‹å’Œç ”ç©¶å…´è¶£åœ¨æ¨åŠ¨é’Ÿæ‘†å›æ‘†ã€‚æˆ‘ä»¬ä¼šçœ‹åˆ°å¼€æºæƒé‡é¢†åŸŸå‡ºç°æ›´å¤šæ··åˆä¸“å®¶æ¨¡å‹ï¼Œå¤§å®éªŒå®¤é‚£è¾¹ä¼šæœ‰æ›´å¤šè’¸é¦æ¨¡å‹ï¼Œ

è§£æï¼š
* **mixture of experts (MoE)** ğŸ”¥ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆä¸€ç§é«˜æ•ˆçš„æ¨¡å‹æ¶æ„ï¼‰
* **distillation** ğŸ”¥ï¼šè’¸é¦ï¼ˆæŠŠå¤§æ¨¡å‹çš„çŸ¥è¯†å‹ç¼©åˆ°å°æ¨¡å‹ä¸­ï¼‰
* **open weights**ï¼šå¼€æºæƒé‡
* **customer pull**ï¼šå®¢æˆ·éœ€æ±‚æ‹‰åŠ¨

---

(41) [30:28-30:46] **that are much more aimed at time to first tokenâ€” getting more attention this year than they did last year. What do you think, Brooke? Yeah, we're definitely seeing the sameâ€” people are doing a lot more background agents or multi-agents working in parallel,**

è¿™äº›æ¨¡å‹ä¼šæ›´æ³¨é‡é¦– **token** å“åº”æ—¶é—´ï¼Œä»Šå¹´ä¼šæ¯”å»å¹´å¾—åˆ°æ›´å¤šå…³æ³¨ã€‚**Brooke** ä½ æ€ä¹ˆçœ‹ï¼Ÿå¯¹ï¼Œæˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†åŒæ ·çš„è¶‹åŠ¿â€”â€”äººä»¬åœ¨åšæ›´å¤šçš„åå° **Agent** æˆ–è€…å¤š **Agent** å¹¶è¡Œè¿è¡Œï¼Œ

è§£æï¼š
* **aimed at**ï¼šé’ˆå¯¹ã€é¢å‘
* **background agents**ï¼šåå°æ™ºèƒ½ä½“ ğŸ”¥
* **working in parallel**ï¼šå¹¶è¡Œè¿è¡Œ

---

(42) [30:46-31:13] **and having those background agents do self-correcting or self-healing systemsâ€” having it in loops or being able to send off an agent to execute on something and come backâ€” especially for these more intensive tool calls. We're constantly pushing the bounds of what voice agents can do in real time and a lot of these systemsâ€” especially legacy agent systems, aka agent systems that were set up for not real-time use cases. How do you adapt those to voice agents?**

ç”¨é‚£äº›åå° **Agent** æ¥åšè‡ªçº æ­£æˆ–è‡ªä¿®å¤ç³»ç»Ÿâ€”â€”è®©å®ƒä»¬åœ¨å¾ªç¯ä¸­è¿è¡Œï¼Œæˆ–è€…æ´¾ä¸€ä¸ª **Agent** å»æ‰§è¡ŒæŸäº›ä»»åŠ¡ç„¶åå›æ¥â€”â€”ç‰¹åˆ«æ˜¯å¯¹äºæ›´è€—æ—¶çš„å‡½æ•°è°ƒç”¨ã€‚æˆ‘ä»¬ä¸€ç›´åœ¨æ¨åŠ¨è¯­éŸ³ **Agent** å®æ—¶èƒ½åŠ›çš„è¾¹ç•Œï¼Œè€Œå¾ˆå¤šç³»ç»Ÿâ€”â€”ç‰¹åˆ«æ˜¯é—ç•™çš„ **Agent** ç³»ç»Ÿï¼Œå°±æ˜¯å½“åˆä¸ºéå®æ—¶åœºæ™¯æ­å»ºçš„â€”â€”æ€ä¹ˆæŠŠå®ƒä»¬æ”¹é€ æˆé€‚åˆè¯­éŸ³ **Agent** çš„å‘¢ï¼Ÿ

è§£æï¼š
* **self-correcting / self-healing** ğŸ”¥ï¼šè‡ªçº æ­£/è‡ªä¿®å¤
* **legacy systems** /ËˆleÉ¡É™si/ ğŸ”¥ï¼šé—ç•™ç³»ç»Ÿã€æ—§ç³»ç»Ÿ
* **push the bounds**ï¼šæ¨åŠ¨è¾¹ç•Œ

---

(43) [31:13-31:36] **I think a lot of times having background as well as more responsive agents can be a great solution. But there are some pitfalls where we're seeing people try to use the same agents for chat as they're using for voice. And this is where people are running into a lot of issues. I think it's going to be hard to solve because ultimately you're just trying to use the same reasoning for two very different systems.**

æˆ‘è§‰å¾—å¾ˆå¤šæ—¶å€™ï¼ŒåŒæ—¶æ‹¥æœ‰åå° **Agent** å’Œæ›´å¿«å“åº”çš„ **Agent** æ˜¯ä¸€ä¸ªä¸é”™çš„è§£å†³æ–¹æ¡ˆã€‚ä½†ä¹Ÿæœ‰äº›å‘â€”â€”æˆ‘ä»¬çœ‹åˆ°æœ‰äººè¯•å›¾æŠŠç”¨äºèŠå¤©çš„åŒä¸€ä¸ª **Agent** ç›´æ¥ç”¨äºè¯­éŸ³ã€‚è¿™å°±å‡ºäº†å¾ˆå¤šé—®é¢˜ï¼Œè€Œä¸”å¾ˆéš¾è§£å†³ï¼Œå› ä¸ºä½ æ˜¯åœ¨å¯¹ä¸¤ä¸ªæˆªç„¶ä¸åŒçš„ç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨ç†é€»è¾‘ã€‚

è§£æï¼š
* **pitfall** /ËˆpÉªtfÉ”Ël/ ğŸ”¥ï¼šåè¯ï¼Œé™·é˜±ã€éšæ‚£
* **run into issues**ï¼šçŸ­è¯­ï¼Œé‡åˆ°é—®é¢˜
* **responsive**ï¼šå½¢å®¹è¯ï¼Œå“åº”å¿«çš„

---

(44) [31:36-31:59] **What you want to see in chat is going to look very different than what you want to see in a voice system. So that's one big areaâ€” looking at benchmarks you might say "great it follows instructions" but now I'm adding so many layers of abstraction because I'm trying to retrofit these systems to be reused across lots of different cases. Have you been seeing that in your customers, Zachâ€” how much their backend systems are impacting your performance?**

ä½ åœ¨èŠå¤©ä¸­æƒ³çœ‹åˆ°çš„è·Ÿè¯­éŸ³ç³»ç»Ÿä¸­æƒ³çœ‹åˆ°çš„ä¼šéå¸¸ä¸ä¸€æ ·ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„é¢†åŸŸâ€”â€”çœ‹åŸºå‡†æµ‹è¯•ä½ å¯èƒ½è¯´"å¾ˆå¥½ï¼Œå®ƒèƒ½éµå¾ªæŒ‡ä»¤"ï¼Œä½†ç„¶åä½ åŠ äº†é‚£ä¹ˆå¤šæŠ½è±¡å±‚æ¥æ”¹é€ è¿™äº›ç³»ç»Ÿä»¥ä¾¿åœ¨å¾ˆå¤šåœºæ™¯ä¸‹å¤ç”¨ã€‚**Zach**ï¼Œä½ åœ¨å®¢æˆ·é‚£è¾¹æœ‰çœ‹åˆ°è¿™ä¸ªæƒ…å†µå—ï¼Ÿä»–ä»¬çš„åç«¯ç³»ç»Ÿåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå½±å“äº†ä½ ä»¬çš„è¡¨ç°ï¼Ÿ

è§£æï¼š
* **layers of abstraction**ï¼šæŠ½è±¡å±‚ ğŸ”¥
* **retrofit** /ËˆretrÉ™ÊŠfÉªt/ ğŸ”¥ï¼šåŠ¨è¯ï¼Œæ”¹é€ ã€æ”¹è£…ï¼ˆä½¿æ—§ç³»ç»Ÿé€‚åº”æ–°éœ€æ±‚ï¼‰

---

(45) [31:59-32:22] **Yeah, we definitely have a lot of interesting use cases where the thinking fast and slow bitsâ€” and we're always biased to think about this not just as an orchestration problem but as a modeling problem. We now know that we can interleave thinking tokens during training for example, and GLM 4.7 is quite interesting in that regardâ€” can we get more dynamic thinking out of the models,**

æ˜¯çš„ï¼Œæˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†å¾ˆå¤šæœ‰æ„æ€çš„ä½¿ç”¨åœºæ™¯ä¸­çš„å¿«æ…¢æ€è€ƒã€‚æˆ‘ä»¬æ€»æ˜¯å€¾å‘äºä¸åªæŠŠå®ƒå½“ä½œç¼–æ’é—®é¢˜ï¼Œè€Œæ˜¯å½“ä½œå»ºæ¨¡é—®é¢˜æ¥æ€è€ƒã€‚æ¯”å¦‚ç°åœ¨æˆ‘ä»¬çŸ¥é“å¯ä»¥åœ¨è®­ç»ƒä¸­äº¤é”™æ’å…¥æ€è€ƒ **token**â€”â€”**GLM 4.7** åœ¨è¿™æ–¹é¢å°±å¾ˆæœ‰æ„æ€â€”â€”æˆ‘ä»¬èƒ½ä¸èƒ½ä»æ¨¡å‹ä¸­è·å¾—æ›´åŠ¨æ€çš„æ€è€ƒèƒ½åŠ›ï¼Œ

è§£æï¼š
* **orchestration** /ËŒÉ”ËrkÉªËˆstreÉªÊƒÉ™n/ ğŸ”¥ï¼šç¼–æ’ï¼ˆå¤šä¸ªç»„ä»¶çš„åè°ƒè°ƒåº¦ï¼‰
* **interleave** /ËŒÉªntÉ™rËˆliËv/ ğŸ”¥ï¼šåŠ¨è¯ï¼Œäº¤é”™ã€äº¤æ›¿
* **thinking tokens**ï¼šæ€è€ƒ tokenï¼ˆæ¨¡å‹æ¨ç†æ—¶ç”Ÿæˆçš„å†…éƒ¨æ¨ç†æ­¥éª¤ï¼‰

---

(46) [32:22-32:50] **where in the right moment we can help them figure out how to think and guide them more. And so it's always this interesting game of what's the right long-term modeling answer and what's the short-term thing we can do now knowing how to manipulate the state of the conversation. Which is alwaysâ€” to Quinn's pointâ€” a fun set of engineering challenges.**

åœ¨åˆé€‚çš„æ—¶æœºå¸®åŠ©å®ƒä»¬æƒ³æ¸…æ¥šæ€ä¹ˆæ€è€ƒï¼Œå¼•å¯¼å®ƒä»¬æ›´å¥½åœ°æ€è€ƒã€‚æ‰€ä»¥ä¸€ç›´éƒ½æ˜¯è¿™ç§æœ‰æ„æ€çš„åšå¼ˆâ€”â€”é•¿æœŸæ­£ç¡®çš„å»ºæ¨¡æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼ŸçŸ­æœŸæˆ‘ä»¬èƒ½åšä»€ä¹ˆï¼Ÿæ€ä¹ˆæ“æ§å¯¹è¯çŠ¶æ€æ¥å®ç°ï¼Ÿç”¨ **Quinn** çš„è¯è¯´ï¼Œè¿™ç¡®å®æ˜¯ä¸€ç»„å¾ˆæœ‰è¶£çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚

è§£æï¼š
* **manipulate** /mÉ™ËˆnÉªpjuleÉªt/ï¼šåŠ¨è¯ï¼Œæ“æ§ã€æ“ä½œ
* **state of the conversation**ï¼šå¯¹è¯çŠ¶æ€

---

(47) [32:50-33:18] **But this is one thing I haven't been able to capture well in evalâ€” I'm curious if you guys have. Because it goes back to the feel of the conversation. One of the interesting challenges of these asynchronous operations behind the scenes trying to drive back inâ€” maybe if you examine the end-to-end experience it might look fairly accurate, but the question is how is the user experiencing these dual brains interacting?**

ä½†æœ‰ä¸€ä»¶äº‹æˆ‘è§‰å¾—åœ¨è¯„æµ‹ä¸­ä¸€ç›´æ²¡èƒ½å¾ˆå¥½åœ°æ•æ‰åˆ°â€”â€”ä¸çŸ¥é“ä½ ä»¬æœ‰æ²¡æœ‰ã€‚å› ä¸ºè¿™åˆå›åˆ°äº†å¯¹è¯çš„"æ„Ÿè§‰"â€”â€”è¿™äº›å¹•åçš„å¼‚æ­¥æ“ä½œæƒ³è¦é©±åŠ¨å›å¯¹è¯ä¸­ï¼Œå¦‚æœä½ æ£€æŸ¥ç«¯åˆ°ç«¯çš„ä½“éªŒå¯èƒ½çœ‹èµ·æ¥ç›¸å½“å‡†ç¡®ï¼Œä½†é—®é¢˜æ˜¯ç”¨æˆ·åˆ°åº•åœ¨æ€ä¹ˆä½“éªŒè¿™ç§"åŒè„‘"äº¤äº’çš„ï¼Ÿ

è§£æï¼š
* **end-to-end experience**ï¼šç«¯åˆ°ç«¯ä½“éªŒ ğŸ”¥
* **dual brains**ï¼šåŒè„‘ï¼ˆæ¯”å–»å¿«æ¨¡å‹å’Œæ…¢æ¨¡å‹çš„ååŒï¼‰
* **behind the scenes**ï¼šå¹•å

---

(48) [33:18-33:50] **And you mentioned guardrails, Quinnâ€” this one has always befuddled me because by the time the guardrail has kicked in, you're sort of past the moment. And the thinking momentsâ€” the models are too slow to keep up. So by the time they have an answer, maybe you've moved on. And I find the evals can mislead meâ€” you get this boost from thinking performance that helps tool calling, but then when I go have the conversation it feels awkward.**

ä½ æåˆ°çš„æŠ¤æ æœºåˆ¶ï¼Œ**Quinn**â€”â€”è¿™ä¸ªä¸€ç›´è®©æˆ‘å›°æƒ‘ï¼Œå› ä¸ºç­‰æŠ¤æ ç”Ÿæ•ˆæ—¶ä½ å·²ç»è¿‡äº†é‚£ä¸ªæ—¶åˆ»äº†ã€‚æ€è€ƒçš„ç¬é—´â€”â€”æ¨¡å‹ä¹Ÿå¤ªæ…¢è·Ÿä¸ä¸ŠèŠ‚å¥ï¼Œç­‰å®ƒæœ‰äº†ç­”æ¡ˆä½ å¯èƒ½å·²ç»ç§»åˆ°ä¸‹ä¸€ä¸ªè¯é¢˜äº†ã€‚æ‰€ä»¥æˆ‘å‘ç°çœ‹è¯„æµ‹æ•°æ®æœ‰æ—¶ä¼šè¯¯å¯¼æˆ‘â€”â€”æ€è€ƒèƒ½åŠ›ç¡®å®åœ¨å‡½æ•°è°ƒç”¨æ–¹é¢å¸¦æ¥äº†æå‡ï¼Œä½†å½“æˆ‘çœŸæ­£å»å¯¹è¯çš„æ—¶å€™ï¼Œæ„Ÿè§‰å°±å¾ˆå°´å°¬ã€‚

è§£æï¼š
* **befuddled** /bÉªËˆfÊŒdÉ™ld/ï¼šå½¢å®¹è¯ï¼Œå›°æƒ‘çš„
* **kick in** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå¼€å§‹ç”Ÿæ•ˆ
* **keep up**ï¼šçŸ­è¯­ï¼Œè·Ÿä¸Š
* **mislead** /mÉªsËˆliËd/ï¼šåŠ¨è¯ï¼Œè¯¯å¯¼

---

(49) [33:50-34:08] **That's one of those thingsâ€” I'm curious if you guys wrestled with that or found ways to help. Do you change the evals at all? Yeah, we've been working a lot on prosody evals in general. So like pauses and what are natural pauses, what is natural intonation, voice quality, etc. A lot of those things fall into that set.**

è¿™æ˜¯æˆ‘å¾ˆå¥½å¥‡çš„â€”â€”ä½ ä»¬æœ‰æ²¡æœ‰å¾ˆçº ç»“è¿™ä¸ªé—®é¢˜æˆ–æ‰¾åˆ°è§£å†³åŠæ³•ï¼Ÿä½ ä»¬ä¼šè°ƒæ•´è¯„æµ‹å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨åšå¾ˆå¤šå…³äºéŸµå¾‹è¯„æµ‹çš„å·¥ä½œâ€”â€”æ¯”å¦‚åœé¡¿ã€ä»€ä¹ˆæ˜¯è‡ªç„¶çš„åœé¡¿ã€ä»€ä¹ˆæ˜¯è‡ªç„¶çš„è¯­è°ƒã€è¯­éŸ³è´¨é‡ç­‰ç­‰ã€‚è¿™äº›éƒ½å½’åˆ°é‚£ä¸€ç±»è¯„æµ‹é‡Œã€‚

è§£æï¼š
* **prosody** /ËˆprÉ’sÉ™di/ ğŸ”¥ï¼šåè¯ï¼ŒéŸµå¾‹ï¼ˆè¯­éŸ³çš„èŠ‚å¥ã€é‡éŸ³ã€è¯­è°ƒæ¨¡å¼ï¼‰
* **intonation** /ËŒÉªntÉ™ËˆneÉªÊƒÉ™n/ï¼šåè¯ï¼Œè¯­è°ƒ
* **wrestle with**ï¼šçŸ­è¯­ï¼Œçº ç»“äºã€åŠªåŠ›åº”å¯¹

---

(50) [34:08-34:38] **I think what's important with naturalness is finding all the cases that felt weird and putting words to what was weird about that. Because often it's like "the tool call came back 10 seconds after the fact"â€” that's an insane amount of time. But this happens honestly. I think we've probably all been there where someone's like "why is my voice agent slow?"**

æˆ‘è§‰å¾—åœ¨è‡ªç„¶åº¦æ–¹é¢çœŸæ­£é‡è¦çš„æ˜¯æ‰¾åˆ°æ‰€æœ‰æ„Ÿè§‰å¥‡æ€ªçš„æ¡ˆä¾‹ï¼Œç„¶åç”¨è¯­è¨€æè¿°å‡ºæ¥åˆ°åº•å“ªé‡Œå¥‡æ€ªã€‚å› ä¸ºç»å¸¸å°±æ˜¯â€”â€”"å‡½æ•°è°ƒç”¨ 10 ç§’åæ‰è¿”å›"â€”â€”é‚£æ˜¯å¾ˆé•¿çš„æ—¶é—´äº†ï¼Œä½†è¯´å®è¯è¿™ç¡®å®ä¼šå‘ç”Ÿã€‚æˆ‘æƒ³æˆ‘ä»¬å¤§æ¦‚éƒ½é‡åˆ°è¿‡æœ‰äººé—®"ä¸ºä»€ä¹ˆæˆ‘çš„è¯­éŸ³ **Agent** è¿™ä¹ˆæ…¢ï¼Ÿ"

è§£æï¼š
* **put words to** ğŸ”¥ï¼šçŸ­è¯­ï¼Œç”¨è¯­è¨€æè¿°ï¼ˆéš¾ä»¥è¡¨è¾¾çš„æ„Ÿå—ï¼‰
* **after the fact**ï¼šçŸ­è¯­ï¼Œäº‹å
* **insane**ï¼šå½¢å®¹è¯ï¼Œç–¯ç‹‚çš„ï¼ˆå£è¯­ä¸­è¡¨ç¤ºç¨‹åº¦æå¤§ï¼‰

---

(51) [34:38-35:04] **And you're like "Well, it's hard when your tool call takes that much latency." I've literally had conversations where I'm like, "What's your tool call latency? Let's look at the traces." And it's like, "Oh, that might be the problem." But then quantifyingâ€” things are out of order, the conversation ordering is inaccurate, or it's repeating itself or getting stuck in loops.**

ç„¶åä½ å°±è¯´ï¼š"å—¯ï¼Œå‡½æ•°è°ƒç”¨å»¶è¿Ÿé‚£ä¹ˆé«˜å°±å¾ˆéš¾åŠäº†ã€‚"æˆ‘ç¡®å®æœ‰è¿‡é‚£ç§å¯¹è¯â€”â€”"ä½ çš„å‡½æ•°è°ƒç”¨å»¶è¿Ÿæ˜¯å¤šå°‘ï¼Ÿæˆ‘ä»¬çœ‹çœ‹è¿½è¸ªè®°å½•ã€‚"ç„¶åå‘ç°"å“¦ï¼Œè¿™å¯èƒ½å°±æ˜¯é—®é¢˜æ‰€åœ¨ã€‚"æ¥ä¸‹æ¥å°±æ˜¯é‡åŒ–è¿™äº›é—®é¢˜â€”â€”å¯¹è¯é¡ºåºä¹±äº†ã€æ’åˆ—ä¸å‡†ç¡®ã€æ¨¡å‹å¼€å§‹è‡ªæˆ‘é‡å¤ã€æˆ–è€…é™·å…¥äº†å¾ªç¯ã€‚

è§£æï¼š
* **traces** ğŸ”¥ï¼šè¿½è¸ªè®°å½•ï¼ˆåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç”¨äºè¿½è¸ªè¯·æ±‚é“¾è·¯çš„æ—¥å¿—ï¼‰
* **stuck in loops**ï¼šé™·å…¥å¾ªç¯
* **out of order**ï¼šé¡ºåºæ··ä¹±

---

(52) [35:04-35:27] **Capturing what makes it really unnaturalâ€” the pauses or the intonation. But yeah, it's definitely hard when it's in the uncanny valley of just one beat off. That's the hardest to get. It's a lot easier when it's obviously an awkward pause or things are out of orderâ€” those we're able to catch. This is where I really like error analysisâ€” being able to go through and label.**

å¦‚ä½•æ•æ‰é‚£äº›è®©å®ƒå˜å¾—ä¸è‡ªç„¶çš„å› ç´ â€”â€”åœé¡¿ã€è¯­è°ƒã€‚æ˜¯çš„ï¼Œå½“å®ƒå¤„åœ¨"ææ€–è°·"åŒºåŸŸã€å°±å·®é‚£ä¹ˆä¸€ä¸ªèŠ‚æ‹çš„æ—¶å€™ç¡®å®æ˜¯æœ€éš¾æçš„ã€‚å¦‚æœæ˜æ˜¾æœ‰å°´å°¬åœé¡¿æˆ–é¡ºåºä¹±äº†åå€’æ¯”è¾ƒå®¹æ˜“æ•æ‰ã€‚è¿™å°±æ˜¯æˆ‘ç‰¹åˆ«å–œæ¬¢é”™è¯¯åˆ†æçš„åœ°æ–¹â€”â€”èƒ½å¤Ÿé€ä¸ªæ£€æŸ¥å’Œæ ‡æ³¨ã€‚

è§£æï¼š
* **uncanny valley** ğŸ”¥ï¼šææ€–è°·ï¼ˆäº‹ç‰©"å‡ ä¹åƒäººä½†åˆä¸å¤Ÿåƒ"æ—¶å¼•èµ·çš„ä¸é€‚æ„Ÿï¼‰
* **one beat off**ï¼šå·®ä¸€ä¸ªèŠ‚æ‹ï¼ˆéŸ³ä¹æœ¯è¯­ï¼Œæ¯”å–»æ—¶æœºå¾®å¦™ä¸å¯¹ï¼‰
* **error analysis** ğŸ”¥ï¼šé”™è¯¯åˆ†æ

---

(53) [35:27-35:57] **Haml Hussein talks a lot about this in his courseâ€” actually going through and labeling all of your calls that feel awkward or unnatural, then looking for trends in those cases. That can help you really narrow down. Again, this is more for task specific evals. I know we're almost at time. Zach, what do you think is still missing from benchmarks? What is not captured?**

**Haml Hussein** åœ¨ä»–çš„è¯¾ç¨‹ä¸­ç»å¸¸è°ˆåˆ°è¿™ä¸ªâ€”â€”é€ä¸ªæŠŠæ‰€æœ‰æ„Ÿè§‰å°´å°¬æˆ–ä¸è‡ªç„¶çš„é€šè¯æ ‡æ³¨å‡ºæ¥ï¼Œç„¶åå¯»æ‰¾è¿™äº›æ¡ˆä¾‹ä¸­çš„è¶‹åŠ¿ï¼Œè¿™èƒ½å¸®ä½ çœŸæ­£ç¼©å°é—®é¢˜èŒƒå›´ã€‚å½“ç„¶è¿™æ›´åå‘ç‰¹å®šä»»åŠ¡çš„è¯„æµ‹ã€‚æ—¶é—´å¿«åˆ°äº†â€”â€”**Zach**ï¼Œä½ è§‰å¾—åŸºå‡†æµ‹è¯•é‡Œè¿˜ç¼ºä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆæ²¡è¢«æ•æ‰åˆ°çš„ï¼Ÿ

è§£æï¼š
* **narrow down** ğŸ”¥ï¼šçŸ­è¯­ï¼Œç¼©å°èŒƒå›´
* **trends**ï¼šåè¯ï¼Œè¶‹åŠ¿ã€è§„å¾‹
* **task specific evals**ï¼šé¢å‘ç‰¹å®šä»»åŠ¡çš„è¯„æµ‹

---

(54) [35:57-36:26] **I think the benchmarks today actually feel fairly decent for where the models are at presently. There are so many obvious things that voice AI is really bad at. When you said about "it's off by just a beat"â€” one of my inner tests for when we've achieved human level voice AI is when they back-channel perfectly.**

æˆ‘è§‰å¾—ç›®å‰çš„åŸºå‡†æµ‹è¯•å¯¹äºæ¨¡å‹å½“å‰çš„æ°´å¹³æ¥è¯´å…¶å®è¿˜ç®—ä¸é”™ã€‚åªæ˜¯è¯­éŸ³ AI è¿˜æœ‰å¤ªå¤šæ˜æ˜¾åšå¾—ä¸å¥½çš„åœ°æ–¹ã€‚ä½ åˆšæ‰è¯´çš„"å°±å·®ä¸€ä¸ªèŠ‚æ‹"â€”â€”æˆ‘çš„ä¸€ä¸ªå†…å¿ƒæµ‹è¯•æ ‡å‡†æ˜¯ï¼šå½“è¯­éŸ³ AI çš„åé¦ˆæ€§åº”ç­”èƒ½åšåˆ°å®Œç¾çš„æ—¶å€™ï¼Œæˆ‘å°±çŸ¥é“æˆ‘ä»¬è¾¾åˆ°äº†æ¥è¿‘äººç±»æ°´å¹³çš„è¯­éŸ³ AIã€‚

è§£æï¼š
* **decent** /ËˆdiËsÉ™nt/ï¼šå½¢å®¹è¯ï¼Œä¸é”™çš„ã€è¿˜è¡Œçš„
* **back-channel** ğŸ”¥ï¼šåé¦ˆæ€§åº”ç­”ï¼ˆå¯¹è¯ä¸­çš„"å—¯""æ˜¯çš„""å¯¹"ç­‰å›åº”ï¼Œè¡¨ç¤ºåœ¨å¬ï¼‰

---

(55) [36:26-36:50] **Because back-channels are a great exampleâ€” they're either exactly correct and on the mark or they're awkward. Every attempt to back-channel as a system level thing to me has failed catastrophicallyâ€” they're just awkward. These small things are like a proxy to me. And I don't think we have any evals for thisâ€” any evals that are good at capturing the nuances of the conversation.**

å› ä¸ºåé¦ˆæ€§åº”ç­”å°±æ˜¯ä¸€ä¸ªç»ä½³ä¾‹å­â€”â€”å®ƒä»¬è¦ä¹ˆæ°åˆ°å¥½å¤„ï¼Œè¦ä¹ˆå°±å¾ˆå°´å°¬ã€‚ä»»ä½•åœ¨ç³»ç»Ÿå±‚é¢å°è¯•åšåé¦ˆæ€§åº”ç­”çš„åŠªåŠ›åœ¨æˆ‘çœ‹æ¥éƒ½æƒ¨è´¥äº†â€”â€”å°±æ˜¯å¾ˆå°´å°¬ã€‚è¿™ç±»å°ç»†èŠ‚å¯¹æˆ‘æ¥è¯´å°±æ˜¯ä¸€ç§ä»£ç†æŒ‡æ ‡ã€‚æˆ‘ä¸è§‰å¾—æˆ‘ä»¬æœ‰ä»»ä½•è¯„æµ‹èƒ½åšåˆ°è¿™ç‚¹â€”â€”ä¹Ÿæ²¡æœ‰è¯„æµ‹èƒ½å¾ˆå¥½åœ°æ•æ‰å¯¹è¯ä¸­çš„ç»†å¾®å·®åˆ«ã€‚

è§£æï¼š
* **on the mark** ğŸ”¥ï¼šçŸ­è¯­ï¼Œæ°åˆ°å¥½å¤„ã€ç²¾å‡†
* **catastrophically** /ËŒkÃ¦tÉ™ËˆstrÉ’fÉªkli/ï¼šå‰¯è¯ï¼Œç¾éš¾æ€§åœ°
* **nuances** /ËˆnjuËÉ‘ËnsÉªz/ï¼šåè¯ï¼Œç»†å¾®å·®åˆ«

---

(56) [36:50-37:22] **Does the "mhm" make sense in the right contextual moment? Another great example is we mistake prosody for a reasonable response to what the user said. When you're dialoguing with another human, the way I'm talking changes the way you respond. If I say something in a particular tone, the interpretation of that tone will change the way you generate text on the other side or change your prosody dynamicallyâ€”**

é‚£ä¸ª"å—¯å“¼"åœ¨æ­£ç¡®çš„è¯­å¢ƒæ—¶åˆ»æœ‰æ²¡æœ‰é“ç†ï¼Ÿå¦ä¸€ä¸ªä¾‹å­æ˜¯æˆ‘ä»¬æŠŠéŸµå¾‹è·Ÿå¯¹ç”¨æˆ·è¯´è¯å†…å®¹çš„åˆç†å›åº”ææ··äº†ã€‚å½“ä½ è·Ÿå¦ä¸€ä¸ªäººå¯¹è¯æ—¶ï¼Œæˆ‘è¯´è¯çš„æ–¹å¼ä¼šæ”¹å˜ä½ çš„å›åº”æ–¹å¼ã€‚å¦‚æœæˆ‘ç”¨ç‰¹å®šçš„è¯­æ°”è¯´äº†ä¸€å¥è¯ï¼Œå¯¹é‚£ä¸ªè¯­æ°”çš„ç†è§£ä¼šæ”¹å˜ä½ ç”Ÿæˆæ–‡å­—çš„æ–¹å¼ï¼Œæˆ–è€…åŠ¨æ€æ”¹å˜ä½ çš„éŸµå¾‹â€”â€”

è§£æï¼š
* **contextual moment**ï¼šè¯­å¢ƒæ—¶åˆ»
* **dialogue** /ËˆdaÉªÉ™lÉ’É¡/ï¼šåŠ¨è¯ï¼ˆæ­¤å¤„ç”¨ä½œåŠ¨è¯ï¼‰ï¼Œå¯¹è¯
* **dynamically**ï¼šå‰¯è¯ï¼ŒåŠ¨æ€åœ°

---

(57) [37:22-37:47] **My anger induces your anger or maybe slows it down. We have no mechanisms to measure any of thisâ€” all our mechanisms are like general prosody in a vacuum or naturalness in a vacuum. But the truth isâ€” why do these things start to feel awkward and flat and robotic over time? It's because we don't have great ways of understanding and measuring this.**

æ¯”å¦‚æˆ‘çš„æ„¤æ€’ä¼šæ¿€å‘ä½ çš„æ„¤æ€’ï¼Œæˆ–è€…è®©ä½ æ”¾æ…¢èŠ‚å¥ã€‚æˆ‘ä»¬å®Œå…¨æ²¡æœ‰æœºåˆ¶æ¥è¡¡é‡è¿™äº›â€”â€”æˆ‘ä»¬æ‰€æœ‰çš„è¡¡é‡æœºåˆ¶éƒ½æ˜¯åœ¨çœŸç©ºä¸­è¯„ä¼°ä¸€èˆ¬æ€§éŸµå¾‹æˆ–è‡ªç„¶åº¦ã€‚ä½†äº‹å®æ˜¯â€”â€”ä¸ºä»€ä¹ˆè¿™äº›ä¸œè¥¿æ—¶é—´ä¸€é•¿å°±å¼€å§‹æ„Ÿè§‰å°´å°¬ã€å¹³æ·¡ã€åƒæœºå™¨äººï¼Ÿå°±æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰å¥½çš„æ–¹æ³•æ¥ç†è§£å’Œè¡¡é‡è¿™äº›ã€‚

è§£æï¼š
* **induce** /ÉªnËˆdjuËs/ ğŸ”¥ï¼šåŠ¨è¯ï¼Œå¼•å‘ã€è¯±å¯¼
* **in a vacuum** ğŸ”¥ï¼šçŸ­è¯­ï¼Œåœ¨çœŸç©ºä¸­ï¼ˆè„±ç¦»å®é™…è¯­å¢ƒï¼‰
* **flat**ï¼šå½¢å®¹è¯ï¼Œå¹³æ·¡çš„ã€æ²¡æœ‰æ„Ÿæƒ…çš„
* **robotic**ï¼šå½¢å®¹è¯ï¼Œåƒæœºå™¨äººçš„

---

(58) [37:47-38:11] **So there's this very long tail of interesting problemsâ€” because we don't know how to model it, we haven't really bothered with the eval side, which kind of makes sense. But now I think we're starting to solve some of the speech understanding problems. I think 2026 is the year of starting to tackle some of these things.**

æ‰€ä»¥è¿™æ˜¯ä¸€æ¡éå¸¸é•¿çš„æœ‰è¶£é—®é¢˜é•¿å°¾â€”â€”å› ä¸ºä¸çŸ¥é“æ€ä¹ˆå»ºæ¨¡ï¼Œæˆ‘ä»¬ä¹Ÿå°±æ²¡å¤ªåœ¨è¯„æµ‹æ–¹é¢ä¸‹åŠŸå¤«ï¼Œè¿™ä¹Ÿè¯´å¾—é€šã€‚ä½†ç°åœ¨æˆ‘ä»¬å¼€å§‹è§£å†³ä¸€äº›è¯­éŸ³ç†è§£çš„é—®é¢˜äº†ã€‚2026 å¹´ä¼šæ˜¯å¼€å§‹æ”»å…‹è¿™äº›é—®é¢˜çš„ä¸€å¹´ã€‚

è§£æï¼š
* **long tail** ğŸ”¥ï¼šé•¿å°¾ï¼ˆå¤§é‡å°ä¼—ä½†ç´¯ç§¯å½±å“å·¨å¤§çš„é—®é¢˜/éœ€æ±‚ï¼‰
* **bother with**ï¼šçŸ­è¯­ï¼Œè´¹å¿ƒå»åš
* **tackle** /ËˆtÃ¦kÉ™l/ ğŸ”¥ï¼šåŠ¨è¯ï¼Œå¤„ç†ã€æ”»å…‹

---

(59) [38:11-38:40] **And so we're going to need brand new evals that right now do not capture at all. Some of the thingsâ€” if you gave it to a human, why does it feel wrong? And putting words to things is sometimes really difficult to describeâ€” "why is that awkward?" is sometimes a really hard thing to do. Yeah, totally. And that's exactly whereâ€” especially on the frontier of these models. Figuring out what are we even trying to do? What's even possible?**

æ‰€ä»¥æˆ‘ä»¬éœ€è¦å…¨æ–°çš„è¯„æµ‹æ ‡å‡†â€”â€”ç°æœ‰çš„å®Œå…¨æ— æ³•æ•æ‰ã€‚æœ‰äº›ä¸œè¥¿æ‹¿ç»™äººç±»å¬ï¼Œä¸ºä»€ä¹ˆæ„Ÿè§‰ä¸å¯¹ï¼Ÿç”¨è¯­è¨€æè¿°è¿™äº›æ„Ÿå—æœ‰æ—¶å€™çœŸçš„å¾ˆéš¾â€”â€”"ä¸ºä»€ä¹ˆè¿™å¾ˆå°´å°¬"æ˜¯ä¸ªç‰¹åˆ«éš¾å›ç­”çš„é—®é¢˜ã€‚å®Œå…¨åŒæ„ã€‚å°¤å…¶å½“ä½ ç«™åœ¨è¿™äº›æ¨¡å‹çš„å‰æ²¿æ—¶â€”â€”ææ¸…æ¥šæˆ‘ä»¬åˆ°åº•æƒ³åšä»€ä¹ˆï¼Ÿä»€ä¹ˆæ˜¯å¯èƒ½çš„ï¼Ÿ

è§£æï¼š
* **brand new**ï¼šå…¨æ–°çš„
* **frontier** /frÊŒnËˆtÉªr/ ğŸ”¥ï¼šåè¯ï¼Œå‰æ²¿ã€å‰çº¿

---

(60) [38:40-39:06] **What does it feel like to be speaking to these models? The human existence can't be captured in test-driven development. And I don't think that's even the purpose of evals. The purpose of evals is how can you quantify something that was working or should be working and use that as a communication way of being very discrete, especially at scale. But "can this thing do it one time" is a very different problem than "can it do it 10 million times."**

è·Ÿè¿™äº›æ¨¡å‹è¯´è¯æ˜¯ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿäººç±»çš„ç”Ÿå‘½ä½“éªŒä¸å¯èƒ½ç”¨æµ‹è¯•é©±åŠ¨å¼€å‘æ¥æ•æ‰ã€‚æˆ‘è§‰å¾—è¿™ç”šè‡³ä¸æ˜¯è¯„æµ‹çš„ç›®çš„ã€‚è¯„æµ‹çš„ç›®çš„æ˜¯â€”â€”æ€ä¹ˆæŠŠä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„æˆ–åº”è¯¥è¿è¡Œçš„åŠŸèƒ½é‡åŒ–ï¼Œç”¨å®ƒä½œä¸ºä¸€ç§ç²¾ç¡®æ²Ÿé€šæ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹ã€‚ä½†"ææ¸…æ¥šè¿™ä¸œè¥¿èƒ½ä¸èƒ½åšåˆ°ä¸€æ¬¡"å’Œ"å®ƒèƒ½ä¸èƒ½åšåˆ°ä¸€åƒä¸‡æ¬¡"æ˜¯å®Œå…¨ä¸åŒçš„é—®é¢˜ã€‚

è§£æï¼š
* **test-driven development (TDD)** ğŸ”¥ï¼šæµ‹è¯•é©±åŠ¨å¼€å‘ï¼ˆå…ˆå†™æµ‹è¯•å†å†™ä»£ç çš„å¼€å‘æ–¹æ³•ï¼‰
* **at scale** ğŸ”¥ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹
* **discrete** /dÉªËˆskriËt/ï¼šå½¢å®¹è¯ï¼Œç²¾ç¡®çš„ã€ç¦»æ•£çš„

---

(61) [39:06-39:29] **Especially when you're operating at the frontier of what these models can doâ€” first figuring that out and then distilling it and saying, "How do we preserve that capability while also continuing to push that frontier?" What do you think, Quinn? Any parting thoughts?**

ç‰¹åˆ«æ˜¯ä½ ä»¬è¿™æ ·ç«™åœ¨æœ€å‰æ²¿è¿è¥çš„å›¢é˜Ÿâ€”â€”å…ˆææ¸…æ¥šæ¨¡å‹èƒ½åŠ›çš„å‰æ²¿åœ¨å“ªé‡Œï¼Œç„¶åæç‚¼å‡ºæ¥è¯´ï¼š"æˆ‘ä»¬æ€ä¹ˆä¿ä½è¿™ä¸ªèƒ½åŠ›çš„åŒæ—¶ç»§ç»­æ¨è¿›å‰æ²¿ï¼Ÿ"**Quinn**ï¼Œä½ æœ‰ä»€ä¹ˆæœ€åçš„æƒ³æ³•å—ï¼Ÿ

è§£æï¼š
* **distill** /dÉªËˆstÉªl/ï¼šåŠ¨è¯ï¼Œæç‚¼ã€èƒå–
* **preserve** /prÉªËˆzÉœËrv/ï¼šåŠ¨è¯ï¼Œä¿å­˜ã€ä¿æŒ
* **parting thoughts** ğŸ”¥ï¼šä¸´åˆ«æ„Ÿè¨€ã€æœ€åçš„æƒ³æ³•

---

(62) [39:29-39:54] **Yeah, I think it's really exciting how you've put the pieces together, Zach, to get us another level of fast smart model. And it's the first speech-to-speech model that I feel really does compete head-to-head against the transcription-LLM-voice generation three layer stack. So that's super exciting. And I totally agree with Brooke that figuring out how to build a good cross-model benchmark and then really good application-specific massive simulation evals feels like the next unlock for those of us at the framework level in 2026.**

æ˜¯çš„ï¼Œ**Zach**ï¼Œæˆ‘è§‰å¾—ä½ æŠŠè¿™äº›æ‹¼å›¾æ‹¼åœ¨ä¸€èµ·çš„æ–¹å¼çœŸçš„å¾ˆä»¤äººå…´å¥‹â€”â€”ç»™æˆ‘ä»¬å¸¦æ¥äº†æ–°ä¸€çº§åˆ«åˆå¿«åˆèªæ˜çš„æ¨¡å‹ã€‚è¿™æ˜¯æˆ‘è§‰å¾—ç¬¬ä¸€ä¸ªçœŸæ­£èƒ½è·Ÿ"è½¬å½•-**LLM**-è¯­éŸ³ç”Ÿæˆ"ä¸‰å±‚æ¶æ„æ­£é¢ç«äº‰çš„ç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ã€‚è¿™å¤ªä»¤äººå…´å¥‹äº†ã€‚æˆ‘å®Œå…¨åŒæ„ **Brooke** è¯´çš„â€”â€”ææ¸…æ¥šæ€ä¹ˆæ„å»ºå¥½çš„è·¨æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠçœŸæ­£å¥½çš„é¢å‘ç‰¹å®šåº”ç”¨çš„å¤§è§„æ¨¡æ¨¡æ‹Ÿè¯„æµ‹ï¼Œæ˜¯ 2026 å¹´æˆ‘ä»¬æ¡†æ¶å±‚çš„ä¸‹ä¸€ä¸ªçªç ´ã€‚

è§£æï¼š
* **speech-to-speech model** ğŸ”¥ï¼šç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ï¼ˆç›´æ¥è¯­éŸ³è¾“å…¥â†’è¯­éŸ³è¾“å‡ºï¼‰
* **three layer stack**ï¼šä¸‰å±‚æ¶æ„ï¼ˆè½¬å½•â†’LLM å¤„ç†â†’è¯­éŸ³åˆæˆï¼‰ğŸ”¥
* **head-to-head**ï¼šæ­£é¢å¯¹å†³ã€ç›´æ¥ç«äº‰
* **unlock**ï¼šåè¯ï¼Œçªç ´ã€è§£é”

---

(63) [40:00-40:27] **Yeah. One of my big takeaways is share the problems you're having with your vendors because everyone is trying to figure it out right now. When we hear people's problemsâ€” hearing from users what's working, what's notâ€” is some of the biggest signal above all else. What are people trying to do in production? What's working? What's not? We learned so much from our customers. You got to build the stuff. Awesome. Thanks everybody.**

å¯¹ï¼Œæˆ‘çš„ä¸€ä¸ªé‡è¦æ”¶è·æ˜¯â€”â€”æŠŠä½ é‡åˆ°çš„é—®é¢˜åˆ†äº«ç»™ä½ çš„ä¾›åº”å•†ï¼Œå› ä¸ºç°åœ¨å¤§å®¶éƒ½åœ¨æ‘¸ç´¢ã€‚ä»ç”¨æˆ·é‚£é‡Œå¬åˆ°ä»€ä¹ˆæœ‰ç”¨ã€ä»€ä¹ˆæ²¡ç”¨ï¼Œæ˜¯æœ€é‡è¦çš„ä¿¡å·ã€‚äººä»¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åˆ°åº•æƒ³åšä»€ä¹ˆï¼Ÿä»€ä¹ˆè¡Œå¾—é€šï¼Ÿä»€ä¹ˆè¡Œä¸é€šï¼Ÿæˆ‘ä»¬ä»å®¢æˆ·èº«ä¸Šå­¦åˆ°äº†å¤ªå¤šã€‚å¾—å»åšå®äº‹æ‰è¡Œã€‚å¤ªæ£’äº†ï¼Œè°¢è°¢å¤§å®¶ï¼

è§£æï¼š
* **takeaway** ğŸ”¥ï¼šåè¯ï¼Œæ”¶è·ã€è¦ç‚¹
* **vendor** /ËˆvendÉ™r/ï¼šåè¯ï¼Œä¾›åº”å•†
* **signal**ï¼šåè¯ï¼Œä¿¡å·ï¼ˆè¿™é‡ŒæŒ‡æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼‰
* **you got to build the stuff**ï¼šä½ å¾—å»åšå®äº‹ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼Œå¼ºè°ƒå®è·µçš„é‡è¦æ€§ï¼‰

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ˜¯ä¸€æœŸå…³äºè¯­éŸ³ AI æŠ€æœ¯çš„æ·±åº¦æ’­å®¢å¯¹è°ˆã€‚ä¸‰ä½å˜‰å®¾ä»æ¨ç†æœåŠ¡å»¶è¿Ÿã€æ¨¡å‹è¯„æµ‹æ–¹æ³•è®ºã€å¤šè½®å¯¹è¯æŒ‘æˆ˜ã€æ€è€ƒå‹æ¨¡å‹åœ¨è¯­éŸ³åœºæ™¯çš„å±€é™æ€§ï¼Œä¸€ç›´è®¨è®ºåˆ° 2026 å¹´çš„æŠ€æœ¯è¶‹åŠ¿ã€‚æ ¸å¿ƒè§‚ç‚¹åŒ…æ‹¬ï¼š(1) åŸºå‡†æµ‹è¯•æœ‰ä»·å€¼ä½†è¿œè¿œä¸å¤Ÿï¼ŒçœŸå®å®¢æˆ·åé¦ˆæ˜¯æœ€é‡è¦çš„ä¿¡å·ï¼›(2) è¯­éŸ³ AI éœ€è¦"å¿«æ€è€ƒä¸æ…¢æ€è€ƒ"çš„å¤šæ¨¡å‹æ¶æ„ï¼›(3) å¯¹è¯çš„è‡ªç„¶åº¦ã€éŸµå¾‹å“åº”ã€åé¦ˆæ€§åº”ç­”ç­‰"äººå‘³"é—®é¢˜æ˜¯ä¸‹ä¸€ä¸ªéœ€è¦æ”»å…‹çš„å‰æ²¿ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **inference layer** | æ¨ç†å±‚ |
| **time to first token (TTFT)** | é¦–ä¸ª token å“åº”æ—¶é—´ |
| **P50 / P90** | ç™¾åˆ†ä½å»¶è¿ŸæŒ‡æ ‡ |
| **bread and butter** | çœ‹å®¶æœ¬é¢†ã€æ ¸å¿ƒä¸šåŠ¡ |
| **guiding star** | æŒ‡å¯¼æ–¹å‘ã€åŒ—ææ˜Ÿ |
| **king of vibes** | æ°›å›´ä¹‹ç‹ï¼ˆå‡­ç›´è§‰åˆ¤æ–­ï¼‰ |
| **gut trust** | ç›´è§‰ä¿¡ä»» |
| **POC (proof of concept)** | æ¦‚å¿µéªŒè¯ |
| **iterative prompting** | è¿­ä»£å¼æç¤ºè¯ä¼˜åŒ– |
| **draw a box around** | åœˆå®šèŒƒå›´ |
| **in/out of distribution** | åˆ†å¸ƒå†…/åˆ†å¸ƒå¤– |
| **BLEU score** | ç¿»è¯‘è´¨é‡è¯„ä¼°æŒ‡æ ‡ |
| **proxy** | ä»£ç†æŒ‡æ ‡ |
| **multi-turn conversation** | å¤šè½®å¯¹è¯ |
| **function calling** | å‡½æ•°è°ƒç”¨ |
| **voice agent bench** | è¯­éŸ³ Agent åŸºå‡†æµ‹è¯• |
| **hill climbing** | çˆ¬å¡/é€æ­¥ä¼˜åŒ– |
| **thinking fast and slow** | å¿«æ€è€ƒä¸æ…¢æ€è€ƒ |
| **mixture of experts (MoE)** | æ··åˆä¸“å®¶æ¨¡å‹ |
| **distillation** | è’¸é¦ |
| **guard rails** | æŠ¤æ /å®‰å…¨æœºåˆ¶ |
| **orchestration** | ç¼–æ’ |
| **interleave** | äº¤é”™ã€äº¤æ›¿ |
| **uncanny valley** | ææ€–è°· |
| **prosody** | éŸµå¾‹ |
| **back-channel** | åé¦ˆæ€§åº”ç­” |
| **induce** | å¼•å‘ã€è¯±å¯¼ |
| **in a vacuum** | åœ¨çœŸç©ºä¸­/è„±ç¦»å®é™… |
| **long tail** | é•¿å°¾ |
| **speech-to-speech model** | ç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ |
| **three layer stack** | ä¸‰å±‚æ¶æ„ |
| **test-driven development (TDD)** | æµ‹è¯•é©±åŠ¨å¼€å‘ |
| **at scale** | åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹ |
| **high leverage** | é«˜æ æ†æ•ˆåº” |
| **retrofit** | æ”¹é€ /æ”¹è£…æ—§ç³»ç»Ÿ |
| **legacy systems** | é—ç•™ç³»ç»Ÿ |
| **pendulum swing** | é’Ÿæ‘†æ•ˆåº” |
| **leave on the table** | ç™½ç™½æµªè´¹ |
