# ğŸ¯ AI å†…è”äº‹å®æ ¸æŸ¥æ¶æ„ è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **22 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:00-0:10] **If you're using AI to generate any important writing and you're leaving the fact-checking to the end, know that you're likely to still have some hallucinations in there.**

å¦‚æœä½ ç”¨ AI ç”Ÿæˆé‡è¦çš„æ–‡å­—å†…å®¹ï¼Œå´æŠŠäº‹å®æ ¸æŸ¥ç•™åˆ°æœ€åæ‰åšï¼Œé‚£ä½ è¦çŸ¥é“ï¼Œé‡Œé¢å¾ˆå¯èƒ½è¿˜æ˜¯ä¼šæœ‰ä¸€äº›å¹»è§‰ï¼ˆè™šå‡ä¿¡æ¯ï¼‰ã€‚

è§£æï¼š
* **fact-checking**ï¼šåè¯ï¼Œäº‹å®æ ¸æŸ¥ã€æŸ¥è¯ï¼ˆæ–°é—»/å†…å®¹é¢†åŸŸå¸¸ç”¨æœ¯è¯­ï¼‰
* **hallucination** /hÉ™ËŒluËsÉªËˆneÉªÊƒn/ï¼šåè¯ï¼Œå¹»è§‰ï¼›åœ¨ AI è¯­å¢ƒä¸­æŒ‡æ¨¡å‹ç”Ÿæˆçš„è™šå‡æˆ–é”™è¯¯ä¿¡æ¯ ğŸ”¥
* **leave... to the end**ï¼šçŸ­è¯­ï¼ŒæŠŠ...ç•™åˆ°æœ€å

---

(2) [0:10-0:26] **Instead, I want to introduce an approach where the agent verifies itself as it's generating. If you don't do this type of self-verification as it's generating, you risk the agent derailing as soon as an infactual claim comes in and then one incorrect claim can turn into a whole chain of confident nonsense.**

ç›¸åï¼Œæˆ‘æƒ³ä»‹ç»ä¸€ç§æ–¹æ³•ï¼šè®© agent åœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶è‡ªæˆ‘éªŒè¯ã€‚å¦‚æœä½ ä¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åšè¿™ç§è‡ªæˆ‘éªŒè¯ï¼Œä¸€æ—¦å‡ºç°ä¸€ä¸ªä¸å®çš„å£°æ˜ï¼Œagent å°±æœ‰å¯èƒ½è„±è½¨ï¼Œç„¶åä¸€ä¸ªé”™è¯¯çš„å£°æ˜å°±ä¼šå˜æˆä¸€æ•´ä¸²"è‡ªä¿¡æ»¡æ»¡çš„èƒ¡è¯´å…«é“"ã€‚

è§£æï¼š
* **agent**ï¼šåè¯ï¼ŒAI ä»£ç†/æ™ºèƒ½ä½“ï¼ˆAI é¢†åŸŸæœ¯è¯­ï¼‰
* **verify** /ËˆverÉªfaÉª/ï¼šåŠ¨è¯ï¼ŒéªŒè¯ã€æ ¸å®
* **derail** /dÉªËˆreÉªl/ï¼šåŠ¨è¯ï¼Œï¼ˆç«è½¦ï¼‰è„±è½¨ï¼›å¼•ç”³ä¸ºåç¦»æ­£è½¨ã€å¤±æ§ ğŸ”¥
* **infactual**ï¼šå½¢å®¹è¯ï¼Œä¸ç¬¦åˆäº‹å®çš„ï¼ˆin- å¦å®šå‰ç¼€ + factualï¼‰
* **claim**ï¼šåè¯ï¼Œå£°æ˜ã€è®ºæ–­
* **confident nonsense**ï¼šçŸ­è¯­ï¼Œè‡ªä¿¡çš„èƒ¡è¯´å…«é“ï¼ˆè®½åˆº AI ä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´ï¼‰ğŸ”¥

---

(3) [0:29-0:44] **Fact-checking while you generate is an approach to avoid this. I want to show you how I've built that today and hopefully convince you that it's the better way of doing things. I've had some interesting conversations about the brain cube fact checker that I'm building.**

è¾¹ç”Ÿæˆè¾¹äº‹å®æ ¸æŸ¥æ˜¯é¿å…è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ã€‚ä»Šå¤©æˆ‘æƒ³ç»™å¤§å®¶å±•ç¤ºæˆ‘æ˜¯æ€ä¹ˆå®ç°çš„ï¼Œå¸Œæœ›èƒ½è¯´æœä½ ä»¬è¿™æ˜¯æ›´å¥½çš„åšæ³•ã€‚æˆ‘å°±æ­£åœ¨å¼€å‘çš„ **Brain Cube** äº‹å®æ ¸æŸ¥å·¥å…·è¿›è¡Œäº†ä¸€äº›æœ‰è¶£çš„è®¨è®ºã€‚

è§£æï¼š
* **fact-checking while you generate**ï¼šåŠ¨åè¯çŸ­è¯­ä½œä¸»è¯­ï¼Œè¾¹ç”Ÿæˆè¾¹æ ¸æŸ¥
* **convince** /kÉ™nËˆvÉªns/ï¼šåŠ¨è¯ï¼Œè¯´æœã€ä½¿ä¿¡æœ
* **Brain Cube**ï¼šä¸“æœ‰åè¯ï¼Œä½œè€…å¼€å‘çš„äº‹å®æ ¸æŸ¥å·¥å…·åç§°

---

(4) [0:44-1:07] **And what was interesting is I talked to a researcher that's working on mathematical program synthesis. I'm not an expert in that area at all. So don't dig too much into my phrasing of that. But basically her research is in building mathematical programs or code or anything that's kind of like logically constrained from the ground up correctly the first time.**

æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘å’Œä¸€ä½ç ”ç©¶"æ•°å­¦ç¨‹åºåˆæˆ"çš„ç ”ç©¶å‘˜èŠäº†èŠã€‚æˆ‘å®Œå…¨ä¸æ˜¯è¿™ä¸ªé¢†åŸŸçš„ä¸“å®¶ï¼Œæ‰€ä»¥åˆ«å¤ªçº ç»“æˆ‘çš„æªè¾ã€‚ä½†åŸºæœ¬ä¸Šï¼Œå¥¹çš„ç ”ç©¶æ˜¯å…³äºå¦‚ä½•ä»ä¸€å¼€å§‹å°±æ­£ç¡®åœ°æ„å»ºæ•°å­¦ç¨‹åºã€ä»£ç ï¼Œæˆ–è€…ä»»ä½•æœ‰é€»è¾‘çº¦æŸçš„ä¸œè¥¿â€”â€”ç¬¬ä¸€æ¬¡å°±åšå¯¹ã€‚

è§£æï¼š
* **mathematical program synthesis**ï¼šåè¯çŸ­è¯­ï¼Œæ•°å­¦ç¨‹åºåˆæˆï¼ˆAI/å½¢å¼åŒ–æ–¹æ³•é¢†åŸŸæœ¯è¯­ï¼‰
* **dig into**ï¼šçŸ­è¯­ï¼Œæ·±å…¥ç ”ç©¶ã€è¿½ç©¶
* **phrasing** /ËˆfreÉªzÉªÅ‹/ï¼šåè¯ï¼Œæªè¾ã€è¡¨è¾¾æ–¹å¼
* **logically constrained**ï¼šå½¢å®¹è¯çŸ­è¯­ï¼Œæœ‰é€»è¾‘çº¦æŸçš„
* **from the ground up**ï¼šçŸ­è¯­ï¼Œä»å¤´å¼€å§‹ã€ä»é›¶å¼€å§‹ ğŸ”¥

---

(5) [1:09-1:24] **So effectively it's a workflow that the verification process is embedded within the generation itself. How does that relate to my fact checker? Right now I'm building the fact checker to check content that has already been generated.**

æ‰€ä»¥å®é™…ä¸Šï¼Œè¿™æ˜¯ä¸€ç§æŠŠéªŒè¯è¿‡ç¨‹åµŒå…¥åˆ°ç”Ÿæˆè¿‡ç¨‹æœ¬èº«çš„å·¥ä½œæµã€‚è¿™å’Œæˆ‘çš„äº‹å®æ ¸æŸ¥å·¥å…·æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿç›®å‰æˆ‘æ„å»ºçš„äº‹å®æ ¸æŸ¥å·¥å…·æ˜¯ç”¨æ¥æ£€æŸ¥å·²ç»ç”Ÿæˆå¥½çš„å†…å®¹çš„ã€‚

è§£æï¼š
* **effectively**ï¼šå‰¯è¯ï¼Œå®é™…ä¸Šã€äº‹å®ä¸Š
* **workflow** /ËˆwÉœËrkfloÊŠ/ï¼šåè¯ï¼Œå·¥ä½œæµç¨‹
* **embed** /ÉªmËˆbed/ï¼šåŠ¨è¯ï¼ŒåµŒå…¥ã€æ¤å…¥ ğŸ”¥
* **relate to**ï¼šçŸ­è¯­ï¼Œä¸...ç›¸å…³

---

(6) [1:24-1:47] **So let's say there's already a 200-page consulting report or paper or whatever that has been generated already by artificial intelligence or even written by a human being. What I'm doing is I'm taking that and I'm breaking it down into its component claims. So kind of atomizing the claims and then using an agentic search to verify those claims against public sources. Great. Cool. So that does work.**

æ¯”æ–¹è¯´ï¼Œå·²ç»æœ‰ä¸€ä»½ 200 é¡µçš„å’¨è¯¢æŠ¥å‘Šã€è®ºæ–‡æˆ–è€…åˆ«çš„ä»€ä¹ˆï¼Œæ˜¯ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆçš„ï¼Œæˆ–è€…ç”šè‡³æ˜¯äººå†™çš„ã€‚æˆ‘åšçš„å°±æ˜¯æŠŠå®ƒæ‹†è§£æˆå„ä¸ªç»„æˆå£°æ˜ï¼Œä¹Ÿå°±æ˜¯æŠŠå£°æ˜"åŸå­åŒ–"ï¼Œç„¶åç”¨ agentic æœç´¢æ¥å¯¹ç…§å…¬å¼€æ¥æºéªŒè¯è¿™äº›å£°æ˜ã€‚å¾ˆå¥½ï¼Œç¡®å®èƒ½ç”¨ã€‚

è§£æï¼š
* **consulting report**ï¼šåè¯çŸ­è¯­ï¼Œå’¨è¯¢æŠ¥å‘Š
* **break down into**ï¼šçŸ­è¯­ï¼Œåˆ†è§£æˆã€æ‹†è§£æˆ ğŸ”¥
* **component**ï¼šåè¯/å½¢å®¹è¯ï¼Œç»„æˆéƒ¨åˆ†çš„
* **atomizing** /ËˆÃ¦tÉ™maÉªzÉªÅ‹/ï¼šåŠ¨åè¯ï¼ŒåŸå­åŒ–ï¼ˆæŠŠå¤§å—å†…å®¹æ‹†æˆæœ€å°å•ä½ï¼‰ğŸ”¥
* **agentic search**ï¼šåè¯çŸ­è¯­ï¼Œä»£ç†å¼æœç´¢ï¼ˆAI agent æ‰§è¡Œçš„æœç´¢ï¼‰
* **verify against**ï¼šçŸ­è¯­ï¼Œå¯¹ç…§...è¿›è¡ŒéªŒè¯

---

(7) [1:50-2:12] **But what this researcher was telling me is that I should try a different approach. Why not embed the verification in the generation? So while the agent is actually generating this 200-page report or whatever, as a claim has been generated, so as we're generating the claim, the verification happens then and there.**

ä½†è¿™ä½ç ”ç©¶å‘˜å‘Šè¯‰æˆ‘ï¼Œåº”è¯¥å°è¯•ä¸åŒçš„æ–¹æ³•ã€‚ä¸ºä»€ä¹ˆä¸æŠŠéªŒè¯åµŒå…¥åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­å‘¢ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ agent åœ¨ç”Ÿæˆè¿™ 200 é¡µæŠ¥å‘Šçš„æ—¶å€™ï¼Œæ¯å½“ä¸€ä¸ªå£°æ˜è¢«ç”Ÿæˆå‡ºæ¥ï¼ŒéªŒè¯å°±åœ¨é‚£ä¸ªå½“ä¸‹ç«‹å³å‘ç”Ÿã€‚

è§£æï¼š
* **Why not...?**ï¼šå¥å‹ï¼Œä¸ºä»€ä¹ˆä¸...ï¼Ÿï¼ˆæå»ºè®®ï¼‰
* **then and there**ï¼šçŸ­è¯­ï¼Œå½“åœºã€ç«‹åˆ» ğŸ”¥
* **as**ï¼šè¿è¯ï¼Œå½“...çš„æ—¶å€™ï¼ˆæ­¤å¤„å¼ºè°ƒåŒæ—¶æ€§ï¼‰

---

(8) [2:12-2:40] **And this is a really interesting approach. It's a fascinating approach to me because it has kind of deeper consequences than what might appear on the surface, right? So on the surface it's like, okay, you're generating claim by claim. Great. How does that make a difference between kind of waiting for everything to be generated and deconstructing it and verifying each claim after you've deconstructed it versus generating claim by claim?**

è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„æ–¹æ³•ã€‚å®ƒè®©æˆ‘ç€è¿·ï¼Œå› ä¸ºå®ƒçš„å½±å“æ¯”è¡¨é¢çœ‹èµ·æ¥è¦æ·±è¿œå¾—å¤šã€‚è¡¨é¢ä¸Šçœ‹ï¼Œå¥½åƒå°±æ˜¯é€ä¸ªå£°æ˜åœ°ç”Ÿæˆã€‚é‚£è¿™å’Œ"ç­‰æ‰€æœ‰å†…å®¹éƒ½ç”Ÿæˆå®Œï¼Œå†æ‹†è§£ã€å†é€ä¸ªéªŒè¯"æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿ

è§£æï¼š
* **fascinating** /ËˆfÃ¦sÉªneÉªtÉªÅ‹/ï¼šå½¢å®¹è¯ï¼Œè¿·äººçš„ã€ä»¤äººç€è¿·çš„
* **consequence** /ËˆkÉ’nsÉªkwÉ™ns/ï¼šåè¯ï¼Œåæœã€å½±å“
* **on the surface**ï¼šçŸ­è¯­ï¼Œè¡¨é¢ä¸Š ğŸ”¥
* **deconstruct** /ËŒdiËkÉ™nËˆstrÊŒkt/ï¼šåŠ¨è¯ï¼Œè§£æ„ã€æ‹†è§£
* **versus** /ËˆvÉœËrsÉ™s/ï¼šä»‹è¯ï¼Œä¸...ç›¸å¯¹ã€å¯¹æ¯”

---

(9) [2:40-3:14] **You got to remember that large language models, the models that these agents are built on top of, are auto-regressive models. So that means that when they're generating their tokens, what happens is they use all of the previous tokens that were generated to generate the next. So if you generate a 200-page report and there's a false claim in some part of that generation. So let's say you get 250 tokens in and there's a false citation, a false claim there. That actually guides the rest of the generation. So it corrupts everything going forward.**

ä½ è¦è®°ä½ï¼Œå¤§è¯­è¨€æ¨¡å‹â€”â€”è¿™äº› agent æ‰€åŸºäºçš„æ¨¡å‹â€”â€”æ˜¯è‡ªå›å½’æ¨¡å‹ã€‚è¿™æ„å‘³ç€å½“å®ƒä»¬ç”Ÿæˆ token æ—¶ï¼Œä¼šç”¨ä¹‹å‰ç”Ÿæˆçš„æ‰€æœ‰ token æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªã€‚æ‰€ä»¥å¦‚æœä½ ç”Ÿæˆä¸€ä»½ 200 é¡µçš„æŠ¥å‘Šï¼Œä¸­é—´æŸå¤„æœ‰ä¸€ä¸ªè™šå‡å£°æ˜â€”â€”æ¯”æ–¹è¯´åˆ°äº†ç¬¬ 250 ä¸ª token å¤„æœ‰ä¸ªè™šå‡å¼•ç”¨â€”â€”è¿™å®é™…ä¸Šä¼šå¼•å¯¼åç»­çš„æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ã€‚å®ƒä¼šæ±¡æŸ“åé¢çš„æ‰€æœ‰å†…å®¹ã€‚

è§£æï¼š
* **auto-regressive model**ï¼šåè¯çŸ­è¯­ï¼Œè‡ªå›å½’æ¨¡å‹ï¼ˆæ¯ä¸ªè¾“å‡ºä¾èµ–ä¹‹å‰çš„è¾“å‡ºï¼‰ğŸ”¥
* **token**ï¼šåè¯ï¼Œä»¤ç‰Œï¼ˆNLP ä¸­æ–‡æœ¬çš„åŸºæœ¬å•ä½ï¼‰
* **citation** /saÉªËˆteÉªÊƒn/ï¼šåè¯ï¼Œå¼•ç”¨ã€å¼•æ–‡
* **corrupt** /kÉ™ËˆrÊŒpt/ï¼šåŠ¨è¯ï¼Œç ´åã€æ±¡æŸ“ ğŸ”¥
* **going forward**ï¼šçŸ­è¯­ï¼Œä»Šåã€å¾€å

---

(10) [3:14-3:43] **So that's an argument for the atomized claim-by-claim verification having that within the generation loop rather than trying to wait for the whole report to be generated and then breaking it down after. The corruption at that point is probably like embedded in the structure of the report. So even if you adjust a claim, it might not adjust itself regressively backwards. And I've seen it do it, but I think it might not be as effective.**

æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦åœ¨ç”Ÿæˆå¾ªç¯å†…éƒ¨åšåŸå­åŒ–çš„é€å£°æ˜éªŒè¯ï¼Œè€Œä¸æ˜¯ç­‰æ•´ä¸ªæŠ¥å‘Šç”Ÿæˆå®Œå†æ‹†è§£ã€‚é‚£æ—¶å€™ï¼Œæ±¡æŸ“å¯èƒ½å·²ç»åµŒå…¥åˆ°æŠ¥å‘Šçš„ç»“æ„ä¸­äº†ã€‚å³ä½¿ä½ ä¿®æ­£äº†ä¸€ä¸ªå£°æ˜ï¼Œå®ƒä¹Ÿå¯èƒ½ä¸ä¼šè‡ªåŠ¨å›æº¯è°ƒæ•´å‰é¢çš„å†…å®¹ã€‚æˆ‘è§è¿‡å®ƒè¿™ä¹ˆåšï¼Œä½†æˆ‘è§‰å¾—æ•ˆæœå¯èƒ½æ²¡é‚£ä¹ˆå¥½ã€‚

è§£æï¼š
* **argument for**ï¼šçŸ­è¯­ï¼Œæ”¯æŒ...çš„è®ºæ®
* **generation loop**ï¼šåè¯çŸ­è¯­ï¼Œç”Ÿæˆå¾ªç¯
* **regressively** /rÉªËˆÉ¡resÉªvli/ï¼šå‰¯è¯ï¼Œå›å½’åœ°ã€å€’é€€åœ°
* **backwards**ï¼šå‰¯è¯ï¼Œå‘åã€å€’å›

---

(11) [3:43-4:11] **So what I want to do is I want to do a little experiment. I'm going to basically take that approach with the Brain Cube fact check and I'm going to do it through the MCP server. So I'm going to build the MCP server such that it does that generation line by lineâ€”not token by token but claim by claimâ€”rather than waiting for the entire thing to be generated.**

æ‰€ä»¥æˆ‘æƒ³åšä¸ªå°å®éªŒã€‚æˆ‘æ‰“ç®—ç”¨ **Brain Cube** äº‹å®æ ¸æŸ¥æ¥é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œé€šè¿‡ **MCP** æœåŠ¡å™¨æ¥å®ç°ã€‚æˆ‘è¦æ„å»ºè¿™ä¸ª MCP æœåŠ¡å™¨ï¼Œè®©å®ƒé€è¡Œç”Ÿæˆâ€”â€”ä¸æ˜¯é€ tokenï¼Œè€Œæ˜¯é€å£°æ˜â€”â€”è€Œä¸æ˜¯ç­‰æ•´ä¸ªå†…å®¹éƒ½ç”Ÿæˆå®Œã€‚

è§£æï¼š
* **experiment** /ÉªkËˆsperÉªmÉ™nt/ï¼šåè¯ï¼Œå®éªŒ
* **MCP server**ï¼šä¸“æœ‰åè¯ï¼ŒModel Context Protocol æœåŠ¡å™¨ï¼ˆAI å·¥å…·è¿æ¥åè®®ï¼‰
* **line by line**ï¼šçŸ­è¯­ï¼Œé€è¡Œåœ°
* **claim by claim**ï¼šçŸ­è¯­ï¼Œé€ä¸ªå£°æ˜åœ°

---

(12) [4:11-4:32] **So yeah, this will be available to use obviously by the Brain Cube checker if you do want to use it. And yeah, let's work on that together. Let's see how it goes. So I want to give you a high-level overview of what my plan is in terms of architecting this new approach to claim and fact verification.**

æ‰€ä»¥ï¼Œå¦‚æœä½ æƒ³ç”¨çš„è¯ï¼Œ**Brain Cube** æ£€æŸ¥å™¨æ˜¾ç„¶ä¼šæä¾›è¿™ä¸ªåŠŸèƒ½ã€‚å¥½ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¥åšï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚æˆ‘æƒ³ç»™å¤§å®¶ä¸€ä¸ªé«˜å±‚æ¬¡çš„æ¦‚è¿°ï¼Œä»‹ç»ä¸€ä¸‹æˆ‘å¯¹è¿™ç§æ–°çš„å£°æ˜å’Œäº‹å®éªŒè¯æ–¹æ³•çš„æ¶æ„è§„åˆ’ã€‚

è§£æï¼š
* **available**ï¼šå½¢å®¹è¯ï¼Œå¯ç”¨çš„
* **high-level overview**ï¼šåè¯çŸ­è¯­ï¼Œé«˜å±‚æ¬¡æ¦‚è¿°ã€æ€»ä½“ä»‹ç» ğŸ”¥
* **in terms of**ï¼šçŸ­è¯­ï¼Œå°±...è€Œè¨€ã€åœ¨...æ–¹é¢ ğŸ”¥
* **architecting**ï¼šåŠ¨åè¯ï¼Œè®¾è®¡æ¶æ„ï¼ˆåŠ¨è¯ç”¨æ³•ï¼‰

---

(13) [4:32-4:59] **So what we do right now at a high level is that we take a completed article or publication or presentation, whatever it is that you want to verify, and we break that down into atomic claims that can be verified against our sources. So right now it's sources on the public internet. There's no reason why these sources couldn't be an internal dataset or a repository of knowledge or whatever, a private source of data, but right now it's public internet verification.**

ç›®å‰æˆ‘ä»¬åœ¨é«˜å±‚æ¬¡ä¸Šåšçš„æ˜¯ï¼šæ‹¿ä¸€ç¯‡å®Œæˆçš„æ–‡ç« ã€å‡ºç‰ˆç‰©æˆ–æ¼”ç¤ºæ–‡ç¨¿â€”â€”ä»»ä½•ä½ æƒ³éªŒè¯çš„ä¸œè¥¿â€”â€”æŠŠå®ƒæ‹†è§£æˆå¯ä»¥å¯¹ç…§æ¥æºéªŒè¯çš„åŸå­å£°æ˜ã€‚ç°åœ¨ç”¨çš„æ˜¯å…¬å…±äº’è”ç½‘ä¸Šçš„æ¥æºã€‚æ²¡æœ‰ç†ç”±ä¸èƒ½ç”¨å†…éƒ¨æ•°æ®é›†ã€çŸ¥è¯†åº“æˆ–å…¶ä»–ç§æœ‰æ•°æ®æºï¼Œä½†ç›®å‰æ˜¯ç”¨å…¬ç½‘éªŒè¯ã€‚

è§£æï¼š
* **publication** /ËŒpÊŒblÉªËˆkeÉªÊƒn/ï¼šåè¯ï¼Œå‡ºç‰ˆç‰©
* **atomic claims**ï¼šåè¯çŸ­è¯­ï¼ŒåŸå­å£°æ˜ï¼ˆä¸å¯å†åˆ†çš„æœ€å°å£°æ˜å•ä½ï¼‰
* **repository** /rÉªËˆpÉ’zÉªtÉ”Ëri/ï¼šåè¯ï¼Œä»“åº“ã€å­˜å‚¨åº“ï¼ˆå¦‚ä»£ç ä»“åº“ã€çŸ¥è¯†åº“ï¼‰ğŸ”¥
* **there's no reason why... couldn't**ï¼šå¥å‹ï¼Œæ²¡æœ‰ç†ç”±ä¸èƒ½...

---

(14) [4:59-5:30] **So as we do that, a lot of the difficulty is of course in breaking down that document as it's already generated. Making sure that you've captured all of the claims is difficult. And the way I approach is to throw high compute at it, use the most capable model available, give it high reasoning and hope that it's able to decompose all of the claims.**

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¾ˆå¤§çš„éš¾ç‚¹å½“ç„¶æ˜¯æ‹†è§£ä¸€ä»½å·²ç»ç”Ÿæˆå¥½çš„æ–‡æ¡£ã€‚ç¡®ä¿ä½ æ•è·äº†æ‰€æœ‰å£°æ˜æ˜¯å¾ˆå›°éš¾çš„ã€‚æˆ‘çš„æ–¹æ³•æ˜¯ç ¸ç®—åŠ›è¿›å»ï¼Œç”¨æœ€å¼ºå¤§çš„å¯ç”¨æ¨¡å‹ï¼Œç»™å®ƒé«˜æ¨ç†èƒ½åŠ›ï¼Œç„¶åæœŸæœ›å®ƒèƒ½åˆ†è§£å‡ºæ‰€æœ‰å£°æ˜ã€‚

è§£æï¼š
* **capture**ï¼šåŠ¨è¯ï¼Œæ•è·ã€æ•æ‰
* **throw... at**ï¼šçŸ­è¯­ï¼ŒæŠ•å…¥...ã€ç ¸...è¿›å» ğŸ”¥
* **high compute**ï¼šåè¯çŸ­è¯­ï¼Œå¤§é‡è®¡ç®—èµ„æº
* **decompose** /ËŒdiËkÉ™mËˆpoÊŠz/ï¼šåŠ¨è¯ï¼Œåˆ†è§£ ğŸ”¥
* **reasoning**ï¼šåè¯ï¼Œæ¨ç†ï¼ˆèƒ½åŠ›ï¼‰

---

(15) [5:21-5:47] **So far, as I've tested, it seems to work okay with careful prompting. And there's also audit loops that I have in there to verify that no claims have been missed. And that kind of combination of high-performance models and audit loops has taken me so far to have some level of consistent claim decomposition over long documents.**

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œç»è¿‡æµ‹è¯•ï¼Œé…åˆä»”ç»†çš„ prompt è®¾è®¡ï¼Œæ•ˆæœè¿˜ä¸é”™ã€‚æˆ‘è¿˜è®¾ç½®äº†å®¡è®¡å¾ªç¯æ¥éªŒè¯æ²¡æœ‰é—æ¼ä»»ä½•å£°æ˜ã€‚é«˜æ€§èƒ½æ¨¡å‹å’Œå®¡è®¡å¾ªç¯çš„è¿™ç§ç»„åˆï¼Œè®©æˆ‘ç›®å‰èƒ½åœ¨é•¿æ–‡æ¡£ä¸Šå®ç°ä¸€å®šç¨‹åº¦ä¸Šä¸€è‡´çš„å£°æ˜åˆ†è§£ã€‚

è§£æï¼š
* **prompting**ï¼šåè¯ï¼Œæç¤ºè¯å·¥ç¨‹/è®¾è®¡ï¼ˆAI æœ¯è¯­ï¼‰
* **audit loop**ï¼šåè¯çŸ­è¯­ï¼Œå®¡è®¡å¾ªç¯ã€æ ¡éªŒå¾ªç¯ ğŸ”¥
* **consistent**ï¼šå½¢å®¹è¯ï¼Œä¸€è‡´çš„ã€ç¨³å®šçš„
* **so far**ï¼šçŸ­è¯­ï¼Œåˆ°ç›®å‰ä¸ºæ­¢

---

(16) [5:44-6:12] **And also there's a process in which we segment the document into smaller token sets. For example, if you want to do claim decomposition on a document that is 1 million tokens, breaking it down into batches of 50,000 tokens means that you're more likely to be able to do that decomposition and capture all the claims in one pass through an LLM and do the auditing as well.**

å¦å¤–è¿˜æœ‰ä¸€ä¸ªè¿‡ç¨‹ï¼Œå°±æ˜¯æŠŠæ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„ token é›†åˆã€‚æ¯”å¦‚ï¼Œå¦‚æœä½ è¦å¯¹ä¸€ä¸ª 100 ä¸‡ token çš„æ–‡æ¡£åšå£°æ˜åˆ†è§£ï¼ŒæŠŠå®ƒæ‹†æˆæ¯æ‰¹ 5 ä¸‡ tokenï¼Œå°±æ›´æœ‰å¯èƒ½åœ¨ LLM çš„ä¸€æ¬¡å¤„ç†ä¸­å®Œæˆåˆ†è§£å¹¶æ•è·æ‰€æœ‰å£°æ˜ï¼ŒåŒæ—¶å®Œæˆå®¡è®¡ã€‚

è§£æï¼š
* **segment** /ËˆseÉ¡mÉ™nt/ï¼šåŠ¨è¯ï¼Œåˆ†å‰²ã€åˆ’åˆ†
* **batch**ï¼šåè¯ï¼Œæ‰¹æ¬¡
* **in one pass**ï¼šçŸ­è¯­ï¼Œä¸€æ¬¡é€šè¿‡ã€ä¸€è¶Ÿ ğŸ”¥
* **LLM**ï¼šç¼©å†™ï¼ŒLarge Language Modelï¼Œå¤§è¯­è¨€æ¨¡å‹

---

(17) [6:12-6:30] **So that's the current approach. And then once those claims have been identified and atomized, you can then run an agentic search for the verification part. And the verification part is what it is and it's what it always has been since the beginning of this project, which is verify against reputable sources available on the public internet.**

è¿™å°±æ˜¯ç›®å‰çš„æ–¹æ³•ã€‚ä¸€æ—¦è¿™äº›å£°æ˜è¢«è¯†åˆ«å’ŒåŸå­åŒ–ä¹‹åï¼Œå°±å¯ä»¥è¿è¡Œ agentic æœç´¢æ¥åšéªŒè¯ã€‚éªŒè¯éƒ¨åˆ†å°±æ˜¯å®ƒä¸€ç›´ä»¥æ¥çš„æ ·å­â€”â€”ä»é¡¹ç›®å¼€å§‹å°±æ˜¯è¿™æ ·â€”â€”å°±æ˜¯å¯¹ç…§å…¬å…±äº’è”ç½‘ä¸Šçš„æƒå¨æ¥æºè¿›è¡ŒéªŒè¯ã€‚

è§£æï¼š
* **identify**ï¼šåŠ¨è¯ï¼Œè¯†åˆ«ã€ç¡®å®š
* **reputable** /ËˆrepjÊŠtÉ™bl/ï¼šå½¢å®¹è¯ï¼Œæœ‰ä¿¡èª‰çš„ã€æƒå¨çš„ ğŸ”¥
* **it is what it is**ï¼šä¹ è¯­ï¼Œå°±æ˜¯è¿™æ ·ã€æœ¬æ¥å¦‚æ­¤

---

(18) [6:30-7:00] **Now this inline approach is going to be different because we're working the other way. So we're actually starting at the generation. As soon as a claim is made or detected by the agent, that is handed off to the verifier and the verifier verifies that inline as the generation is happening. Now obviously there's some real technical difficulties to this approach. The first and most obvious one is latency.**

ç°åœ¨è¿™ç§å†…è”æ–¹æ³•ä¼šä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯åè¿‡æ¥åšçš„ã€‚æˆ‘ä»¬å®é™…ä¸Šæ˜¯ä»ç”Ÿæˆå¼€å§‹ã€‚ä¸€æ—¦ agent äº§ç”Ÿæˆ–æ£€æµ‹åˆ°ä¸€ä¸ªå£°æ˜ï¼Œå°±ä¼šæŠŠå®ƒäº¤ç»™éªŒè¯å™¨ï¼ŒéªŒè¯å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶éªŒè¯ã€‚æ˜¾ç„¶ï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸€äº›çœŸæ­£çš„æŠ€æœ¯éš¾é¢˜ã€‚ç¬¬ä¸€ä¸ªä¹Ÿæ˜¯æœ€æ˜æ˜¾çš„å°±æ˜¯å»¶è¿Ÿã€‚

è§£æï¼š
* **inline**ï¼šå½¢å®¹è¯/å‰¯è¯ï¼Œå†…è”çš„ã€å®æ—¶çš„ ğŸ”¥
* **the other way**ï¼šçŸ­è¯­ï¼Œåè¿‡æ¥ã€ç›¸åçš„æ–¹å¼
* **hand off**ï¼šçŸ­è¯­ï¼Œäº¤æ¥ã€ç§»äº¤ ğŸ”¥
* **verifier**ï¼šåè¯ï¼ŒéªŒè¯å™¨
* **latency** /ËˆleÉªtÉ™nsi/ï¼šåè¯ï¼Œå»¶è¿Ÿï¼ˆæŠ€æœ¯æœ¯è¯­ï¼‰ğŸ”¥

---

(19) [6:52-7:36] **If you're asking the agent to verify itself as claims are written, that's going to introduce latency. We can still use the fast models that we're using for verification downstream because it's still going to be atomized. So latency is introduced into generation which you might not mind if you're expecting this thing to go off and run for half an hour. And you might not mind as well if you are operating in a domain where your written content you need to have absolute certainty that all the claims are correctâ€”things like medical writing or legal writing or even just consultancy papers you're delivering for high-value clientsâ€”anything where the cost of mistakes can be very high. So yes, it introduces latency, but that might not necessarily be an issue.**

å¦‚æœä½ è®© agent åœ¨å†™å‡ºå£°æ˜æ—¶è‡ªæˆ‘éªŒè¯ï¼Œé‚£è‚¯å®šä¼šå¼•å…¥å»¶è¿Ÿã€‚æˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨ç”¨äºä¸‹æ¸¸éªŒè¯çš„å¿«é€Ÿæ¨¡å‹ï¼Œå› ä¸ºå£°æ˜ä»ç„¶æ˜¯åŸå­åŒ–çš„ã€‚æ‰€ä»¥å»¶è¿Ÿè¢«å¼•å…¥åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­â€”â€”å¦‚æœä½ æœ¬æ¥å°±é¢„æœŸè¿™ä¸œè¥¿è¦è·‘åŠä¸ªå°æ—¶ï¼Œä½ å¯èƒ½ä¸ä»‹æ„ã€‚å¦‚æœä½ åœ¨ä¸€ä¸ªéœ€è¦ç»å¯¹ç¡®ä¿æ‰€æœ‰å£°æ˜éƒ½æ­£ç¡®çš„é¢†åŸŸå·¥ä½œï¼Œä½ å¯èƒ½ä¹Ÿä¸ä»‹æ„â€”â€”æ¯”å¦‚åŒ»å­¦å†™ä½œã€æ³•å¾‹å†™ä½œï¼Œæˆ–è€…ç»™é«˜ä»·å€¼å®¢æˆ·äº¤ä»˜çš„å’¨è¯¢æŠ¥å‘Šâ€”â€”ä»»ä½•é”™è¯¯ä»£ä»·éå¸¸é«˜çš„åœºæ™¯ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œå®ƒå¼•å…¥äº†å»¶è¿Ÿï¼Œä½†è¿™ä¸ä¸€å®šæ˜¯ä¸ªé—®é¢˜ã€‚

è§£æï¼š
* **downstream**ï¼šå½¢å®¹è¯/å‰¯è¯ï¼Œä¸‹æ¸¸çš„ï¼ˆæµç¨‹åé¢çš„ç¯èŠ‚ï¼‰
* **go off and run**ï¼šçŸ­è¯­ï¼Œå¼€å§‹è¿è¡Œ
* **absolute certainty**ï¼šåè¯çŸ­è¯­ï¼Œç»å¯¹ç¡®å®šæ€§
* **high-value clients**ï¼šåè¯çŸ­è¯­ï¼Œé«˜ä»·å€¼å®¢æˆ·
* **cost of mistakes**ï¼šåè¯çŸ­è¯­ï¼Œé”™è¯¯çš„ä»£ä»·

---

(20) [7:43-8:19] **The other issue is handoff, right? So handoff is an issue because we're doing this via MCP. So the architecture of MCP is pretty simple. It's basically a client-server architecture, right? You have the MCP server, which is what you're familiar with. You can connect your client, your agent to an MCP server, but the agent itself is actually the client, right? So handoff between the agent client side and the MCP server needs to be immaculate, right? You want the agent to detect every single time it's written a claim and be able to hand that off to the MCP server for fact-checking.**

å¦ä¸€ä¸ªé—®é¢˜æ˜¯äº¤æ¥ã€‚äº¤æ¥æ˜¯ä¸ªé—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯é€šè¿‡ **MCP** æ¥åšçš„ã€‚**MCP** çš„æ¶æ„å¾ˆç®€å•ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ã€‚ä½ æœ‰ MCP æœåŠ¡å™¨ï¼Œè¿™æ˜¯ä½ ç†Ÿæ‚‰çš„ã€‚ä½ å¯ä»¥æŠŠå®¢æˆ·ç«¯ï¼ˆä½ çš„ agentï¼‰è¿æ¥åˆ° MCP æœåŠ¡å™¨ï¼Œä½† agent æœ¬èº«å®é™…ä¸Šæ˜¯å®¢æˆ·ç«¯ã€‚æ‰€ä»¥ agent å®¢æˆ·ç«¯å’Œ MCP æœåŠ¡å™¨ä¹‹é—´çš„äº¤æ¥å¿…é¡»å®Œç¾æ— ç¼ºã€‚ä½ å¸Œæœ› agent æ¯æ¬¡å†™å‡ºä¸€ä¸ªå£°æ˜éƒ½èƒ½æ£€æµ‹åˆ°ï¼Œå¹¶èƒ½æŠŠå®ƒäº¤ç»™ MCP æœåŠ¡å™¨è¿›è¡Œäº‹å®æ ¸æŸ¥ã€‚

è§£æï¼š
* **handoff** /ËˆhÃ¦ndÉ”Ëf/ï¼šåè¯ï¼Œäº¤æ¥ã€ç§»äº¤ ğŸ”¥
* **client-server architecture**ï¼šåè¯çŸ­è¯­ï¼Œå®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„
* **immaculate** /ÉªËˆmÃ¦kjÊŠlÉ™t/ï¼šå½¢å®¹è¯ï¼Œå®Œç¾æ— ç‘•çš„ã€æ— å¯æŒ‘å‰”çš„ ğŸ”¥
* **detect**ï¼šåŠ¨è¯ï¼Œæ£€æµ‹ã€å‘ç°

---

(21) [8:19-8:58] **So in this case, the Brain Cube fact check tool. Hand that off and then bring it back and continue the generation inline. And you want it to do that consistently across the entire generation until it's through to synthesis of the report. I'm going to be caching those claims. So every time a claim is verified, it will be cached and the agent will have access to that cache inline as it's verifying. So imagine for every inline claim it will be able to use the cache as a tool and draw back all of the previous claims in order to have that full context verifying and generating from claim to claim for whatever document type you're generating.**

åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­å°±æ˜¯ **Brain Cube** äº‹å®æ ¸æŸ¥å·¥å…·ã€‚æŠŠå£°æ˜äº¤å‡ºå»ï¼Œç„¶åæ‹¿å›æ¥ï¼Œç»§ç»­å†…è”ç”Ÿæˆã€‚ä½ å¸Œæœ›å®ƒåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­å§‹ç»ˆå¦‚ä¸€åœ°è¿™æ ·åšï¼Œç›´åˆ°å®ŒæˆæŠ¥å‘Šçš„ç»¼åˆã€‚æˆ‘ä¼šç¼“å­˜è¿™äº›å£°æ˜ã€‚æ¯å½“ä¸€ä¸ªå£°æ˜è¢«éªŒè¯ï¼Œå®ƒå°±ä¼šè¢«ç¼“å­˜ï¼Œagent åœ¨éªŒè¯æ—¶å¯ä»¥å®æ—¶è®¿é—®è¿™ä¸ªç¼“å­˜ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¯¹äºæ¯ä¸ªå†…è”å£°æ˜ï¼Œå®ƒéƒ½èƒ½æŠŠç¼“å­˜å½“ä½œå·¥å…·ï¼Œè°ƒå–ä¹‹å‰æ‰€æœ‰çš„å£°æ˜ï¼Œä»è€Œåœ¨ä»ä¸€ä¸ªå£°æ˜ç”Ÿæˆåˆ°ä¸‹ä¸€ä¸ªå£°æ˜æ—¶æ‹¥æœ‰å®Œæ•´çš„ä¸Šä¸‹æ–‡â€”â€”æ— è®ºä½ åœ¨ç”Ÿæˆä»€ä¹ˆç±»å‹çš„æ–‡æ¡£ã€‚

è§£æï¼š
* **consistently**ï¼šå‰¯è¯ï¼Œå§‹ç»ˆå¦‚ä¸€åœ°
* **synthesis** /ËˆsÉªnÎ¸É™sÉªs/ï¼šåè¯ï¼Œç»¼åˆã€åˆæˆ
* **cache** /kÃ¦Êƒ/ï¼šåè¯/åŠ¨è¯ï¼Œç¼“å­˜ ğŸ”¥
* **draw back**ï¼šçŸ­è¯­ï¼Œå–å›ã€è°ƒå–
* **full context**ï¼šåè¯çŸ­è¯­ï¼Œå®Œæ•´ä¸Šä¸‹æ–‡

---

(22) [8:58-9:59] **So that's the architecture from a high level. I'm excited to implement it. So my implementation stack is Claude Code. I've been using this to build the fact checker software really from the start and it's worked well so far. I have already created the architecture document. What I tend to do, my process is I'll usually try to spec things out as best as I can. Obviously, you can never get to a perfect spec, but I will spend quite a bit of time upfront specking it out. I already have tests implemented in my codebase. So whenever I do make these kind of changes to add features, all the tests I'll run by the agent to make sure that none of the existing functionality has broken or regressed. And I'll do several passes of the spec to make sure that I'm capturing what needs to be captured. That's pretty much my setup and this is the approach I want to take. Let's get into actually testing it out in the next section.**

è¿™å°±æ˜¯é«˜å±‚æ¬¡çš„æ¶æ„ã€‚æˆ‘å¾ˆæœŸå¾…å®ç°å®ƒã€‚æˆ‘çš„å®ç°æŠ€æœ¯æ ˆæ˜¯ **Claude Code**ã€‚ä»ä¸€å¼€å§‹æˆ‘å°±ç”¨å®ƒæ¥æ„å»ºäº‹å®æ ¸æŸ¥è½¯ä»¶ï¼Œåˆ°ç›®å‰ä¸ºæ­¢æ•ˆæœå¾ˆå¥½ã€‚æˆ‘å·²ç»åˆ›å»ºäº†æ¶æ„æ–‡æ¡£ã€‚æˆ‘çš„ä¹ æƒ¯æ˜¯â€”â€”æˆ‘çš„æµç¨‹æ˜¯å°½å¯èƒ½æŠŠè§„æ ¼è¯´æ˜ä¹¦å†™æ¸…æ¥šã€‚æ˜¾ç„¶ï¼Œä½ æ°¸è¿œæ— æ³•å†™å‡ºå®Œç¾çš„è§„æ ¼ï¼Œä½†æˆ‘ä¼šèŠ±ç›¸å½“å¤šçš„æ—¶é—´é¢„å…ˆæŠŠå®ƒå†™å¥½ã€‚æˆ‘çš„ä»£ç åº“é‡Œå·²ç»æœ‰æµ‹è¯•äº†ã€‚æ¯å½“æˆ‘åšè¿™ç±»æ·»åŠ åŠŸèƒ½çš„æ›´æ”¹æ—¶ï¼Œæˆ‘éƒ½ä¼šè®© agent è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼Œç¡®ä¿ç°æœ‰åŠŸèƒ½æ²¡æœ‰è¢«ç ´åæˆ–é€€åŒ–ã€‚æˆ‘è¿˜ä¼šå¯¹è§„æ ¼åšå‡ è½®å®¡æŸ¥ï¼Œç¡®ä¿æ•è·äº†éœ€è¦æ•è·çš„å†…å®¹ã€‚è¿™åŸºæœ¬å°±æ˜¯æˆ‘çš„è®¾ç½®ï¼Œè¿™æ˜¯æˆ‘æƒ³é‡‡ç”¨çš„æ–¹æ³•ã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬æ¥å®é™…æµ‹è¯•ä¸€ä¸‹ã€‚

è§£æï¼š
* **implementation stack**ï¼šåè¯çŸ­è¯­ï¼Œå®ç°æŠ€æœ¯æ ˆ
* **spec out**ï¼šçŸ­è¯­ï¼Œå†™è§„æ ¼è¯´æ˜ã€è¯¦ç»†è§„åˆ’ ğŸ”¥
* **upfront**ï¼šå‰¯è¯ï¼Œé¢„å…ˆã€æå‰ ğŸ”¥
* **regress** /rÉªËˆÉ¡res/ï¼šåŠ¨è¯ï¼Œé€€åŒ–ã€å›å½’ï¼ˆè½¯ä»¶æµ‹è¯•æœ¯è¯­ï¼ŒæŒ‡åŠŸèƒ½é€€åŒ–ï¼‰ğŸ”¥
* **several passes**ï¼šåè¯çŸ­è¯­ï¼Œå‡ è½®å®¡æŸ¥

---

(23) [9:59-10:54] **So, I have built the inline verification fact-checking MCP and now I want to test it live with you guys. I'm going to give Claude a task. By the way, I'm using Claude because it's the most reliable agentic harness that I've seen, especially through the chat application. So, that's why I'm using Claude. Just to show you guys really quickly how easy it is to hook up this MCP server. All you have to do if you've got a Claude account and you'd like to use this is you go to settings and then you go to connectors. So you go to settings and you want to connect your check tool and I have instructions on the main website to show you how to actually set up the server itself. But connecting it, you just authenticate. I'm authenticating through Google and then you're in. And I want to just show you the tool itself really quickly too. So when you actually configure it, you can see all of the tools and the permissions.**

å¥½ï¼Œæˆ‘å·²ç»æ„å»ºå¥½äº†å†…è”éªŒè¯äº‹å®æ ¸æŸ¥ **MCP**ï¼Œç°åœ¨æˆ‘æƒ³å’Œå¤§å®¶ä¸€èµ·å®æ—¶æµ‹è¯•ã€‚æˆ‘è¦ç»™ **Claude** ä¸€ä¸ªä»»åŠ¡ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæˆ‘ç”¨ **Claude** æ˜¯å› ä¸ºå®ƒæ˜¯æˆ‘è§è¿‡çš„æœ€å¯é çš„ agentic å·¥å…·æ¡†æ¶ï¼Œå°¤å…¶æ˜¯é€šè¿‡èŠå¤©åº”ç”¨ä½¿ç”¨æ—¶ã€‚è¿™å°±æ˜¯æˆ‘ç”¨ **Claude** çš„åŸå› ã€‚æˆ‘å¿«é€Ÿç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹è¿æ¥è¿™ä¸ª MCP æœåŠ¡å™¨æœ‰å¤šç®€å•ã€‚å¦‚æœä½ æœ‰ Claude è´¦æˆ·å¹¶ä¸”æƒ³ä½¿ç”¨å®ƒï¼Œä½ åªéœ€è¦è¿›å…¥è®¾ç½®ï¼Œç„¶åè¿›å…¥è¿æ¥å™¨ã€‚ä½ è¿›å…¥è®¾ç½®ï¼Œè¿æ¥ä½ çš„æ£€æŸ¥å·¥å…·â€”â€”æˆ‘åœ¨ä¸»ç½‘ç«™ä¸Šæœ‰è¯´æ˜ï¼Œå‘Šè¯‰ä½ å¦‚ä½•å®é™…è®¾ç½®æœåŠ¡å™¨æœ¬èº«ã€‚ä½†è¿æ¥å®ƒï¼Œä½ åªéœ€è¦è®¤è¯å°±è¡Œã€‚æˆ‘é€šè¿‡ Google è®¤è¯ï¼Œç„¶åå°±è¿›å»äº†ã€‚æˆ‘è¿˜æƒ³å¿«é€Ÿç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹è¿™ä¸ªå·¥å…·æœ¬èº«ã€‚å½“ä½ é…ç½®å¥½ä¹‹åï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„å·¥å…·å’Œæƒé™ã€‚

è§£æï¼š
* **live**ï¼šå‰¯è¯ï¼Œå®æ—¶åœ°ã€ç°åœºåœ°
* **agentic harness**ï¼šåè¯çŸ­è¯­ï¼Œä»£ç†å·¥å…·æ¡†æ¶/è½½ä½“ ğŸ”¥
* **hook up**ï¼šçŸ­è¯­ï¼Œè¿æ¥ã€æ¥å…¥ ğŸ”¥
* **connector**ï¼šåè¯ï¼Œè¿æ¥å™¨
* **authenticate** /É”ËËˆÎ¸entÉªkeÉªt/ï¼šåŠ¨è¯ï¼Œè®¤è¯ã€éªŒè¯èº«ä»½ ğŸ”¥
* **permissions**ï¼šåè¯ï¼Œæƒé™

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ®µè§†é¢‘è®²è§£äº† AI ç”Ÿæˆå†…å®¹æ—¶çš„äº‹å®æ ¸æŸ¥é—®é¢˜ã€‚æ¼”è®²è€…ä»‹ç»äº†ä»–æ­£åœ¨å¼€å‘çš„ **Brain Cube** äº‹å®æ ¸æŸ¥å·¥å…·ï¼Œé‡ç‚¹å¯¹æ¯”äº†ä¸¤ç§æ–¹æ³•ï¼š1ï¼‰ä¼ ç»Ÿçš„"ç”ŸæˆåéªŒè¯"â€”â€”ç­‰å†…å®¹å…¨éƒ¨ç”Ÿæˆå®Œå†æ‹†è§£éªŒè¯ï¼›2ï¼‰åˆ›æ–°çš„"å†…è”éªŒè¯"â€”â€”è¾¹ç”Ÿæˆè¾¹éªŒè¯ã€‚ä»–è§£é‡Šäº†ä¸ºä»€ä¹ˆå†…è”éªŒè¯æ›´å¥½ï¼šç”±äºå¤§è¯­è¨€æ¨¡å‹æ˜¯è‡ªå›å½’çš„ï¼Œä¸€ä¸ªé”™è¯¯å£°æ˜ä¼šæ±¡æŸ“åç»­æ‰€æœ‰ç”Ÿæˆå†…å®¹ã€‚ä»–è¿˜è®¨è®ºäº†å†…è”æ–¹æ³•çš„æŠ€æœ¯æŒ‘æˆ˜ï¼ˆå»¶è¿Ÿå’Œäº¤æ¥é—®é¢˜ï¼‰ï¼Œä»¥åŠä»–ä½¿ç”¨ **MCP** æœåŠ¡å™¨å’Œ **Claude Code** å®ç°è¿™ä¸ªæ–¹æ¡ˆçš„è¿‡ç¨‹ã€‚

---

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **hallucination** | å¹»è§‰ï¼›AI ç”Ÿæˆçš„è™šå‡ä¿¡æ¯ |
| **derail** | è„±è½¨ã€åç¦»æ­£è½¨ |
| **atomizing** | åŸå­åŒ–ï¼ˆæ‹†æˆæœ€å°å•ä½ï¼‰ |
| **auto-regressive model** | è‡ªå›å½’æ¨¡å‹ |
| **corrupt** | æ±¡æŸ“ã€ç ´å |
| **embed** | åµŒå…¥ |
| **from the ground up** | ä»å¤´å¼€å§‹ |
| **throw... at** | æŠ•å…¥...ã€ç ¸... |
| **inline** | å†…è”çš„ã€å®æ—¶çš„ |
| **latency** | å»¶è¿Ÿ |
| **handoff** | äº¤æ¥ã€ç§»äº¤ |
| **immaculate** | å®Œç¾æ— ç‘•çš„ |
| **cache** | ç¼“å­˜ |
| **repository** | ä»“åº“ã€å­˜å‚¨åº“ |
| **spec out** | å†™è§„æ ¼è¯´æ˜ |
| **regress** | é€€åŒ–ï¼ˆè½¯ä»¶æµ‹è¯•æœ¯è¯­ï¼‰ |
| **agentic harness** | ä»£ç†å·¥å…·æ¡†æ¶ |
| **hook up** | è¿æ¥ã€æ¥å…¥ |
| **authenticate** | è®¤è¯ã€éªŒè¯èº«ä»½ |
