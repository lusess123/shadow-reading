# ğŸ¯ Prompt Caching æŠ€æœ¯è¯¦è§£ è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **20 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:00-0:12] **Prompt caching can significantly improve the speed and cost effectiveness of large language models. Sounds good. Sign me up. But, um. But what is prompt caching?**

**Prompt caching** å¯ä»¥æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„é€Ÿåº¦å’Œæˆæœ¬æ•ˆç›Šã€‚å¬èµ·æ¥ä¸é”™ï¼Œç®—æˆ‘ä¸€ä¸ªã€‚ä½†æ˜¯ï¼Œå—¯â€¦â€¦**prompt caching** åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

è§£æï¼š
* **significantly** /sÉªÉ¡ËˆnÉªfÉªkÉ™ntli/ï¼šå‰¯è¯ï¼Œæ˜¾è‘—åœ°ã€å¤§å¹…åº¦åœ°
* **cost effectiveness**ï¼šçŸ­è¯­ï¼Œæˆæœ¬æ•ˆç›Šã€æ€§ä»·æ¯”
* **sign me up** ğŸ”¥ï¼šå£è¯­çŸ­è¯­ï¼Œå­—é¢æ„æ€æ˜¯"ç»™æˆ‘æŠ¥å"ï¼Œå¼•ç”³ä¸º"ç®—æˆ‘ä¸€ä¸ªã€æˆ‘è¦å‚åŠ "ï¼Œè¡¨ç¤ºå¯¹æŸäº‹æ„Ÿå…´è¶£

---

(2) [0:13-0:32] **Well, let me start by defining what prompt caching is not. So it is not regular output focused caching. So let me give you an example of that. If you've sent in a query and you've sent that query into, let's say, a database, maybe it's a SQL query.**

å¥½ï¼Œæˆ‘å…ˆä»"ä»€ä¹ˆä¸æ˜¯ prompt caching"è¯´èµ·ã€‚å®ƒä¸æ˜¯å¸¸è§„çš„ä»¥è¾“å‡ºä¸ºä¸­å¿ƒçš„ç¼“å­˜ã€‚ä¸¾ä¸ªä¾‹å­ï¼šå‡è®¾ä½ å‘é€äº†ä¸€ä¸ªæŸ¥è¯¢ï¼ŒæŠŠè¿™ä¸ªæŸ¥è¯¢å‘åˆ°ä¸€ä¸ªæ•°æ®åº“é‡Œï¼Œæ¯”å¦‚è¯´æ˜¯ä¸€ä¸ª **SQL** æŸ¥è¯¢ã€‚

è§£æï¼š
* **let me start by doing sth** ğŸ”¥ï¼šçŸ­è¯­ï¼Œ"è®©æˆ‘å…ˆä»â€¦â€¦å¼€å§‹"ï¼Œå¸¸ç”¨äºæ¼”è®²å¼€å¤´
* **output focused**ï¼šå¤åˆå½¢å®¹è¯ï¼Œä»¥è¾“å‡ºä¸ºå¯¼å‘çš„
* **query** /ËˆkwÉªri/ï¼šåè¯ï¼ŒæŸ¥è¯¢ï¼ˆæ•°æ®åº“æœ¯è¯­ï¼‰

---

(3) [0:33-0:59] **Well, the database is going to process that result and give you the output here. So we've got the result from the database lookup. Now those results they can be stored in a cache. And when somebody else makes that same query soon afterwards, well, we can just kind of skip the lookup and go directly to the result stored in the cache.**

æ•°æ®åº“ä¼šå¤„ç†è¿™ä¸ªè¯·æ±‚å¹¶ç»™ä½ è¾“å‡ºç»“æœã€‚æˆ‘ä»¬ä»æ•°æ®åº“æŸ¥æ‰¾ä¸­å¾—åˆ°äº†ç»“æœã€‚è¿™äº›ç»“æœå¯ä»¥è¢«å­˜å‚¨åœ¨ç¼“å­˜ä¸­ã€‚å½“ä¸ä¹…åå¦ä¸€ä¸ªäººå‘èµ·åŒæ ·çš„æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥è·³è¿‡æ•°æ®åº“æŸ¥æ‰¾ï¼Œç›´æ¥è¿”å›ç¼“å­˜ä¸­çš„ï¿½ï¿½æœã€‚

è§£æï¼š
* **lookup** /ËˆlÊŠkÊŒp/ï¼šåè¯ï¼ŒæŸ¥æ‰¾ã€æ£€ç´¢ï¼ˆæŠ€æœ¯æœ¯è¯­ï¼‰
* **cache** /kÃ¦Êƒ/ï¼šåè¯ï¼Œç¼“å­˜ï¼ˆæ³¨æ„å‘éŸ³å’Œ cash ç›¸åŒï¼‰
* **skip** /skÉªp/ï¼šåŠ¨è¯ï¼Œè·³è¿‡ã€çœç•¥
* **kind of** ğŸ”¥ï¼šå£è¯­çŸ­è¯­ï¼Œ"æœ‰ç‚¹ã€ç®—æ˜¯"ï¼Œç”¨äºä½¿è¡¨è¾¾æ›´éšæ„

---

(4) [1:00-1:27] **Great. Now let's apply this concept to an LLM. So in this case, we have an input prompt that we've received from a user. And that input prompt is sent for processing into a large language model. And then, the large language model is going to process that and send back some sort of output. We'll call this the response from the LLM.**

å¥½çš„ï¼Œç°åœ¨è®©æˆ‘ä»¬æŠŠè¿™ä¸ªæ¦‚å¿µåº”ç”¨åˆ° **LLM** ä¸Šã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä»ç”¨æˆ·é‚£é‡Œæ”¶åˆ°çš„è¾“å…¥æç¤ºè¯ã€‚è¿™ä¸ªè¾“å…¥æç¤ºè¯è¢«å‘é€åˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ï¼Œç„¶åå¤§è¯­è¨€æ¨¡å‹ä¼šå¤„ç†å®ƒå¹¶è¿”å›æŸç§è¾“å‡ºï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º **LLM** çš„å“åº”ã€‚

è§£æï¼š
* **apply... to...** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå°†â€¦â€¦åº”ç”¨åˆ°â€¦â€¦
* **input prompt**ï¼šè¾“å…¥æç¤ºè¯ï¼ˆAI é¢†åŸŸæ ¸å¿ƒæœ¯è¯­ï¼‰
* **some sort of**ï¼šçŸ­è¯­ï¼ŒæŸç§ç±»å‹çš„ã€æŸç§

---

(5) [1:27-1:53] **Okay. So that's pretty standard practice. But what happens if soon after another user sends in the same prompt? Does that prompt caching mean that we can just kind of skip the LLM call and just go directly to the cached response? No. What I'm describing here is actually called output caching. And it's certainly something that can be done with large language models, but it is not what prompt caching is.**

è¿™æ˜¯å¾ˆæ ‡å‡†çš„åšæ³•ã€‚ä½†å¦‚æœä¸ä¹…åå¦ä¸€ä¸ªç”¨æˆ·å‘é€äº†åŒæ ·çš„æç¤ºè¯ä¼šæ€æ ·ï¼Ÿ**prompt caching** æ˜¯ä¸æ˜¯æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç›´æ¥è·³è¿‡ **LLM** è°ƒç”¨ï¼Œç›´æ¥è·å–ç¼“å­˜çš„å“åº”ï¼Ÿä¸æ˜¯çš„ã€‚æˆ‘è¿™é‡Œæè¿°çš„å®é™…ä¸Šå«åš**è¾“å‡ºç¼“å­˜**ï¼ˆ**output caching**ï¼‰ã€‚è¿™å½“ç„¶å¯ä»¥ç”¨åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸Šï¼Œä½†å®ƒä¸æ˜¯ **prompt caching**ã€‚

è§£æï¼š
* **standard practice**ï¼šçŸ­è¯­ï¼Œæ ‡å‡†åšæ³•ã€å¸¸è§„æ“ä½œ
* **output caching**ï¼šè¾“å‡ºç¼“å­˜ï¼Œç¼“å­˜æ¨¡å‹çš„å®Œæ•´è¾“å‡ºç»“æœ
* **certainly** /ËˆsÉœËrtnli/ï¼šå‰¯è¯ï¼Œå½“ç„¶ã€ç¡®å®

---

(6) [1:53-2:12] **Prompt caching is about caching only the input prompt. Only caching this part here so that the LLM doesn't need to process it a second time. Now, to understand prompt caching, you need to understand what happens when we take a large language model, an LLM, and we send to that large language model a prompt.**

**Prompt caching** æ˜¯åªç¼“å­˜è¾“å…¥æç¤ºè¯ã€‚åªç¼“å­˜è¿™ä¸ªéƒ¨åˆ†ï¼Œè¿™æ · **LLM** å°±ä¸éœ€è¦å†æ¬¡å¤„ç†å®ƒã€‚è¦ç†è§£ **prompt caching**ï¼Œä½ éœ€è¦ç†è§£å½“æˆ‘ä»¬å‘å¤§è¯­è¨€æ¨¡å‹å‘é€ä¸€ä¸ªæç¤ºè¯æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

è§£æï¼š
* **a second time**ï¼šçŸ­è¯­ï¼Œç¬¬äºŒæ¬¡ã€å†ä¸€æ¬¡
* **cache... so that...**ï¼šç¼“å­˜â€¦â€¦ä»¥ä¾¿â€¦â€¦ï¼ˆç›®çš„çŠ¶è¯­ä»å¥ï¼‰

---

(7) [2:13-2:45] **So this is where we are sending in a request prompt. Now what happens here is the model computes something called key-value pairs. So let's model that in the LLM here as KV pairs. And it does that at every transformer layer for every token in your input.**

è¿™å°±æ˜¯æˆ‘ä»¬å‘é€è¯·æ±‚æç¤ºè¯çš„åœ°æ–¹ã€‚æ¨¡å‹ä¼šè®¡ç®—ä¸€ç§å«åš**é”®å€¼å¯¹**ï¼ˆ**key-value pairs**ï¼‰çš„ä¸œè¥¿ã€‚æˆ‘ä»¬åœ¨ **LLM** ä¸­æŠŠå®ƒç§°ä¸º **KV pairs**ã€‚æ¨¡å‹åœ¨æ¯ä¸€ä¸ª **transformer** å±‚ã€å¯¹è¾“å…¥ä¸­çš„æ¯ä¸€ä¸ª **token** éƒ½ä¼šè®¡ç®—è¿™ä¸ªã€‚

è§£æï¼š
* **compute** /kÉ™mËˆpjuËt/ï¼šåŠ¨è¯ï¼Œè®¡ç®—ï¼ˆæ¯” calculate æ›´åæŠ€æœ¯é¢†åŸŸï¼‰
* **key-value pairs**ï¼ˆ**KV pairs**ï¼‰ï¼šé”®å€¼å¯¹ï¼ŒTransformer æ¶æ„ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ•°æ®ç»“æ„
* **transformer layer**ï¼šTransformer å±‚ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºæœ¬ç»„ä»¶
* **token** /ËˆtoÊŠkÉ™n/ï¼šåè¯ï¼Œè¯å…ƒï¼ˆNLP æœ¯è¯­ï¼Œæ–‡æœ¬çš„æœ€å°å¤„ç†å•ä½ï¼‰

---

(8) [2:46-3:03] **And we can think of these key pairs as the model's internal understanding of your prompt. So how every word relates to every other word, what context matters, what information to focus on. And computing these, it can be a bit expensive. This is the prefilled phase that happens before the LLM actually can generate its first token of output.**

æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›é”®å€¼å¯¹çœ‹ä½œæ¨¡å‹å¯¹ä½ çš„æç¤ºè¯çš„å†…éƒ¨ç†è§£â€”â€”æ¯ä¸ªè¯ä¸å…¶ä»–è¯çš„å…³ç³»æ˜¯ä»€ä¹ˆã€å“ªäº›ä¸Šä¸‹æ–‡é‡è¦ã€åº”è¯¥å…³æ³¨å“ªäº›ä¿¡æ¯ã€‚è®¡ç®—è¿™äº›å¼€é”€ä¸å°ã€‚è¿™å°±æ˜¯ **LLM** åœ¨çœŸæ­£ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡º token ä¹‹å‰çš„**é¢„å¡«å……é˜¶æ®µ**ï¼ˆ**prefill phase**ï¼‰ã€‚

è§£æï¼š
* **internal understanding**ï¼šå†…éƒ¨ç†è§£ï¼ˆæ¨¡å‹å†…éƒ¨çš„è¡¨å¾ï¼‰
* **relate to** ğŸ”¥ï¼šçŸ­è¯­ï¼Œä¸â€¦â€¦ç›¸å…³ã€å…³è”
* **prefilled phase**ï¼šé¢„å¡«å……é˜¶æ®µï¼ˆLLM æ¨ç†çš„ç¬¬ä¸€ä¸ªé˜¶æ®µï¼Œå¤„ç†æ‰€æœ‰è¾“å…¥ tokenï¼‰
* **a bit expensive**ï¼šæœ‰ç‚¹æ˜‚è´µï¼ˆè¿™é‡ŒæŒ‡è®¡ç®—æˆæœ¬é«˜ï¼‰

---

(9) [3:03-3:24] **So what we can do is cache the KV pairs. So this here is where the prompt caching actually applies. It stores these precomputed KV pairs.**

æ‰€ä»¥æˆ‘ä»¬èƒ½åšçš„å°±æ˜¯ç¼“å­˜è¿™äº› **KV pairs**ã€‚è¿™æ‰æ˜¯ **prompt caching** çœŸæ­£å‘æŒ¥ä½œç”¨çš„åœ°æ–¹â€”â€”å®ƒå­˜å‚¨è¿™äº›**é¢„å…ˆè®¡ç®—å¥½çš„ KV pairs**ã€‚

è§£æï¼š
* **precomputed** /priËkÉ™mËˆpjuËtÉªd/ï¼šå½¢å®¹è¯ï¼Œé¢„å…ˆè®¡ç®—çš„ï¼ˆpre- å‰ç¼€ + computedï¼‰
* **this is where... actually applies**ï¼šè¿™æ‰æ˜¯â€¦â€¦çœŸæ­£é€‚ç”¨çš„åœ°æ–¹

---

(10) [3:24-3:42] **Now for a simple prompt like what's the capital of France? Well, this caching isn't going to save much time. Processing a handful of tokens that's cheap and fast you don't need caching. But what if we structured our prompt in a little bit of a different way? So let's consider here a prompt that we're going to build in detail.**

å¯¹äºä¸€ä¸ªç®€å•çš„æç¤ºè¯ï¼Œæ¯”å¦‚"æ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"â€”â€”ç¼“å­˜å¹¶ä¸ä¼šçœå¤šå°‘æ—¶é—´ã€‚å¤„ç†å°‘é‡ token æœ¬æ¥å°±ä¾¿å®œåˆå¿«ï¼Œä¸éœ€è¦ç¼“å­˜ã€‚ä½†å¦‚æœæˆ‘ä»¬æ¢ä¸€ç§æ–¹å¼æ¥ç»„ç»‡æç¤ºè¯å‘¢ï¼Ÿè®©æˆ‘ä»¬æ¥è¯¦ç»†æ„å»ºä¸€ä¸ªæç¤ºè¯ã€‚

è§£æï¼š
* **a handful of** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå°‘é‡çš„ã€ä¸€æŠŠçš„ï¼ˆå­—é¢ï¼šä¸€æŠŠèƒ½æŠ“ä½çš„é‡ï¼‰
* **structured** /ËˆstrÊŒktÊƒÉ™rd/ï¼šåŠ¨è¯è¿‡å»å¼ï¼Œç»“æ„åŒ–åœ°ç»„ç»‡

---

(11) [3:42-4:08] **And in this prompt, we have put a bunch of stuff. So, for example, we're going to put a big document in our context window here. So this is a 50-page document. And then, in our prompt, we are going to ask the large language model to summarize that document. So here is the summarize request.**

åœ¨è¿™ä¸ªæç¤ºè¯é‡Œï¼Œæˆ‘ä»¬æ”¾äº†ä¸€å †ä¸œè¥¿ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬åœ¨ä¸Šä¸‹æ–‡çª—å£é‡Œæ”¾å…¥ä¸€ä¸ªå¤§æ–‡æ¡£â€”â€”è¿™æ˜¯ä¸€ä»½ 50 é¡µçš„æ–‡æ¡£ã€‚ç„¶åï¼Œåœ¨æç¤ºè¯ä¸­æˆ‘ä»¬è¦æ±‚å¤§è¯­è¨€æ¨¡å‹å¯¹è¿™ä»½æ–‡æ¡£è¿›è¡Œæ‘˜è¦ã€‚è¿™å°±æ˜¯æ‘˜è¦è¯·æ±‚ã€‚

è§£æï¼š
* **a bunch of stuff** ğŸ”¥ï¼šå£è¯­çŸ­è¯­ï¼Œä¸€å †ä¸œè¥¿
* **context window**ï¼šä¸Šä¸‹æ–‡çª—å£ï¼ˆLLM èƒ½ä¸€æ¬¡æ€§å¤„ç†çš„è¾“å…¥é•¿åº¦é™åˆ¶ï¼‰
* **summarize** /ËˆsÊŒmÉ™raÉªz/ï¼šåŠ¨è¯ï¼Œæ€»ç»“ã€æ‘˜è¦

---

(12) [4:08-4:27] **So now the model has to compute KV pairs for thousands of tokens across dozens of transformer layers, millions of operations before it can ever start generating that response. So with prompt caching, that processing work is getting saved.**

ç°åœ¨æ¨¡å‹å¿…é¡»åœ¨å‡ åä¸ª **transformer** å±‚ä¸Šä¸ºæ•°åƒä¸ª **token** è®¡ç®— **KV pairs**â€”â€”åœ¨å®ƒèƒ½å¼€å§‹ç”Ÿæˆå“åº”ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œæ•°ç™¾ä¸‡æ¬¡è¿ç®—ã€‚æœ‰äº† **prompt caching**ï¼Œè¿™äº›å¤„ç†å·¥ä½œå°±è¢«ä¿å­˜ä¸‹æ¥äº†ã€‚

è§£æï¼š
* **dozens of**ï¼šçŸ­è¯­ï¼Œå‡ åä¸ª
* **millions of operations**ï¼šæ•°ç™¾ä¸‡æ¬¡è¿ç®—
* **before it can ever start**ï¼šåœ¨å®ƒèƒ½å¤Ÿå¼€å§‹ä¹‹å‰ï¼ˆever åŠ å¼ºè¯­æ°”ï¼‰

---

(13) [4:27-4:52] **Now, later on in a new conversation or an API call, you could send that same 50-page document followed this time by instead of asking to summarize, we just provide some kind of different question. And the system will recognize that document prefix. It will retrieve the cached KV pairs and then only process the new question at the end. And that's a pretty notable saving in latency and cost.**

ä¹‹ååœ¨ä¸€ä¸ªæ–°çš„å¯¹è¯æˆ– **API** è°ƒç”¨ä¸­ï¼Œä½ å¯ä»¥å‘é€åŒæ ·çš„ 50 é¡µæ–‡æ¡£ï¼Œä½†è¿™æ¬¡ä¸æ˜¯è®©å®ƒåšæ‘˜è¦ï¼Œè€Œæ˜¯æä¸€ä¸ªä¸åŒçš„é—®é¢˜ã€‚ç³»ç»Ÿä¼šè¯†åˆ«å‡ºé‚£ä¸ªæ–‡æ¡£å‰ç¼€ï¼Œæå–ç¼“å­˜çš„ **KV pairs**ï¼Œç„¶ååªå¤„ç†æœ«å°¾çš„æ–°é—®é¢˜ã€‚è¿™åœ¨å»¶è¿Ÿå’Œæˆæœ¬æ–¹é¢æ˜¯ç›¸å½“å¯è§‚çš„èŠ‚çœã€‚

è§£æï¼š
* **prefix** /ËˆpriËfÉªks/ï¼šåè¯ï¼Œå‰ç¼€ï¼ˆè¿™é‡ŒæŒ‡æç¤ºè¯ä¸­ä¸å˜çš„å‰é¢éƒ¨åˆ†ï¼‰
* **retrieve** /rÉªËˆtriËv/ï¼šåŠ¨è¯ï¼Œæ£€ç´¢ã€æå–
* **latency** /ËˆleÉªtÉ™nsi/ï¼šåè¯ï¼Œå»¶è¿Ÿï¼ˆç³»ç»Ÿå“åº”æ—¶é—´ï¼‰
* **notable** /ËˆnoÊŠtÉ™bl/ï¼šå½¢å®¹è¯ï¼Œæ˜¾è‘—çš„ã€å€¼å¾—æ³¨æ„çš„

---

(14) [4:52-5:17] **Now what kinds of things can be cached? Let's answer that question. What can be cached? Well, I've already mentioned that you can put documents into a prompt into the context window. So like a 50-page product manual or a research paper or a legal contract or anything, you want to kind of ask multiple questions about that can be stored in the cache.**

é‚£ä»€ä¹ˆç±»å‹çš„å†…å®¹å¯ä»¥è¢«ç¼“å­˜å‘¢ï¼Ÿæˆ‘å·²ç»æåˆ°äº†å¯ä»¥æŠŠæ–‡æ¡£æ”¾è¿›æç¤ºè¯çš„ä¸Šä¸‹æ–‡çª—å£é‡Œâ€”â€”æ¯”å¦‚ 50 é¡µçš„äº§å“æ‰‹å†Œã€ç ”ç©¶è®ºæ–‡ã€æ³•å¾‹åˆåŒï¼Œæˆ–è€…ä»»ä½•ä½ æƒ³å¯¹å…¶åå¤æé—®çš„å†…å®¹ï¼Œéƒ½å¯ä»¥å­˜åœ¨ç¼“å­˜ä¸­ã€‚

è§£æï¼š
* **product manual**ï¼šäº§å“æ‰‹å†Œ
* **research paper**ï¼šç ”ç©¶è®ºæ–‡
* **legal contract**ï¼šæ³•å¾‹åˆåŒ
* **multiple questions**ï¼šå¤šä¸ªé—®é¢˜

---

(15) [5:17-5:47] **But another good example, and probably the most common thing that gets cached, is the system prompt. Now, system prompts are the most common use case, where really every chatbot has instructions that tell it how to operate. It defines its personality and its rules and its behavior. You know, things like you're a helpful customer service agent, blah, blah, blah. That sort of thing can be cached.**

ä½†å¦ä¸€ä¸ªå¥½ä¾‹å­ï¼Œä¹Ÿå¯èƒ½æ˜¯æœ€å¸¸è¢«ç¼“å­˜çš„å†…å®¹ï¼Œå°±æ˜¯**ç³»ç»Ÿæç¤ºè¯**ï¼ˆ**system prompt**ï¼‰ã€‚ç³»ç»Ÿæç¤ºè¯æ˜¯æœ€å¸¸è§çš„ä½¿ç”¨åœºæ™¯â€”â€”å‡ ä¹æ¯ä¸ªèŠå¤©æœºå™¨äººéƒ½æœ‰æŒ‡ä»¤æ¥å‘Šè¯‰å®ƒå¦‚ä½•è¿ä½œï¼Œå®šä¹‰å®ƒçš„ä¸ªæ€§ã€è§„åˆ™å’Œè¡Œä¸ºã€‚æ¯”å¦‚"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„å®¢æœä»£ç†"ä¹‹ç±»çš„ã€‚è¿™ç±»å†…å®¹å°±å¯ä»¥è¢«ç¼“å­˜ã€‚

è§£æï¼š
* **system prompt** ğŸ”¥ï¼šç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰ AI è¡Œä¸ºå’Œè§„åˆ™çš„éšè—æŒ‡ä»¤ï¼‰
* **use case**ï¼šä½¿ç”¨åœºæ™¯ã€ç”¨ä¾‹
* **blah, blah, blah** ğŸ”¥ï¼šå£è¯­ï¼Œ"è¯¸å¦‚æ­¤ç±»ã€ç­‰ç­‰ç­‰ç­‰"ï¼ˆè¡¨ç¤ºçœç•¥ä¸é‡è¦çš„å†…å®¹ï¼‰

---

(16) [5:47-6:05] **And we can also put into the cache few shot examples. So when you want the model to format responses a certain way, you show it examples. There are a bunch of other things in a prompt that could be cached as well, like tool and function definitions and conversation history.**

æˆ‘ä»¬è¿˜å¯ä»¥æŠŠ**å°‘æ ·æœ¬ç¤ºä¾‹**ï¼ˆ**few-shot examples**ï¼‰æ”¾å…¥ç¼“å­˜ã€‚å½“ä½ å¸Œæœ›æ¨¡å‹æŒ‰ç‰¹å®šæ ¼å¼å›å¤æ—¶ï¼Œå°±ç»™å®ƒå±•ç¤ºç¤ºä¾‹ã€‚è¿˜æœ‰å¾ˆå¤šå…¶ä»–ä¸œè¥¿ä¹Ÿå¯ä»¥è¢«ç¼“å­˜ï¼Œæ¯”å¦‚å·¥å…·å’Œå‡½æ•°å®šä¹‰ï¼Œä»¥åŠå¯¹è¯å†å²ã€‚

è§£æï¼š
* **few-shot examples** ğŸ”¥ï¼šå°‘æ ·æœ¬ç¤ºä¾‹ï¼ˆåœ¨æç¤ºè¯ä¸­ç»™æ¨¡å‹å±•ç¤ºå‡ ä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹æ¥å¼•å¯¼è¡Œä¸ºï¼‰
* **format responses**ï¼šæ ¼å¼åŒ–å›å¤
* **tool and function definitions**ï¼šå·¥å…·å’Œå‡½æ•°å®šä¹‰ï¼ˆLLM çš„ function calling åŠŸèƒ½ï¼‰
* **conversation history**ï¼šå¯¹è¯å†å²

---

(17) [6:05-6:36] **Okay, but when does an LLM know what gets cached? Well, it comes down to a technique called prefix matching to figure out what should actually be stored in the cache. So the cache system matches your prompt from the very beginning. Token by token. And when it encounters the first token that differs from what's cached, then caching stops and normal processing takes over. And that makes prompt structure important for automatic caching.**

ä½† **LLM** æ€ä¹ˆçŸ¥é“å“ªäº›å†…å®¹è¯¥è¢«ç¼“å­˜å‘¢ï¼Ÿè¿™å½’ç»“äºä¸€ç§å«åš**å‰ç¼€åŒ¹é…**ï¼ˆ**prefix matching**ï¼‰çš„æŠ€æœ¯ã€‚ç¼“å­˜ç³»ç»Ÿä»æç¤ºè¯çš„æœ€å¼€å¤´å¼€å§‹é€ä¸ª token åŒ¹é…ã€‚å½“é‡åˆ°ç¬¬ä¸€ä¸ªä¸ç¼“å­˜ä¸åŒçš„ token æ—¶ï¼Œç¼“å­˜å°±åœæ­¢äº†ï¼Œæ­£å¸¸å¤„ç†æ¥ç®¡ã€‚è¿™ä½¿å¾—æç¤ºè¯çš„ç»“æ„å¯¹äºè‡ªåŠ¨ç¼“å­˜æ¥è¯´éå¸¸é‡è¦ã€‚

è§£æï¼š
* **it comes down to** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå½’ç»“ä¸ºã€å–å†³äº
* **prefix matching**ï¼šå‰ç¼€åŒ¹é…ï¼ˆä»å¤´å¼€å§‹é€ä¸€æ¯”å¯¹çš„åŒ¹é…ç­–ç•¥ï¼‰
* **encounter** /ÉªnËˆkaÊŠntÉ™r/ï¼šåŠ¨è¯ï¼Œé‡åˆ°
* **take over** ğŸ”¥ï¼šçŸ­è¯­ï¼Œæ¥ç®¡ã€æ¥æ›¿

---

(18) [6:36-7:07] **So let's consider a prompt structure. Let's say that we've got a prompt that has in it some system instructions. And then, we're going to put a document in as well. Let's say there's some kind of 20-page manual that's here. And then, yeah, we're going to include some few shot examples as well. And then finally we're going to submit a user question. So the user question might be like what are the warranty terms. Am I going to use this manual to figure out the answer.**

æ¥çœ‹ä¸€ä¸ªæç¤ºè¯ç»“æ„ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæç¤ºè¯ï¼Œé‡Œé¢åŒ…å«ç³»ç»ŸæŒ‡ä»¤ï¼Œç„¶åæ”¾å…¥ä¸€ä»½æ–‡æ¡£â€”â€”æ¯”å¦‚ä¸€ä»½ 20 é¡µçš„æ‰‹å†Œï¼Œå†åŠ ä¸Šä¸€äº›å°‘æ ·æœ¬ç¤ºä¾‹ï¼Œæœ€åæäº¤ç”¨æˆ·çš„é—®é¢˜ã€‚ç”¨æˆ·é—®é¢˜å¯èƒ½æ˜¯"ä¿ä¿®æ¡æ¬¾æ˜¯ä»€ä¹ˆï¼Ÿ"â€”â€”æˆ‘è¦ç”¨è¿™ä¸ªæ‰‹å†Œæ¥æ‰¾åˆ°ç­”æ¡ˆã€‚

è§£æï¼š
* **warranty terms**ï¼šä¿ä¿®æ¡æ¬¾
* **figure out** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå¼„æ¸…æ¥šã€æ‰¾å‡ºç­”æ¡ˆ
* **submit** /sÉ™bËˆmÉªt/ï¼šåŠ¨è¯ï¼Œæäº¤

---

(19) [7:07-7:52] **Well, this structure puts all of the static content first. So when the next request comes in with just a different question, like now it's going to say what's the return policy? The cache matches through all of this static content here. All of this stuff is actually inside of the cache. And we only need to process the new question at the end. But if we were to flip this around and we were to put the dynamic stuff first, put the question up here, then the cache is going to fail immediately when the question changes. You'd have to reprocess everything here again, so it's best to put that stuff up front specifically when we're talking about automatic prompt caching.**

è¿™ä¸ªç»“æ„æŠŠæ‰€æœ‰é™æ€å†…å®¹æ”¾åœ¨å‰é¢ã€‚å½“ä¸‹ä¸€ä¸ªè¯·æ±‚å¸¦ç€ä¸åŒçš„é—®é¢˜è¿›æ¥â€”â€”æ¯”å¦‚"é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"â€”â€”ç¼“å­˜å¯ä»¥åŒ¹é…æ‰€æœ‰è¿™äº›é™æ€å†…å®¹ï¼Œæˆ‘ä»¬åªéœ€è¦å¤„ç†æœ«å°¾çš„æ–°é—®é¢˜ã€‚ä½†å¦‚æœåè¿‡æ¥ï¼ŒæŠŠåŠ¨æ€å†…å®¹æ”¾åœ¨å‰é¢ã€æŠŠé—®é¢˜æ”¾åœ¨æœ€å‰é¢ï¼Œé‚£ä¹ˆå½“é—®é¢˜ä¸€å˜åŒ–ï¼Œç¼“å­˜å°±ä¼šç«‹å³å¤±æ•ˆã€‚ä½ ä¸å¾—ä¸é‡æ–°å¤„ç†æ‰€æœ‰å†…å®¹ã€‚æ‰€ä»¥åœ¨è‡ªåŠ¨ **prompt caching** çš„åœºæ™¯ä¸‹ï¼Œæœ€å¥½æŠŠé™æ€å†…å®¹æ”¾åœ¨æœ€å‰é¢ã€‚

è§£æï¼š
* **static content**ï¼šé™æ€å†…å®¹ï¼ˆä¸å˜çš„éƒ¨åˆ†ï¼‰
* **dynamic stuff**ï¼šåŠ¨æ€å†…å®¹ï¼ˆä¼šå˜åŒ–çš„éƒ¨åˆ†ï¼‰
* **flip this around** ğŸ”¥ï¼šçŸ­è¯­ï¼Œåè¿‡æ¥ã€é¢ å€’
* **return policy**ï¼šé€€è´§æ”¿ç­–
* **up front** ğŸ”¥ï¼šçŸ­è¯­ï¼Œåœ¨å‰é¢ã€æå‰ï¼ˆè¿™é‡ŒæŒ‡ç»“æ„ä¸Šæ”¾åœ¨å‰é¢ï¼‰
* **reprocess**ï¼šåŠ¨è¯ï¼Œé‡æ–°å¤„ç†ï¼ˆre- å‰ç¼€è¡¨ç¤º"å†æ¬¡"ï¼‰

---

(20) [7:52-8:57] **So that's what gets cached. But a couple of other notes based on prompt caching that we should talk about. One is like how much stuff to get cached. Well, typically, you need at least 1024 tokens to initiate caching. That's really before caching provides any benefit. Because below that threshold, the overhead of managing the cache really exceeds the savings. And also caches don't last forever. They're usually cleared after 5 to 10 minutes just to keep the data fresh. Although some can hang around for up to 24 hours. And some providers provide the automatic caching that I've been talking about here. But some providers actually require you to explicitly mark which parts of your prompt should be cached in your API calls. So these are the explicit types of caching. So, so, that's prompt caching. And with the right use case it's a way to reduce LLM costs and latency.**

æœ€åè¿˜æœ‰å‡ ä¸ªå…³äº **prompt caching** çš„è¦ç‚¹ã€‚é¦–å…ˆæ˜¯éœ€è¦ç¼“å­˜å¤šå°‘å†…å®¹â€”â€”é€šå¸¸è‡³å°‘éœ€è¦ **1024** ä¸ª token æ‰èƒ½å¯åŠ¨ç¼“å­˜ï¼Œå› ä¸ºä½äºè¿™ä¸ªé˜ˆå€¼ï¼Œç®¡ç†ç¼“å­˜çš„å¼€é”€å®é™…ä¸Šè¶…è¿‡äº†èŠ‚çœçš„éƒ¨åˆ†ã€‚å…¶æ¬¡ï¼Œç¼“å­˜ä¸ä¼šæ°¸è¿œå­˜åœ¨ï¼Œé€šå¸¸åœ¨ 5 åˆ° 10 åˆ†é’Ÿåå°±ä¼šè¢«æ¸…é™¤ä»¥ä¿æŒæ•°æ®æ–°é²œï¼Œä¸è¿‡æœ‰äº›å¯ä»¥ä¿ç•™é•¿è¾¾ 24 å°æ—¶ã€‚å¦å¤–ï¼Œæœ‰äº›æä¾›å•†æä¾›è‡ªåŠ¨ç¼“å­˜ï¼Œä½†æœ‰äº›æä¾›å•†è¦æ±‚ä½ åœ¨ **API** è°ƒç”¨ä¸­æ˜¾å¼æ ‡è®°å“ªäº›éƒ¨åˆ†åº”è¯¥è¢«ç¼“å­˜ã€‚æ€»ä¹‹ï¼Œè¿™å°±æ˜¯ **prompt caching**â€”â€”åœ¨åˆé€‚çš„åœºæ™¯ä¸‹ï¼Œå®ƒæ˜¯é™ä½ **LLM** æˆæœ¬å’Œå»¶è¿Ÿçš„æœ‰æ•ˆæ–¹å¼ã€‚

è§£æï¼š
* **initiate** /ÉªËˆnÉªÊƒieÉªt/ï¼šåŠ¨è¯ï¼Œå¯åŠ¨ã€å¼€å§‹
* **threshold** /ËˆÎ¸reÊƒhoÊŠld/ï¼šåè¯ï¼Œé˜ˆå€¼ã€é—¨æ§›
* **overhead** /ËˆoÊŠvÉ™rhed/ ğŸ”¥ï¼šåè¯ï¼Œå¼€é”€ã€é¢å¤–æˆæœ¬ï¼ˆæŠ€æœ¯æœ¯è¯­ï¼‰
* **exceed** /ÉªkËˆsiËd/ï¼šåŠ¨è¯ï¼Œè¶…è¿‡
* **hang around** ğŸ”¥ï¼šå£è¯­çŸ­è¯­ï¼Œé€—ç•™ã€å­˜åœ¨ä¸€æ®µæ—¶é—´
* **explicitly** /ÉªkËˆsplÉªsÉªtli/ï¼šå‰¯è¯ï¼Œæ˜¾å¼åœ°ã€æ˜ç¡®åœ°ï¼ˆä¸ implicitly ç›¸å¯¹ï¼‰
* **provider**ï¼šåè¯ï¼Œæä¾›å•†ï¼ˆè¿™é‡ŒæŒ‡ AI æœåŠ¡æä¾›å•†å¦‚ OpenAIã€Anthropic ç­‰ï¼‰

---

## ğŸ“š æ®µè½å°ç»“

æœ¬è§†é¢‘è¯¦ç»†è®²è§£äº† **Prompt Caching** çš„æ¦‚å¿µâ€”â€”å®ƒä¸æ˜¯ç¼“å­˜æ¨¡å‹è¾“å‡ºï¼Œè€Œæ˜¯ç¼“å­˜è¾“å…¥æç¤ºè¯çš„ **KV pairs**ï¼ˆé¢„è®¡ç®—çš„é”®å€¼å¯¹ï¼‰ï¼Œä»è€Œè·³è¿‡é‡å¤çš„é¢„å¡«å……é˜¶æ®µã€‚é€‚åˆç¼“å­˜çš„å†…å®¹åŒ…æ‹¬é•¿æ–‡æ¡£ã€ç³»ç»Ÿæç¤ºè¯ã€å°‘æ ·æœ¬ç¤ºä¾‹ç­‰ã€‚å…³é”®è¦ç‚¹æ˜¯æç¤ºè¯ç»“æ„å¿…é¡»æŠŠé™æ€å†…å®¹æ”¾åœ¨å‰é¢ï¼ˆå‰ç¼€åŒ¹é…åŸåˆ™ï¼‰ï¼Œä¸”è‡³å°‘éœ€è¦ 1024 ä¸ª token æ‰èƒ½è·ç›Šã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **prompt caching** | æç¤ºè¯ç¼“å­˜ï¼Œç¼“å­˜è¾“å…¥çš„ KV pairs ä»¥é¿å…é‡å¤è®¡ç®— |
| **key-value pairs (KV pairs)** | é”®å€¼å¯¹ï¼ŒTransformer æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ•°æ®ç»“æ„ |
| **prefill phase** | é¢„å¡«å……é˜¶æ®µï¼ŒLLM ç”Ÿæˆè¾“å‡ºå‰å¤„ç†æ‰€æœ‰è¾“å…¥ token çš„é˜¶æ®µ |
| **prefix matching** | å‰ç¼€åŒ¹é…ï¼Œä»å¤´é€ token æ¯”å¯¹æ¥ç¡®å®šç¼“å­˜å‘½ä¸­èŒƒå›´ |
| **system prompt** | ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰ AI è¡Œä¸ºè§„åˆ™çš„éšè—æŒ‡ä»¤ |
| **few-shot examples** | å°‘æ ·æœ¬ç¤ºä¾‹ï¼Œåœ¨æç¤ºè¯ä¸­å±•ç¤ºçš„è¾“å…¥è¾“å‡ºèŒƒä¾‹ |
| **context window** | ä¸Šä¸‹æ–‡çª—å£ï¼Œæ¨¡å‹ä¸€æ¬¡æ€§èƒ½å¤„ç†çš„æœ€å¤§è¾“å…¥é•¿åº¦ |
| **overhead** | å¼€é”€ï¼Œé¢å¤–çš„è®¡ç®—æˆ–ç®¡ç†æˆæœ¬ |
| **threshold** | é˜ˆå€¼ï¼Œè§¦å‘æŸç§è¡Œä¸ºçš„æœ€ä½ç•Œé™ |
| **latency** | å»¶è¿Ÿï¼Œç³»ç»Ÿå“åº”æ‰€éœ€æ—¶é—´ |
| **sign me up** | ç®—æˆ‘ä¸€ä¸ªï¼ˆå£è¯­ï¼Œè¡¨ç¤ºæ„Ÿå…´è¶£ï¼‰ |
| **it comes down to** | å½’ç»“ä¸ºã€å–å†³äº |
| **flip this around** | åè¿‡æ¥ã€é¢ å€’é¡ºåº |
| **hang around** | é€—ç•™ã€åœç•™ä¸€æ®µæ—¶é—´ |
| **a handful of** | å°‘é‡çš„ |
