# ğŸ¯ MIT é€’å½’è¯­è¨€æ¨¡å‹è§£å†³ä¸Šä¸‹æ–‡çª—å£é—®é¢˜ è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **33 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:00-0:17] **MIT might have just solved the context window problem. Just a few days ago, MIT released a technical paper claiming that through their system of recursive language models, they've made giant steps forward in solving one of the biggest issues in the AI space today, context rot.**

**MIT** å¯èƒ½åˆšåˆšè§£å†³äº†ä¸Šä¸‹æ–‡çª—å£é—®é¢˜ã€‚å°±åœ¨å‡ å¤©å‰ï¼Œ**MIT** å‘å¸ƒäº†ä¸€ç¯‡æŠ€æœ¯è®ºæ–‡ï¼Œå£°ç§°ä»–ä»¬é€šè¿‡é€’å½’è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œåœ¨è§£å†³å½“ä»Š AI é¢†åŸŸæœ€å¤§çš„é—®é¢˜ä¹‹ä¸€â€”â€”**context rot**ï¼ˆä¸Šä¸‹æ–‡è…çƒ‚ï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›å±•ã€‚

è§£æï¼š
* **context window** /ËˆkÉ’ntekst ËˆwÉªndÉ™ÊŠ/ï¼šåè¯çŸ­è¯­ï¼Œä¸Šä¸‹æ–‡çª—å£ï¼ˆAI æœ¯è¯­ï¼ŒæŒ‡æ¨¡å‹èƒ½å¤„ç†çš„æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼‰
* **claiming that**ï¼šå£°ç§°ï¼Œä¸»å¼ ï¼ˆåæ¥å®¾è¯­ä»å¥ï¼‰
* **giant steps forward** ğŸ”¥ï¼šå·¨å¤§çš„è¿›æ­¥ï¼ˆæ¯” big progress æ›´ç”ŸåŠ¨ï¼‰
* **context rot**ï¼šä¸Šä¸‹æ–‡è…çƒ‚ï¼ˆAI æœ¯è¯­ï¼ŒæŒ‡éšç€ä¸Šä¸‹æ–‡å˜é•¿ï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™çš„ç°è±¡ï¼‰

---

(2) [0:17-0:36] **Now, context rot is something we have known about for a while. Just this past summer, Chroma did a deep dive on this topic and the results we got were that context window length doesn't actually matter. Whether it's 200,000 tokens or a million tokens, the effectiveness of your large language model is going to drop after about 100,000 tokens.**

**context rot** æ˜¯æˆ‘ä»¬å·²ç»çŸ¥é“ä¸€æ®µæ—¶é—´çš„é—®é¢˜äº†ã€‚å°±åœ¨ä»Šå¹´å¤å¤©ï¼Œ**Chroma** å¯¹è¿™ä¸ªè¯é¢˜åšäº†æ·±å…¥ç ”ç©¶ï¼Œå¾—å‡ºçš„ç»“è®ºæ˜¯ï¼šä¸Šä¸‹æ–‡çª—å£é•¿åº¦å®é™…ä¸Šå¹¶ä¸é‡è¦ã€‚æ— è®ºæ˜¯ 20 ä¸‡ tokens è¿˜æ˜¯ 100 ä¸‡ tokensï¼Œä½ çš„å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆæœåœ¨å¤§çº¦ 10 ä¸‡ tokens ä¹‹åå°±ä¼šä¸‹é™ã€‚

è§£æï¼š
* **for a while**ï¼šä¸€æ®µæ—¶é—´
* **deep dive** ğŸ”¥ï¼šæ·±å…¥ç ”ç©¶ã€æ·±å…¥æ¢è®¨ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼Œéå¸¸å®ç”¨ï¼‰
* **effectiveness** /ÉªËŒfekËˆtÉªvnÉ™s/ï¼šåè¯ï¼Œæœ‰æ•ˆæ€§ã€æ•ˆæœ
* **token**ï¼šåè¯ï¼Œä»¤ç‰Œï¼ˆAI æœ¯è¯­ï¼Œæ–‡æœ¬çš„æœ€å°å¤„ç†å•ä½ï¼‰

---

(3) [0:39-0:58] **However, in this paper from MIT, their recursive language model setup is showing high performance that not 200,000 tokens, not a million tokens, but when dealing with data sets of over 10 million tokens. Those numbers are absolutely insane. So, how did they actually achieve this? And what does this mean for you? Well, that's exactly what we're going to cover today.**

ç„¶è€Œï¼Œåœ¨è¿™ç¯‡ **MIT** çš„è®ºæ–‡ä¸­ï¼Œä»–ä»¬çš„é€’å½’è¯­è¨€æ¨¡å‹è®¾ç½®å±•ç¤ºäº†é«˜æ€§èƒ½â€”â€”ä¸æ˜¯ 20 ä¸‡ tokensï¼Œä¸æ˜¯ 100 ä¸‡ tokensï¼Œè€Œæ˜¯åœ¨å¤„ç†è¶…è¿‡ 1000 ä¸‡ tokens çš„æ•°æ®é›†æ—¶ã€‚è¿™äº›æ•°å­—ç®€ç›´ç–¯ç‹‚ã€‚é‚£ä¹ˆï¼Œä»–ä»¬åˆ°åº•æ˜¯å¦‚ä½•åšåˆ°çš„ï¼Ÿè¿™å¯¹ä½ æ„å‘³ç€ä»€ä¹ˆï¼Ÿè¿™æ­£æ˜¯æˆ‘ä»¬ä»Šå¤©è¦è®²çš„å†…å®¹ã€‚

è§£æï¼š
* **recursive** /rÉªËˆkÉœËsÉªv/ï¼šå½¢å®¹è¯ï¼Œé€’å½’çš„ï¼ˆè®¡ç®—æœºæœ¯è¯­ï¼‰
* **setup** /ËˆsetÊŒp/ï¼šåè¯ï¼Œè®¾ç½®ã€é…ç½®
* **absolutely insane** ğŸ”¥ï¼šç»å¯¹ç–¯ç‹‚çš„ï¼ˆå£è¯­åŒ–å¼ºè°ƒï¼Œè¡¨ç¤ºéš¾ä»¥ç½®ä¿¡ï¼‰
* **cover**ï¼šåŠ¨è¯ï¼Œæ¶µç›–ã€è®²è§£

---

(4) [0:59-1:14] **So, let's hop into this paper. So, this is the paper from MIT. It's titled recursive language models. And recursive language models are what this whole thing is all about and how they use RLMs to deal with very large prompts that otherwise these models couldn't handle just due to the size.**

é‚£æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ç¯‡è®ºæ–‡ã€‚è¿™æ˜¯æ¥è‡ª **MIT** çš„è®ºæ–‡ï¼Œæ ‡é¢˜æ˜¯"é€’å½’è¯­è¨€æ¨¡å‹"ã€‚é€’å½’è¯­è¨€æ¨¡å‹æ­£æ˜¯è¿™ä¸€åˆ‡çš„æ ¸å¿ƒï¼Œä»¥åŠä»–ä»¬å¦‚ä½•ä½¿ç”¨ **RLM** æ¥å¤„ç†é‚£äº›å› ä¸ºä½“ç§¯å¤ªå¤§è€Œæ¨¡å‹æ— æ³•å¤„ç†çš„è¶…å¤§æç¤ºè¯ã€‚

è§£æï¼š
* **hop into** ğŸ”¥ï¼šè·³å…¥ã€å¼€å§‹ï¼ˆå£è¯­åŒ–ï¼Œæ¯” start æ›´ç”ŸåŠ¨æ´»æ³¼ï¼‰
* **titled**ï¼šåŠ¨è¯è¿‡å»åˆ†è¯ï¼Œæ ‡é¢˜ä¸º...
* **prompt** /prÉ’mpt/ï¼šåè¯ï¼Œæç¤ºè¯ï¼ˆAI æœ¯è¯­ï¼‰
* **otherwise**ï¼šå‰¯è¯ï¼Œå¦åˆ™ã€ä¸ç„¶
* **due to**ï¼šä»‹è¯çŸ­è¯­ï¼Œç”±äº

---

(5) [1:14-1:32] **So imagine we're using GPT5 which is what they use in this study. It has a context window length of 272,000 tokens yet using RLMs it's able to handle data sets that are 10 million plus tokens long. How does it do that? Well, that's what this paper is all about.**

æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ **GPT5**ï¼Œè¿™æ­£æ˜¯ä»–ä»¬åœ¨ç ”ç©¶ä¸­ä½¿ç”¨çš„æ¨¡å‹ã€‚å®ƒçš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ˜¯ 27.2 ä¸‡ tokensï¼Œä½†é€šè¿‡ä½¿ç”¨ **RLM**ï¼Œå®ƒèƒ½å¤Ÿå¤„ç†è¶…è¿‡ 1000 ä¸‡ tokens çš„æ•°æ®é›†ã€‚å®ƒæ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿè¿™æ­£æ˜¯è¿™ç¯‡è®ºæ–‡çš„å…¨éƒ¨å†…å®¹ã€‚

è§£æï¼š
* **yet**ï¼šè¿è¯ï¼Œç„¶è€Œã€ä½†æ˜¯ï¼ˆè¡¨è½¬æŠ˜ï¼‰
* **be able to**ï¼šèƒ½å¤Ÿï¼ˆä¸ can åŒä¹‰ï¼Œä½†æ›´æ­£å¼ï¼‰
* **plus**ï¼šå‰¯è¯ï¼Œè¶…è¿‡ã€ä»¥ä¸Š

---

(6) [1:32-1:54] **And let's start with the actual results. So, let's start by just looking at these two graphs because it kind of illustrates this entire study pretty succinctly. So, on the left we have GPT5 and that's just the base model. So, imagine you're just kind of inside of ChatGPT's interface. And then on the right, we have the RLM version. Now, it shows two different tests. The first test is a very simplistic test and they both absolutely crush it.**

è®©æˆ‘ä»¬ä»å®é™…ç»“æœå¼€å§‹ã€‚è®©æˆ‘ä»¬å…ˆçœ‹çœ‹è¿™ä¸¤å¼ å›¾è¡¨ï¼Œå› ä¸ºå®ƒä»¬ç›¸å½“ç®€æ´åœ°è¯´æ˜äº†æ•´ä¸ªç ”ç©¶ã€‚å·¦è¾¹æ˜¯ **GPT5**ï¼Œå°±æ˜¯åŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥æƒ³è±¡æˆä½ åœ¨ **ChatGPT** ç•Œé¢é‡Œã€‚å³è¾¹æ˜¯ **RLM** ç‰ˆæœ¬ã€‚å®ƒå±•ç¤ºäº†ä¸¤ä¸ªä¸åŒçš„æµ‹è¯•ã€‚ç¬¬ä¸€ä¸ªæµ‹è¯•éå¸¸ç®€å•ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½å®Œå…¨ç¢¾å‹äº†å®ƒã€‚

è§£æï¼š
* **illustrate** /ËˆÉªlÉ™streÉªt/ï¼šåŠ¨è¯ï¼Œè¯´æ˜ã€é˜è¿°
* **succinctly** /sÉ™kËˆsÉªÅ‹ktli/ï¼šå‰¯è¯ï¼Œç®€æ´åœ°
* **base model**ï¼šåŸºç¡€æ¨¡å‹
* **simplistic** /sÉªmËˆplÉªstÉªk/ï¼šå½¢å®¹è¯ï¼Œè¿‡äºç®€å•çš„
* **crush it** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œç¢¾å‹ã€è½»æ¾æå®šï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰

---

(7) [1:55-2:14] **It's called a needle in the haystack test. It's essentially giving the large language model a large document and somewhere buried in that document is the answer to your question. So in the document it says my favorite color is yellow. I ask the LLM what's my favorite color? It comes back with yellow. These systems, these frontier models are very good at this no matter the context length because it's such a simple ask.**

è¿™å«åš"å¤§æµ·æé’ˆ"æµ‹è¯•ã€‚æœ¬è´¨ä¸Šå°±æ˜¯ç»™å¤§è¯­è¨€æ¨¡å‹ä¸€ä¸ªå¤§æ–‡æ¡£ï¼Œç­”æ¡ˆåŸ‹è—åœ¨æ–‡æ¡£çš„æŸä¸ªåœ°æ–¹ã€‚æ¯”å¦‚æ–‡æ¡£é‡Œå†™ç€"æˆ‘æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯é»„è‰²"ï¼Œæˆ‘é—® **LLM** æˆ‘æœ€å–œæ¬¢ä»€ä¹ˆé¢œè‰²ï¼Œå®ƒå›ç­”é»„è‰²ã€‚è¿™äº›ç³»ç»Ÿï¼Œè¿™äº›å‰æ²¿æ¨¡å‹åœ¨è¿™æ–¹é¢éå¸¸æ“…é•¿ï¼Œæ— è®ºä¸Šä¸‹æ–‡å¤šé•¿ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¦‚æ­¤ç®€å•çš„è¯·æ±‚ã€‚

è§£æï¼š
* **needle in the haystack** ğŸ”¥ï¼šä¹ è¯­ï¼Œå¤§æµ·æé’ˆï¼ˆhaystack = å¹²è‰å †ï¼‰
* **buried** /Ëˆberid/ï¼šåŠ¨è¯è¿‡å»åˆ†è¯ï¼ŒåŸ‹è—çš„
* **frontier models**ï¼šå‰æ²¿æ¨¡å‹ï¼ˆæŒ‡æœ€å…ˆè¿›çš„ AI æ¨¡å‹ï¼‰
* **ask**ï¼šè¿™é‡Œä½œåè¯ï¼Œè¯·æ±‚ã€è¦æ±‚

---

(8) [2:14-2:43] **Yet if we give it a more complicated task like the Olong tasks, you start to see this idea of context rot and how RLMs are able to sort of ignore that problem. So the olong test to put it simply is when they give the AI system a data set and then they ask the AI to find all the combinations of the entries inside that data set that meet certain criteria. The Olong pairs test then essentially ramps up the complexity of that task.**

ä½†å¦‚æœæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ **Olong** ä»»åŠ¡ï¼Œä½ å°±ä¼šå¼€å§‹çœ‹åˆ° **context rot** è¿™ä¸ªæ¦‚å¿µï¼Œä»¥åŠ **RLM** æ˜¯å¦‚ä½•å¿½ç•¥è¿™ä¸ªé—®é¢˜çš„ã€‚ç®€å•æ¥è¯´ï¼Œ**Olong** æµ‹è¯•æ˜¯ç»™ AI ç³»ç»Ÿä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶åè¦æ±‚ AI æ‰¾å‡ºæ•°æ®é›†ä¸­æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„æ‰€æœ‰æ¡ç›®ç»„åˆã€‚**Olong pairs** æµ‹è¯•åˆ™è¿›ä¸€æ­¥æå‡äº†ä»»åŠ¡çš„å¤æ‚åº¦ã€‚

è§£æï¼š
* **sort of**ï¼šå‰¯è¯çŸ­è¯­ï¼Œæœ‰ç‚¹ã€æŸç§ç¨‹åº¦ä¸Š
* **to put it simply** ğŸ”¥ï¼šç®€å•æ¥è¯´ï¼ˆéå¸¸å®ç”¨çš„è¿‡æ¸¡çŸ­è¯­ï¼‰
* **combination** /ËŒkÉ’mbÉªËˆneÉªÊƒn/ï¼šåè¯ï¼Œç»„åˆ
* **entry** /Ëˆentri/ï¼šåè¯ï¼Œæ¡ç›®ã€é¡¹
* **criteria** /kraÉªËˆtÉªÉ™riÉ™/ï¼šåè¯ï¼Œæ ‡å‡†ã€æ¡ä»¶ï¼ˆcriterion çš„å¤æ•°ï¼‰
* **ramp up** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼ŒåŠ é€Ÿã€æå‡

---

(9) [2:43-3:07] **And so you can see with this sort of test context rot at work as we increase the input context length here on the bottom axes the effectiveness the score drops dramatically. Right? And it stops exactly at the context window length. And here it shows 262. Now, if we look at the RLM, first of all, notice we don't stop at 262. This pushes all the way to 1 million.**

ä½ å¯ä»¥çœ‹åˆ°åœ¨è¿™ç§æµ‹è¯•ä¸­ **context rot** æ˜¯å¦‚ä½•èµ·ä½œç”¨çš„â€”â€”å½“æˆ‘ä»¬å¢åŠ åº•éƒ¨åæ ‡è½´ä¸Šçš„è¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦æ—¶ï¼Œæ•ˆæœåˆ†æ•°æ€¥å‰§ä¸‹é™ã€‚å¯¹å§ï¼Ÿè€Œä¸”å®ƒæ­£å¥½åœ¨ä¸Šä¸‹æ–‡çª—å£é•¿åº¦å¤„åœæ­¢ï¼Œè¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ 262ï¼ˆåƒï¼‰ã€‚ç°åœ¨å¦‚æœæˆ‘ä»¬çœ‹ **RLM**ï¼Œé¦–å…ˆæ³¨æ„æˆ‘ä»¬ä¸ä¼šåœ¨ 262 å¤„åœæ­¢ï¼Œè€Œæ˜¯ä¸€ç›´æ¨è¿›åˆ° 100 ä¸‡ã€‚

è§£æï¼š
* **at work**ï¼šçŸ­è¯­ï¼Œåœ¨èµ·ä½œç”¨ã€åœ¨è¿ä½œ
* **axes** /ËˆÃ¦ksiËz/ï¼šåè¯ï¼Œè½´ï¼ˆaxis çš„å¤æ•°ï¼‰
* **dramatically** /drÉ™ËˆmÃ¦tÉªkli/ï¼šå‰¯è¯ï¼Œæ€¥å‰§åœ°ã€æ˜¾è‘—åœ°
* **push all the way to** ğŸ”¥ï¼šä¸€ç›´æ¨è¿›åˆ°

---

(10) [3:07-3:31] **So, we can handle a much larger data set. Secondly, notice the score. The score doesn't drop nearly as much. And at a certain point, it almost sort of levels off. As we go from 1 million to 262, it's virtually the same. And if you look at olong pairs, right, we're sitting at about what 50 here on RLM. We're at zero over here on the left. That is a wild difference.**

æ‰€ä»¥æˆ‘ä»¬èƒ½å¤„ç†å¤§å¾—å¤šçš„æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæ³¨æ„åˆ†æ•°ã€‚åˆ†æ•°çš„ä¸‹é™å¹…åº¦è¿œæ²¡æœ‰é‚£ä¹ˆå¤§ã€‚è€Œä¸”åœ¨æŸä¸ªç‚¹ï¼Œå®ƒå‡ ä¹è¶‹äºå¹³ç¨³ã€‚ä» 100 ä¸‡åˆ° 262ï¼ˆåƒï¼‰ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„ã€‚å¦‚æœä½ çœ‹ **Olong pairs**ï¼Œ**RLM** è¿™è¾¹å¤§çº¦æ˜¯ 50 åˆ†ï¼Œå·¦è¾¹åŸºç¡€æ¨¡å‹æ˜¯é›¶åˆ†ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„å·®è·ã€‚

è§£æï¼š
* **nearly as much**ï¼šè¿œæ²¡æœ‰é‚£ä¹ˆå¤š
* **level off** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œè¶‹äºå¹³ç¨³ã€ç¨³å®šä¸‹æ¥
* **virtually** /ËˆvÉœËtÊƒuÉ™li/ï¼šå‰¯è¯ï¼Œå‡ ä¹ã€å®é™…ä¸Š
* **wild difference** ğŸ”¥ï¼šå·¨å¤§çš„å·®å¼‚ï¼ˆwild åœ¨å£è¯­ä¸­è¡¨ç¤º"æƒŠäººçš„"ï¼‰

---

(11) [3:31-4:08] **And let's break down these numbers a little bit more. Here's some more results of four different tests. So it shows the Qwen 3 coder and GPT5. They ran the study on both models, but for today's discussion, we're just going to focus on GPT5. So I have the base model highlighted and the RLM highlighted. Same two models you saw before. And the two tests I want to bring to your attention are the browse comp and olong pairs. So we already saw olong pairs before. And the interesting thing about olong pairs is the task length aka the data set is pretty small 32k. Yet RLM crushes the base model 58 versus 04.**

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°åˆ†è§£è¿™äº›æ•°å­—ã€‚è¿™æ˜¯å››ä¸ªä¸åŒæµ‹è¯•çš„æ›´å¤šç»“æœã€‚å®ƒæ˜¾ç¤ºäº† **Qwen 3 coder** å’Œ **GPT5**ã€‚ä»–ä»¬åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸Šéƒ½è¿›è¡Œäº†ç ”ç©¶ï¼Œä½†ä»Šå¤©çš„è®¨è®ºæˆ‘ä»¬åªå…³æ³¨ **GPT5**ã€‚æˆ‘æ ‡æ³¨äº†åŸºç¡€æ¨¡å‹å’Œ **RLM**ï¼Œå°±æ˜¯ä½ ä¹‹å‰çœ‹åˆ°çš„é‚£ä¸¤ä¸ªæ¨¡å‹ã€‚æˆ‘æƒ³è®©ä½ æ³¨æ„çš„ä¸¤ä¸ªæµ‹è¯•æ˜¯ **browse comp** å’Œ **Olong pairs**ã€‚æˆ‘ä»¬ä¹‹å‰å·²ç»çœ‹è¿‡ **Olong pairs** äº†ã€‚æœ‰è¶£çš„æ˜¯ **Olong pairs** çš„ä»»åŠ¡é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯æ•°æ®é›†ç›¸å½“å°ï¼Œåªæœ‰ 32kã€‚ä½† **RLM** ç¢¾å‹äº†åŸºç¡€æ¨¡å‹ï¼Œ58 å¯¹ 04ã€‚

è§£æï¼š
* **break down**ï¼šåŠ¨è¯çŸ­è¯­ï¼Œåˆ†è§£ã€è¯¦ç»†åˆ†æ
* **highlight** /ËˆhaÉªlaÉªt/ï¼šåŠ¨è¯ï¼Œæ ‡æ³¨ã€çªå‡ºæ˜¾ç¤º
* **bring to your attention** ğŸ”¥ï¼šè®©ä½ æ³¨æ„åˆ°ï¼ˆæ­£å¼ä½†å¸¸ç”¨çš„è¡¨è¾¾ï¼‰
* **aka** /ËŒeÉªkeÉªËˆeÉª/ï¼šalso known as çš„ç¼©å†™ï¼Œåˆåã€ä¹Ÿå°±æ˜¯
* **versus** /ËˆvÉœËsÉ™s/ï¼šä»‹è¯ï¼Œå¯¹æ¯”ã€ç›¸å¯¹äº

---

(12) [4:08-4:18] **Which is very interesting because like I talked about in the beginning the RLM was supposed to deal with huge context window length issues right yet when we look at a smaller task it's still way more effective which is very interesting.**

è¿™éå¸¸æœ‰è¶£ï¼Œå› ä¸ºæ­£å¦‚æˆ‘å¼€å¤´è¯´çš„ï¼Œ**RLM** æœ¬æ¥æ˜¯ç”¨æ¥å¤„ç†å·¨å¤§çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦é—®é¢˜çš„ï¼Œå¯¹å§ï¼Ÿä½†å½“æˆ‘ä»¬çœ‹ä¸€ä¸ªè¾ƒå°çš„ä»»åŠ¡æ—¶ï¼Œå®ƒä»ç„¶æœ‰æ•ˆå¾—å¤šï¼Œè¿™éå¸¸æœ‰è¶£ã€‚

è§£æï¼š
* **be supposed to**ï¼šåº”è¯¥ã€æœ¬æ¥æ˜¯ç”¨æ¥
* **way more** ğŸ”¥ï¼šå£è¯­åŒ–è¡¨è¾¾ï¼Œè¿œè¿œæ›´...ï¼ˆæ¯” much more æ›´å£è¯­ï¼‰

---

(13) [4:19-4:52] **Now, if we look at browse comp, we're on the other end of the spectrum and we're dealing with a massive task length, right? 11 million tokens, right? You know, 20 what 40 times the context length essentially, right? Quick math there. The base model can't even deal with it. And of note, what this test is, it's kind of like needle in the haystack except instead of one document, it's 100 documents and it also needs to not only find things inside those documents, but sort of synthesize it to get the answer.**

ç°åœ¨å¦‚æœæˆ‘ä»¬çœ‹ **browse comp**ï¼Œæˆ‘ä»¬å¤„äºå¦ä¸€ä¸ªæç«¯ï¼Œåœ¨å¤„ç†ä¸€ä¸ªå·¨å¤§çš„ä»»åŠ¡é•¿åº¦ï¼Œå¯¹å§ï¼Ÿ1100 ä¸‡ tokensï¼Œå¤§çº¦æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦çš„ 20 åˆ° 40 å€ï¼Œå¿«é€Ÿå¿ƒç®—ä¸€ä¸‹ã€‚åŸºç¡€æ¨¡å‹æ ¹æœ¬æ— æ³•å¤„ç†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªæµ‹è¯•æœ‰ç‚¹åƒ"å¤§æµ·æé’ˆ"ï¼Œåªä¸è¿‡ä¸æ˜¯ä¸€ä¸ªæ–‡æ¡£è€Œæ˜¯ 100 ä¸ªæ–‡æ¡£ï¼Œè€Œä¸”ä¸ä»…éœ€è¦åœ¨è¿™äº›æ–‡æ¡£ä¸­æ‰¾åˆ°ä¸œè¥¿ï¼Œè¿˜éœ€è¦ç»¼åˆä¿¡æ¯æ¥å¾—å‡ºç­”æ¡ˆã€‚

è§£æï¼š
* **on the other end of the spectrum** ğŸ”¥ï¼šåœ¨å¦ä¸€ä¸ªæç«¯ï¼ˆéå¸¸å®ç”¨çš„è¡¨è¾¾ï¼‰
* **massive** /ËˆmÃ¦sÉªv/ï¼šå½¢å®¹è¯ï¼Œå·¨å¤§çš„
* **of note**ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯
* **synthesize** /ËˆsÉªnÎ¸É™saÉªz/ï¼šåŠ¨è¯ï¼Œç»¼åˆã€åˆæˆ

---

(14) [4:46-5:08] **So, the base model can't handle it, right? Can't handle it at all. Yet the RLM scored 91. So what do we see here from the results? Right? We see a system that can not only handle huge tasks but perform more effectively at very complicated tasks at small token lengths, right? Crazy stuff. So this then begs the question, what the heck are RLMs and how do they work?**

æ‰€ä»¥åŸºç¡€æ¨¡å‹æ ¹æœ¬å¤„ç†ä¸äº†ï¼Œå¯¹å§ï¼Ÿå®Œå…¨å¤„ç†ä¸äº†ã€‚ä½† **RLM** å¾—åˆ† 91ã€‚é‚£ä¹ˆæˆ‘ä»¬ä»è¿™äº›ç»“æœä¸­çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªç³»ç»Ÿä¸ä»…èƒ½å¤„ç†å·¨å¤§çš„ä»»åŠ¡ï¼Œè€Œä¸”åœ¨å° token é•¿åº¦çš„å¤æ‚ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å¾—æ›´æœ‰æ•ˆï¼Œå¯¹å§ï¼Ÿå¤ªç–¯ç‹‚äº†ã€‚è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼š**RLM** åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

è§£æï¼š
* **beg the question** ğŸ”¥ï¼šå¼•å‡ºé—®é¢˜ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯å£è¯­ç”¨æ³•ï¼Œåœ¨æ­£å¼é€»è¾‘å­¦ä¸­æœ‰ä¸åŒå«ä¹‰ï¼‰
* **what the heck** ğŸ”¥ï¼šåˆ°åº•ï¼ˆå£è¯­åŒ–ï¼Œæ¯” what æ›´å¼ºè°ƒæƒŠè®¶æˆ–å›°æƒ‘ï¼‰

---

(15) [5:08-5:28] **So this is the RLM. It's pretty self-explanatory. So I'll just leave this on the screen for like 15 seconds and then we'll just continue on. I'm just kidding. This is actually very complicated and convoluted at first, right?**

è¿™å°±æ˜¯ **RLM**ã€‚å®ƒéå¸¸ä¸€ç›®äº†ç„¶ï¼Œæ‰€ä»¥æˆ‘å°±æŠŠå®ƒæ”¾åœ¨å±å¹•ä¸Šå¤§æ¦‚ 15 ç§’ï¼Œç„¶åæˆ‘ä»¬ç»§ç»­ã€‚å¼€ç©ç¬‘çš„ã€‚å®é™…ä¸Šè¿™ä¸œè¥¿ä¸€å¼€å§‹éå¸¸å¤æ‚å’Œä»¤äººè´¹è§£ï¼Œå¯¹å§ï¼Ÿ

è§£æï¼š
* **self-explanatory** /ËŒselfÉªkËˆsplÃ¦nÉ™tri/ï¼šå½¢å®¹è¯ï¼Œä¸è¨€è‡ªæ˜çš„ã€ä¸€ç›®äº†ç„¶çš„
* **convoluted** /ËˆkÉ’nvÉ™luËtÉªd/ï¼šå½¢å®¹è¯ï¼Œå¤æ‚è´¹è§£çš„ã€è¿‚å›çš„
* **I'm just kidding**ï¼šå¼€ç©ç¬‘çš„ï¼ˆå¸¸ç”¨å£è¯­ï¼‰

---

(16) [5:26-6:09] **The way they describe it is right here. Right? Again, very very self-explanatory. So, it's a recursive language model treats prompts as part of the environment. It loads the input prompt as a variable inside a Python REPL environment and writes code to peek into, decompose, and invoke itself recursively over programmatic snippets of the variable. Duh. Easy, right? Light work. But actually simpler than it sounds. And let me explain it. And as I walk through this explanation, let me caution you. You're going to be very confused at first. Yet by the end, you're just going to be like, "Oh, duh." Obviously, and I guarantee you there'll be like 20 people in the comments like, "This is how I already do it."**

ä»–ä»¬çš„æè¿°åœ¨è¿™é‡Œã€‚å†æ¬¡è¯´æ˜ï¼Œéå¸¸ä¸€ç›®äº†ç„¶ã€‚é€’å½’è¯­è¨€æ¨¡å‹æŠŠæç¤ºè¯å½“ä½œç¯å¢ƒçš„ä¸€éƒ¨åˆ†ã€‚å®ƒåœ¨ **Python REPL** ç¯å¢ƒä¸­æŠŠè¾“å…¥æç¤ºè¯ä½œä¸ºä¸€ä¸ªå˜é‡åŠ è½½ï¼Œç„¶åç¼–å†™ä»£ç æ¥æŸ¥çœ‹ã€åˆ†è§£ï¼Œå¹¶åœ¨å˜é‡çš„ç¨‹åºç‰‡æ®µä¸Šé€’å½’è°ƒç”¨è‡ªå·±ã€‚ç®€å•å§ï¼Ÿå°èœä¸€ç¢Ÿã€‚ä½†å®é™…ä¸Šæ¯”å¬èµ·æ¥ç®€å•ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹ã€‚åœ¨æˆ‘è®²è§£çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘è¦æé†’ä½ ï¼Œä½ ä¸€å¼€å§‹ä¼šéå¸¸å›°æƒ‘ã€‚ä½†åˆ°æœ€åï¼Œä½ ä¼šæç„¶å¤§æ‚Ÿï¼š"å“¦ï¼ŒåŸæ¥å¦‚æ­¤ï¼"æˆ‘ä¿è¯è¯„è®ºåŒºä¼šæœ‰ 20 ä¸ªäººè¯´ï¼š"æˆ‘æ—©å°±æ˜¯è¿™ä¹ˆåšçš„äº†ã€‚"

è§£æï¼š
* **REPL** /ËˆrepÉ™l/ï¼šRead-Eval-Print Loop çš„ç¼©å†™ï¼Œè¯»å–-æ±‚å€¼-æ‰“å°å¾ªç¯ï¼ˆç¼–ç¨‹æœ¯è¯­ï¼‰
* **peek into**ï¼šåŠ¨è¯çŸ­è¯­ï¼Œçª¥è§†ã€æŸ¥çœ‹
* **decompose** /ËŒdiËkÉ™mËˆpÉ™ÊŠz/ï¼šåŠ¨è¯ï¼Œåˆ†è§£
* **invoke** /ÉªnËˆvÉ™ÊŠk/ï¼šåŠ¨è¯ï¼Œè°ƒç”¨ï¼ˆç¼–ç¨‹æœ¯è¯­ï¼‰
* **snippet** /ËˆsnÉªpÉªt/ï¼šåè¯ï¼Œç‰‡æ®µã€ä»£ç ç‰‡æ®µ
* **Duh** ğŸ”¥ï¼šæ„Ÿå¹è¯ï¼Œè¡¨ç¤º"è¿™ä¸æ˜¯æ˜¾è€Œæ˜“è§å—"ï¼ˆè®½åˆºç”¨æ³•ï¼‰
* **light work** ğŸ”¥ï¼šå°èœä¸€ç¢Ÿã€è½»æ¾æå®š
* **caution** /ËˆkÉ”ËÊƒn/ï¼šåŠ¨è¯ï¼Œæé†’ã€è­¦å‘Š

---

(17) [6:09-6:31] **So you'll understand by the end. So first things first, they're doing this inside of a Python environment. What does that mean? For all intents and purposes, they're doing the study on a computer that's running Python and GPT5 can interact with Python and it can have Python run code for it. Okay, so in this example, we have GPT5 over here, 272 context. It's our primary large language model. We want to have GPT5 answer some questions about our very large document.**

æ‰€ä»¥ä½ æœ€ç»ˆä¼šç†è§£çš„ã€‚é¦–å…ˆï¼Œä»–ä»¬æ˜¯åœ¨ **Python** ç¯å¢ƒä¸­åšè¿™äº›çš„ã€‚è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå®é™…ä¸Šï¼Œä»–ä»¬æ˜¯åœ¨ä¸€å°è¿è¡Œ **Python** çš„ç”µè„‘ä¸Šè¿›è¡Œç ”ç©¶ï¼Œ**GPT5** å¯ä»¥ä¸ **Python** äº¤äº’ï¼Œå¯ä»¥è®© **Python** ä¸ºå®ƒè¿è¡Œä»£ç ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¿™è¾¹æœ‰ **GPT5**ï¼Œ272ï¼ˆåƒï¼‰ä¸Šä¸‹æ–‡ï¼Œæ˜¯æˆ‘ä»¬çš„ä¸»è¦å¤§è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æƒ³è®© **GPT5** å›ç­”ä¸€äº›å…³äºæˆ‘ä»¬è¶…å¤§æ–‡æ¡£çš„é—®é¢˜ã€‚

è§£æï¼š
* **first things first** ğŸ”¥ï¼šé¦–å…ˆã€é¦–è¦çš„äº‹æƒ…å…ˆåšï¼ˆå¸¸ç”¨è¡¨è¾¾ï¼‰
* **for all intents and purposes** ğŸ”¥ï¼šå®é™…ä¸Šã€ä»å„æ–¹é¢æ¥è¯´
* **interact with**ï¼šä¸...äº¤äº’

---

(18) [6:35-7:38] **In this case, we're going to say we wanted to answer questions about War and Peace, a huge novel that we are going to say is 1 million tokens. Obviously, it's probably in the training data. Assume it's not in the training data. And I can't, right? I can't just take this million token document and shove it into ChatGPT. I can't just drop it into the chat window. That won't work. So, how do I actually get accurate answers from this document? This is where the RLM system comes in. So, as they stated, we are going to programmatically figure out what's in that document first. What does that mean? That means our system, our GPT5 is going to have Python run some code to give us some information about this document. And by information, I mean it's going to run some code for us. So essentially what the model is going to do in this RLM system is it's going to get sort of like a reconnaissance of the huge document. It's going to see what the lay of the land is and how it can sort of piecemeal this.**

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå‡è®¾æˆ‘ä»¬æƒ³å›ç­”å…³äºã€Šæˆ˜äº‰ä¸å’Œå¹³ã€‹çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€éƒ¨å·¨å¤§çš„å°è¯´ï¼Œæˆ‘ä»¬å‡è®¾å®ƒæœ‰ 100 ä¸‡ tokensã€‚æ˜¾ç„¶å®ƒå¯èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼Œä½†å‡è®¾å®ƒä¸åœ¨ã€‚æˆ‘ä¸èƒ½ç›´æ¥æŠŠè¿™ä¸ª 100 ä¸‡ tokens çš„æ–‡æ¡£å¡è¿› **ChatGPT**ï¼Œä¸èƒ½ç›´æ¥ä¸¢è¿›èŠå¤©çª—å£ï¼Œé‚£è¡Œä¸é€šã€‚é‚£æˆ‘æ€ä¹ˆæ‰èƒ½ä»è¿™ä¸ªæ–‡æ¡£ä¸­å¾—åˆ°å‡†ç¡®çš„ç­”æ¡ˆå‘¢ï¼Ÿè¿™å°±æ˜¯ **RLM** ç³»ç»Ÿå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚æ­£å¦‚ä»–ä»¬æ‰€è¯´ï¼Œæˆ‘ä»¬è¦é¦–å…ˆé€šè¿‡ç¼–ç¨‹æ–¹å¼å¼„æ¸…æ¥šè¿™ä¸ªæ–‡æ¡£é‡Œæœ‰ä»€ä¹ˆã€‚è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæ„æ€æ˜¯æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬çš„ **GPT5** ä¼šè®© **Python** è¿è¡Œä¸€äº›ä»£ç æ¥ç»™æˆ‘ä»¬ä¸€äº›å…³äºè¿™ä¸ªæ–‡æ¡£çš„ä¿¡æ¯ã€‚æœ¬è´¨ä¸Šï¼Œåœ¨è¿™ä¸ª **RLM** ç³»ç»Ÿä¸­ï¼Œæ¨¡å‹è¦åšçš„æ˜¯å¯¹è¿™ä¸ªå·¨å¤§æ–‡æ¡£è¿›è¡Œä¸€æ¬¡ä¾¦å¯Ÿï¼Œçœ‹çœ‹æ•´ä½“æƒ…å†µæ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä»¥åŠå¦‚ä½•æŠŠå®ƒæ‹†åˆ†å¤„ç†ã€‚

è§£æï¼š
* **shove** /ÊƒÊŒv/ï¼šåŠ¨è¯ï¼Œå¡ã€æ¨ï¼ˆå£è¯­åŒ–ï¼‰
* **programmatically** /ËŒprÉ™ÊŠÉ¡rÉ™ËˆmÃ¦tÉªkli/ï¼šå‰¯è¯ï¼Œä»¥ç¼–ç¨‹æ–¹å¼
* **figure out** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œå¼„æ¸…æ¥šã€ææ˜ç™½
* **reconnaissance** /rÉªËˆkÉ’nÉªsÉ™ns/ï¼šåè¯ï¼Œä¾¦å¯Ÿï¼ˆåŸä¸ºå†›äº‹æœ¯è¯­ï¼‰
* **the lay of the land** ğŸ”¥ï¼šä¹ è¯­ï¼Œå½¢åŠ¿ã€æ•´ä½“æƒ…å†µ
* **piecemeal** /ËˆpiËsmiËl/ï¼šå‰¯è¯/åŠ¨è¯ï¼Œé€æ­¥åœ°ã€ä¸€ç‚¹ä¸€ç‚¹åœ°å¤„ç†

---

(19) [7:39-8:06] **So in our case, we're saying, "Hey, we want our main prompt is I want you to break down every interaction involving Napoleon in War and Peace." So our system's just going to run some Python and get some information. In this case, what it wants to know is what's the length of this book and can we sort of break it down into component parts? That is literally what you see in blue. That's one line of code. So seven tokens have been used.**

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¯´ï¼š"å˜¿ï¼Œæˆ‘ä»¬çš„ä¸»è¦æç¤ºè¯æ˜¯è®©ä½ åˆ†è§£ã€Šæˆ˜äº‰ä¸å’Œå¹³ã€‹ä¸­æ¶‰åŠæ‹¿ç ´ä»‘çš„æ¯ä¸€æ¬¡äº’åŠ¨ã€‚"æ‰€ä»¥æˆ‘ä»¬çš„ç³»ç»Ÿåªéœ€è¦è¿è¡Œä¸€äº› **Python** ä»£ç æ¥è·å–ä¿¡æ¯ã€‚åœ¨è¿™ä¸ªæƒ…å†µä¸‹ï¼Œå®ƒæƒ³çŸ¥é“çš„æ˜¯è¿™æœ¬ä¹¦æœ‰å¤šé•¿ï¼Œä»¥åŠæˆ‘ä»¬èƒ½å¦æŠŠå®ƒåˆ†è§£æˆç»„æˆéƒ¨åˆ†ã€‚è¿™å°±æ˜¯ä½ çœ‹åˆ°çš„è“è‰²ä»£ç ï¼Œå°±ä¸€è¡Œä»£ç ï¼Œåªç”¨äº† 7 ä¸ª tokensã€‚

è§£æï¼š
* **break down**ï¼šåŠ¨è¯çŸ­è¯­ï¼Œåˆ†è§£ã€æ‹†åˆ†
* **interaction** /ËŒÉªntÉ™rËˆÃ¦kÊƒn/ï¼šåè¯ï¼Œäº’åŠ¨ã€äº¤äº’
* **involving** /ÉªnËˆvÉ’lvÉªÅ‹/ï¼šä»‹è¯ï¼Œæ¶‰åŠ
* **component parts**ï¼šç»„æˆéƒ¨åˆ†

---

(20) [8:06-8:28] **And what we get back from Python isn't the whole document. We just get the information we need. So, it's going to tell us, hey, you're dealing with 2.4 million characters. And by the way, you wanted the first thousand words in the document. Well, here they are. So, it realizes, here's how big it is. Here's the first like couple pages. Okay, it has chapters. Well, let's break it down into chapters and then figure out which chapters include Napoleon in there.**

æˆ‘ä»¬ä» **Python** å¾—åˆ°çš„ä¸æ˜¯æ•´ä¸ªæ–‡æ¡£ï¼Œæˆ‘ä»¬åªå¾—åˆ°éœ€è¦çš„ä¿¡æ¯ã€‚å®ƒä¼šå‘Šè¯‰æˆ‘ä»¬ï¼šå˜¿ï¼Œä½ åœ¨å¤„ç† 240 ä¸‡ä¸ªå­—ç¬¦ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œä½ æƒ³è¦æ–‡æ¡£çš„å‰ä¸€åƒä¸ªå•è¯ï¼Œç»™ä½ ã€‚æ‰€ä»¥å®ƒæ„è¯†åˆ°ï¼Œè¿™æ˜¯æ–‡æ¡£çš„å¤§å°ï¼Œè¿™æ˜¯å‰å‡ é¡µã€‚å¥½çš„ï¼Œå®ƒæœ‰ç« èŠ‚ã€‚é‚£è®©æˆ‘ä»¬æŒ‰ç« èŠ‚åˆ†è§£ï¼Œç„¶åæ‰¾å‡ºå“ªäº›ç« èŠ‚åŒ…å«æ‹¿ç ´ä»‘ã€‚

è§£æï¼š
* **deal with**ï¼šå¤„ç†ã€åº”å¯¹
* **by the way** ğŸ”¥ï¼šé¡ºä¾¿è¯´ä¸€ä¸‹ï¼ˆéå¸¸å¸¸ç”¨çš„å£è¯­è¡¨è¾¾ï¼‰
* **a couple of**ï¼šå‡ ä¸ªï¼ˆå£è¯­ä¸­å¸¸çœç•¥ ofï¼‰
* **figure out**ï¼šå¼„æ¸…æ¥š

---

(21) [8:28-8:55] **Okay, it's doing this just through Python. That's just a couple lines of code. And Python is going to bring back that information. And that information is what you see here, right? It's going to say, "Hey, in that book I found Napoleon in chapters 3, 7, 12, 15, 18, 24, blah blah blah blah blah." So when we say, "Hey, these RLMs interact programmatically with the document." That's what we're saying. So GPT5 created the code, Python ran the code, and now it got that information back about Napoleon's in these chapters.**

å®ƒåªæ˜¯é€šè¿‡ **Python** æ¥åšè¿™äº›ï¼Œå°±å‡ è¡Œä»£ç ã€‚**Python** ä¼šæŠŠä¿¡æ¯å¸¦å›æ¥ï¼Œè¿™äº›ä¿¡æ¯å°±æ˜¯ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ã€‚å®ƒä¼šè¯´ï¼š"å˜¿ï¼Œåœ¨é‚£æœ¬ä¹¦é‡Œæˆ‘åœ¨ç¬¬ 3ã€7ã€12ã€15ã€18ã€24 ç« ç­‰ç­‰æ‰¾åˆ°äº†æ‹¿ç ´ä»‘ã€‚"æ‰€ä»¥å½“æˆ‘ä»¬è¯´"è¿™äº› **RLM** ä»¥ç¼–ç¨‹æ–¹å¼ä¸æ–‡æ¡£äº¤äº’"ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„æ„æ€ã€‚**GPT5** åˆ›å»ºäº†ä»£ç ï¼Œ**Python** è¿è¡Œäº†ä»£ç ï¼Œç°åœ¨å®ƒå¾—åˆ°äº†å…³äºæ‹¿ç ´ä»‘åœ¨å“ªäº›ç« èŠ‚çš„ä¿¡æ¯ã€‚

è§£æï¼š
* **bring back**ï¼šå¸¦å›æ¥
* **blah blah blah** ğŸ”¥ï¼šç­‰ç­‰ç­‰ç­‰ï¼ˆå£è¯­ä¸­è¡¨ç¤ºçœç•¥åç»­å†…å®¹ï¼‰
* **interact with**ï¼šä¸...äº¤äº’

---

(22) [8:55-9:29] **Now what happens, right? Imagine your GPT5, you know, it's in that book, but I can't take all that information in. I can't even take in that chapter by chapter. That's still too much. So what do we do? Well, this is where the recursive part of the recursive language model comes in. And that's just a fancy way of saying our large language model is going to spawn little sub-agents, right? You see GPT5 mini, that's the recursive section, right? It's just going to do tool calls to smaller versions of itself to handle all this chapter information.**

ç°åœ¨ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæƒ³è±¡ä½ æ˜¯ **GPT5**ï¼Œä½ çŸ¥é“ä¿¡æ¯åœ¨é‚£æœ¬ä¹¦é‡Œï¼Œä½†æˆ‘æ— æ³•å¸æ”¶æ‰€æœ‰ä¿¡æ¯ï¼Œæˆ‘ç”šè‡³æ— æ³•é€ç« èŠ‚åœ°å¸æ”¶ï¼Œé‚£è¿˜æ˜¯å¤ªå¤šäº†ã€‚é‚£æˆ‘ä»¬æ€ä¹ˆåŠï¼Ÿè¿™å°±æ˜¯é€’å½’è¯­è¨€æ¨¡å‹ä¸­"é€’å½’"éƒ¨åˆ†å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¿™åªæ˜¯ä¸€ç§èŠ±å“¨çš„è¯´æ³•ï¼Œæ„æ€æ˜¯æˆ‘ä»¬çš„å¤§è¯­è¨€æ¨¡å‹ä¼šç”Ÿæˆå°çš„å­ä»£ç†ã€‚ä½ çœ‹ **GPT5 mini**ï¼Œé‚£å°±æ˜¯é€’å½’éƒ¨åˆ†ã€‚å®ƒåªæ˜¯å¯¹è‡ªå·±çš„å°ç‰ˆæœ¬è¿›è¡Œå·¥å…·è°ƒç”¨æ¥å¤„ç†æ‰€æœ‰è¿™äº›ç« èŠ‚ä¿¡æ¯ã€‚

è§£æï¼š
* **take in**ï¼šå¸æ”¶ã€æ¥æ”¶
* **chapter by chapter**ï¼šé€ç« èŠ‚åœ°
* **a fancy way of saying** ğŸ”¥ï¼šä¸€ç§èŠ±å“¨çš„è¯´æ³•ï¼ˆè¡¨ç¤º"è¯´ç™½äº†å°±æ˜¯..."ï¼‰
* **spawn** /spÉ”Ën/ï¼šåŠ¨è¯ï¼Œç”Ÿæˆã€äº§ç”Ÿ
* **sub-agent**ï¼šå­ä»£ç†
* **tool call**ï¼šå·¥å…·è°ƒç”¨ï¼ˆAI æœ¯è¯­ï¼‰

---

(23) [9:29-10:10] **So I need to know about Napoleon in chapter 3. Yet I can't put all of chapter 3 and 7 and 12 into here. So instead, we're going to take chapter 3 and we're going to give it to GPT5 mini down here, right, as a tool call. So GPT5 Mini is now looking at all chapter 3. It gets the answer. Napoleon did this in chapter 3 and it sends that answer back. We then repeat this process over and over and over. So I need chapter 7. Okay, GPT5 mini number two, you're up. Here's chapter 7. Figure out what the answer is. Send it back to me. And it does it over and over again and you eventually get your answer.**

æˆ‘éœ€è¦çŸ¥é“ç¬¬ 3 ç« ä¸­å…³äºæ‹¿ç ´ä»‘çš„å†…å®¹ï¼Œä½†æˆ‘æ— æ³•æŠŠç¬¬ 3ã€7ã€12 ç« å…¨éƒ¨æ”¾è¿›æ¥ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æŠŠç¬¬ 3 ç« æ‹¿å‡ºæ¥ï¼Œä½œä¸ºå·¥å…·è°ƒç”¨ä¼ ç»™ä¸‹é¢çš„ **GPT5 mini**ã€‚æ‰€ä»¥ **GPT5 mini** ç°åœ¨åœ¨çœ‹æ•´ä¸ªç¬¬ 3 ç« ï¼Œå®ƒå¾—åˆ°ç­”æ¡ˆï¼šæ‹¿ç ´ä»‘åœ¨ç¬¬ 3 ç« åšäº†è¿™äº›ï¼Œç„¶åæŠŠç­”æ¡ˆå‘å›æ¥ã€‚æˆ‘ä»¬ä¸€éåˆä¸€éåœ°é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚æˆ‘éœ€è¦ç¬¬ 7 ç« ã€‚å¥½çš„ï¼Œ**GPT5 mini** äºŒå·ï¼Œè½®åˆ°ä½ äº†ã€‚è¿™æ˜¯ç¬¬ 7 ç« ï¼Œæ‰¾å‡ºç­”æ¡ˆï¼Œå‘å›ç»™æˆ‘ã€‚å®ƒä¸€éåˆä¸€éåœ°è¿™æ ·åšï¼Œæœ€ç»ˆä½ å¾—åˆ°äº†ç­”æ¡ˆã€‚

è§£æï¼š
* **instead**ï¼šå‰¯è¯ï¼Œç›¸åã€å–è€Œä»£ä¹‹
* **you're up** ğŸ”¥ï¼šè½®åˆ°ä½ äº†ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰
* **over and over (again)** ğŸ”¥ï¼šä¸€éåˆä¸€éåœ°
* **eventually** /ÉªËˆventÊƒuÉ™li/ï¼šå‰¯è¯ï¼Œæœ€ç»ˆ

---

(24) [10:10-10:43] **And so what we've done, right, and I said this actually isn't that complicated. All we've done is we've taken this giant document and we're just chunking it up essentially, right? We're smartly chunking it up because of this code we ran at the beginning, but we're just chunking it up and we're giving each chunk to essentially a sub-large language model that we've used as a tool call. That's it. That's recursive language models. We're just offloading the context to some little mini agent that we're using as a tool call. That's it. That's RLMs in a nutshell.**

æ‰€ä»¥æˆ‘ä»¬åšäº†ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘è¯´è¿‡è¿™å®é™…ä¸Šå¹¶ä¸å¤æ‚ã€‚æˆ‘ä»¬æ‰€åšçš„å°±æ˜¯æŠŠè¿™ä¸ªå·¨å¤§çš„æ–‡æ¡£åˆ†å—ï¼Œå¯¹å§ï¼Ÿå› ä¸ºæˆ‘ä»¬ä¸€å¼€å§‹è¿è¡Œçš„ä»£ç ï¼Œæˆ‘ä»¬æ˜¯æ™ºèƒ½åœ°åˆ†å—ï¼Œä½†æœ¬è´¨ä¸Šå°±æ˜¯åˆ†å—ï¼Œç„¶åæŠŠæ¯ä¸ªå—äº¤ç»™ä¸€ä¸ªå­å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬æŠŠå®ƒä½œä¸ºå·¥å…·è°ƒç”¨æ¥ä½¿ç”¨ã€‚å°±æ˜¯è¿™æ ·ã€‚è¿™å°±æ˜¯é€’å½’è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬åªæ˜¯æŠŠä¸Šä¸‹æ–‡å¸è½½ç»™ä¸€äº›å°çš„è¿·ä½ ä»£ç†ï¼Œä½œä¸ºå·¥å…·è°ƒç”¨æ¥ä½¿ç”¨ã€‚å°±è¿™æ ·ã€‚è¿™å°±æ˜¯ **RLM** çš„ç®€è¦æ¦‚è¿°ã€‚

è§£æï¼š
* **chunk up** ğŸ”¥ï¼šåŠ¨è¯çŸ­è¯­ï¼Œåˆ†å—ã€åˆ‡å—
* **smartly**ï¼šå‰¯è¯ï¼Œæ™ºèƒ½åœ°ã€èªæ˜åœ°
* **offload** /ËŒÉ’fËˆlÉ™ÊŠd/ï¼šåŠ¨è¯ï¼Œå¸è½½ã€è½¬ç§»ï¼ˆè´Ÿæ‹…ï¼‰
* **in a nutshell** ğŸ”¥ï¼šç®€è€Œè¨€ä¹‹ã€æ¦‚æ‹¬åœ°è¯´ï¼ˆéå¸¸å®ç”¨çš„è¡¨è¾¾ï¼‰

---

(25) [10:43-11:02] **Okay. So if you've ever used a sub-agent in Claude Code to do something on your behalf because you didn't want all that context to go in the main context window, you've kind of been using some of the fundamentals of these RLM systems, right? That's all it is.**

å¦‚æœä½ æ›¾ç»åœ¨ **Claude Code** ä¸­ä½¿ç”¨è¿‡å­ä»£ç†æ¥ä»£æ›¿ä½ åšä¸€äº›äº‹æƒ…ï¼Œå› ä¸ºä½ ä¸æƒ³è®©æ‰€æœ‰ä¸Šä¸‹æ–‡éƒ½è¿›å…¥ä¸»ä¸Šä¸‹æ–‡çª—å£ï¼Œé‚£ä½ å®é™…ä¸Šå°±å·²ç»åœ¨ä½¿ç”¨è¿™äº› **RLM** ç³»ç»Ÿçš„ä¸€äº›åŸºæœ¬åŸç†äº†ï¼Œå¯¹å§ï¼Ÿå°±æ˜¯è¿™ä¹ˆå›äº‹ã€‚

è§£æï¼š
* **on your behalf** ğŸ”¥ï¼šä»£è¡¨ä½ ã€æ›¿ä½ ï¼ˆæ­£å¼ä½†å¸¸ç”¨çš„è¡¨è¾¾ï¼‰
* **fundamentals** /ËŒfÊŒndÉ™Ëˆmentlz/ï¼šåè¯ï¼ŒåŸºæœ¬åŸç†ã€åŸºç¡€

---

(26) [11:02-11:30] **And this is just one recursion layer deep. Okay? And they only did one recursion layer deep in the study. And they talked about in theory they would like to see people go deeper. And what do I mean by that? Well, imagine, hey, remember I said we have chapter 3. Well, let's say I gave chapter three to this guy. What if chapter 3 is too big? Well, if chapter 3 is too big, well, we could just repeat this process with this guy where it does like, you know, act one of chapter 3, act two, act three, and that becomes another mini, right?**

è¿™åªæ˜¯ä¸€å±‚é€’å½’æ·±åº¦ã€‚ä»–ä»¬åœ¨ç ”ç©¶ä¸­åªåšäº†ä¸€å±‚é€’å½’æ·±åº¦ï¼Œä»–ä»¬æåˆ°ç†è®ºä¸Šä»–ä»¬æƒ³çœ‹åˆ°äººä»¬åšå¾—æ›´æ·±ã€‚æˆ‘è¿™æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œè®°å¾—æˆ‘è¯´çš„ç¬¬ 3 ç« å—ï¼Ÿå‡è®¾æˆ‘æŠŠç¬¬ 3 ç« ç»™äº†è¿™ä¸ªå®¶ä¼™ã€‚å¦‚æœç¬¬ 3 ç« å¤ªå¤§æ€ä¹ˆåŠï¼Ÿå¦‚æœç¬¬ 3 ç« å¤ªå¤§ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªå®¶ä¼™é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œè®©å®ƒå¤„ç†ç¬¬ 3 ç« çš„ç¬¬ä¸€å¹•ã€ç¬¬äºŒå¹•ã€ç¬¬ä¸‰å¹•ï¼Œç„¶åé‚£åˆå˜æˆå¦ä¸€ä¸ªè¿·ä½ ç‰ˆæœ¬ï¼Œå¯¹å§ï¼Ÿ

è§£æï¼š
* **recursion layer**ï¼šé€’å½’å±‚
* **go deeper**ï¼šæ›´æ·±å…¥
* **act**ï¼šåè¯ï¼Œå¹•ï¼ˆæˆå‰§æœ¯è¯­ï¼Œè¿™é‡ŒæŒ‡ç« èŠ‚çš„å­éƒ¨åˆ†ï¼‰

---

(27) [11:28-11:53] **And it just repeats the same process. It's just LLMs all the way down. So, all that to say is that is how the recursive language model works. We have a big document. We have our large language model. We use code to figure out how we can sort of break down this huge document and then we spawn miniature versions of our large language model, GPT5 mini in the case of the study, to handle those individual sections that give us the answer.**

å®ƒåªæ˜¯é‡å¤åŒæ ·çš„è¿‡ç¨‹ã€‚ä¸€ç›´æ˜¯ **LLM** å¥—å¨ƒã€‚æ€»ä¹‹ï¼Œè¿™å°±æ˜¯é€’å½’è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§æ–‡æ¡£ï¼Œæˆ‘ä»¬æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ç”¨ä»£ç æ¥å¼„æ¸…æ¥šå¦‚ä½•åˆ†è§£è¿™ä¸ªå·¨å¤§çš„æ–‡æ¡£ï¼Œç„¶åæˆ‘ä»¬ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹çš„è¿·ä½ ç‰ˆæœ¬â€”â€”åœ¨è¿™ä¸ªç ”ç©¶ä¸­æ˜¯ **GPT5 mini**â€”â€”æ¥å¤„ç†é‚£äº›ç»™æˆ‘ä»¬ç­”æ¡ˆçš„å„ä¸ªéƒ¨åˆ†ã€‚

è§£æï¼š
* **all the way down** ğŸ”¥ï¼šä¸€ç›´å¾€ä¸‹ã€å½»åº•åœ°ï¼ˆè¿™é‡ŒæŒ‡"å¥—å¨ƒ"ï¼‰
* **all that to say** ğŸ”¥ï¼šæ€»ä¹‹ã€ç®€è¨€ä¹‹
* **miniature** /ËˆmÉªnÉ™tÊƒÉ™/ï¼šå½¢å®¹è¯ï¼Œå¾®å‹çš„ã€è¿·ä½ çš„

---

(28) [11:53-12:15] **It gets aggregated and that way our guy over here is able to handle these huge prompts without ever actually ingesting them, right? Because he's kind of a liar and he just has everyone else do his work for him and then claims he did it at the end. That's RLMs.**

ç­”æ¡ˆè¢«èšåˆèµ·æ¥ï¼Œè¿™æ ·æˆ‘ä»¬è¿™è¾¹çš„å®¶ä¼™å°±èƒ½å¤„ç†è¿™äº›å·¨å¤§çš„æç¤ºè¯ï¼Œè€Œå®é™…ä¸Šä»æœªçœŸæ­£å¸æ”¶å®ƒä»¬ã€‚å› ä¸ºä»–æœ‰ç‚¹åƒä¸ªéª—å­ï¼Œè®©åˆ«äººæ›¿ä»–å¹²æ´»ï¼Œæœ€åå´å£°ç§°æ˜¯ä»–è‡ªå·±åšçš„ã€‚è¿™å°±æ˜¯ **RLM**ã€‚

è§£æï¼š
* **aggregate** /ËˆÃ¦É¡rÉªÉ¡eÉªt/ï¼šåŠ¨è¯ï¼Œèšåˆã€æ±‡æ€»
* **ingest** /ÉªnËˆdÊ’est/ï¼šåŠ¨è¯ï¼Œæ‘„å–ã€å¸æ”¶ï¼ˆè¿™é‡ŒæŒ‡å¤„ç†æ•°æ®ï¼‰
* **liar** /ËˆlaÉªÉ™/ï¼šåè¯ï¼Œéª—å­
* **claim** /kleÉªm/ï¼šåŠ¨è¯ï¼Œå£°ç§°

---

(29) [12:12-12:43] **And that is essentially what this picture and these next few paragraphs are explaining. That's all it is. Which is why I guarantee you someone in the comments like I already do that. Yeah. Okay. But for the rest of you, hopefully that kind of made sense. And hopefully that sort of explained why it's able to do these things so well because it never has to deal with context rot because we just keep splitting the context up so many times amongst so many, you know, call them sub-agents, call it whatever you want.**

è¿™åŸºæœ¬ä¸Šå°±æ˜¯è¿™å¼ å›¾å’Œæ¥ä¸‹æ¥å‡ æ®µåœ¨è§£é‡Šçš„å†…å®¹ã€‚å°±è¿™ä¹ˆç®€å•ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä¿è¯è¯„è®ºåŒºä¼šæœ‰äººè¯´"æˆ‘æ—©å°±è¿™ä¹ˆåšäº†"ã€‚æ˜¯å•Šï¼Œå¥½å§ã€‚ä½†å¯¹äºå…¶ä»–äººï¼Œå¸Œæœ›è¿™è®²å¾—æœ‰ç‚¹é“ç†ã€‚ä¹Ÿå¸Œæœ›è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå®ƒèƒ½æŠŠè¿™äº›äº‹æƒ…åšå¾—è¿™ä¹ˆå¥½â€”â€”å› ä¸ºå®ƒæ°¸è¿œä¸éœ€è¦å¤„ç†ä¸Šä¸‹æ–‡è…çƒ‚ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æ–­åœ°æŠŠä¸Šä¸‹æ–‡æ‹†åˆ†æˆå¾ˆå¤šä»½ç»™å¾ˆå¤šä¸ªï¼Œä½ çŸ¥é“çš„ï¼Œå«å®ƒä»¬å­ä»£ç†ä¹Ÿå¥½ï¼Œå«ä»€ä¹ˆéƒ½è¡Œã€‚

è§£æï¼š
* **make sense** ğŸ”¥ï¼šæœ‰é“ç†ã€è®²å¾—é€š
* **deal with**ï¼šå¤„ç†ã€åº”å¯¹
* **split up**ï¼šåˆ†å‰²ã€æ‹†åˆ†
* **amongst**ï¼šä»‹è¯ï¼Œåœ¨...ä¹‹ä¸­ï¼ˆ= amongï¼‰

---

(30) [12:43-13:04] **Right, we've essentially spread this giant 1 million context document so thin that everyone can work on it effectively. And the way they describe that concept in here is they say the long prompts aka the large documents should not be fed into the neural network. Aka don't just dump them into the large language model, but should instead be treated as part of the environment that the large language model can symbolically interact with.**

æˆ‘ä»¬å®é™…ä¸ŠæŠŠè¿™ä¸ª 100 ä¸‡ä¸Šä¸‹æ–‡çš„å·¨å¤§æ–‡æ¡£åˆ†æ‘Šå¾—å¦‚æ­¤ä¹‹è–„ï¼Œä»¥è‡³äºæ¯ä¸ªäººéƒ½èƒ½æœ‰æ•ˆåœ°å¤„ç†å®ƒã€‚ä»–ä»¬åœ¨è¿™é‡Œæè¿°è¿™ä¸ªæ¦‚å¿µçš„æ–¹å¼æ˜¯ï¼šé•¿æç¤ºè¯ï¼Œä¹Ÿå°±æ˜¯å¤§æ–‡æ¡£ï¼Œä¸åº”è¯¥ç›´æ¥å–‚ç»™ç¥ç»ç½‘ç»œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸è¦ç›´æ¥æŠŠå®ƒä»¬å€¾å€’è¿›å¤§è¯­è¨€æ¨¡å‹ï¼Œè€Œåº”è¯¥æŠŠå®ƒä»¬å½“ä½œç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹å¯ä»¥ç¬¦å·åŒ–åœ°ä¸ä¹‹äº¤äº’ã€‚

è§£æï¼š
* **spread thin** ğŸ”¥ï¼šåˆ†æ‘Šå¾—å¾ˆè–„ã€ç¨€é‡Š
* **feed into**ï¼šå–‚ç»™ã€è¾“å…¥åˆ°
* **neural network**ï¼šç¥ç»ç½‘ç»œ
* **dump** /dÊŒmp/ï¼šåŠ¨è¯ï¼Œå€¾å€’ã€ä¸¢å¼ƒ
* **symbolically** /sÉªmËˆbÉ’lÉªkli/ï¼šå‰¯è¯ï¼Œç¬¦å·åŒ–åœ°

---

(31) [13:09-13:36] **Right? When we say it's part of the environment, it's just something it knows exists, right? It can interact with this thing. It just doesn't pull it all in. That's what this paper is saying. And the paper goes on to make these observations that essentially reinforce everything I just told you, namely that they can scale to 10 million token regimes and outperform these base LMs. They also talk about the recursive sub-calling of RLMs providing strong benefits on information-dense inputs.**

å½“æˆ‘ä»¬è¯´å®ƒæ˜¯ç¯å¢ƒçš„ä¸€éƒ¨åˆ†æ—¶ï¼Œå®ƒåªæ˜¯çŸ¥é“è¿™ä¸œè¥¿å­˜åœ¨ï¼Œå¯¹å§ï¼Ÿå®ƒå¯ä»¥ä¸è¿™ä¸œè¥¿äº¤äº’ï¼Œåªæ˜¯ä¸ä¼šæŠŠå®ƒå…¨éƒ¨æ‹‰è¿›æ¥ã€‚è¿™å°±æ˜¯è¿™ç¯‡è®ºæ–‡åœ¨è¯´çš„ã€‚è®ºæ–‡ç»§ç»­åšå‡ºäº†è¿™äº›è§‚å¯Ÿï¼ŒåŸºæœ¬ä¸Šå¼ºåŒ–äº†æˆ‘åˆšæ‰å‘Šè¯‰ä½ çš„ä¸€åˆ‡ï¼Œå³å®ƒä»¬å¯ä»¥æ‰©å±•åˆ° 1000 ä¸‡ tokens çš„èŒƒå›´å¹¶è¶…è¶Šè¿™äº›åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ã€‚ä»–ä»¬è¿˜è°ˆåˆ° **RLM** çš„é€’å½’å­è°ƒç”¨åœ¨ä¿¡æ¯å¯†é›†å‹è¾“å…¥ä¸Šæä¾›äº†å¼ºå¤§çš„å¥½å¤„ã€‚

è§£æï¼š
* **pull in**ï¼šæ‹‰è¿›æ¥ã€å¸æ”¶
* **observation** /ËŒÉ’bzÉ™ËˆveÉªÊƒn/ï¼šåè¯ï¼Œè§‚å¯Ÿã€å‘ç°
* **reinforce** /ËŒriËÉªnËˆfÉ”Ës/ï¼šåŠ¨è¯ï¼Œå¼ºåŒ–ã€åŠ å¼º
* **namely**ï¼šå‰¯è¯ï¼Œå³ã€ä¹Ÿå°±æ˜¯
* **regime** /reÉªËˆÊ’iËm/ï¼šåè¯ï¼ŒèŒƒå›´ã€ä½“åˆ¶
* **outperform** /ËŒaÊŠtpÉ™ËˆfÉ”Ëm/ï¼šåŠ¨è¯ï¼Œè¶…è¶Šã€è¡¨ç°ä¼˜äº
* **information-dense**ï¼šä¿¡æ¯å¯†é›†å‹çš„

---

(32) [13:32-14:02] **And I want to take a second to talk about this because I think at this point you're like, okay, this is awesome. Like how do I actually recreate this? How can I actually take these fundamentals? I think the idea of sub-calling right really in context of something like Claude Code like sub-agents, I think is really where you can see something like this work. Again, observation three: the RLM not only scales better on large context stuff but it also performs better on the small context stuff if it's very complicated, right, this olong stuff.**

æˆ‘æƒ³èŠ±ä¸€ç§’é’Ÿè°ˆè°ˆè¿™ä¸ªï¼Œå› ä¸ºæˆ‘è§‰å¾—åˆ°è¿™ä¸ªç‚¹ä¸Šä½ ä¼šæƒ³ï¼Œå¥½çš„ï¼Œè¿™å¤ªæ£’äº†ï¼Œæˆ‘æ€ä¹ˆå®é™…é‡ç°è¿™ä¸ªï¼Ÿæˆ‘æ€ä¹ˆå®é™…è¿ç”¨è¿™äº›åŸºæœ¬åŸç†ï¼Ÿæˆ‘è®¤ä¸ºå­è°ƒç”¨è¿™ä¸ªæƒ³æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ **Claude Code** çš„å­ä»£ç†è¿™æ ·çš„åœºæ™¯ä¸‹ï¼ŒçœŸçš„æ˜¯ä½ èƒ½çœ‹åˆ°è¿™ç±»ä¸œè¥¿å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚å†çœ‹è§‚å¯Ÿä¸‰ï¼š**RLM** ä¸ä»…åœ¨å¤§ä¸Šä¸‹æ–‡å†…å®¹ä¸Šæ‰©å±•å¾—æ›´å¥½ï¼Œè€Œä¸”åœ¨éå¸¸å¤æ‚çš„å°ä¸Šä¸‹æ–‡å†…å®¹ä¸Šä¹Ÿè¡¨ç°æ›´å¥½ï¼Œå°±æ˜¯è¿™ä¸ª **Olong** æµ‹è¯•ã€‚

è§£æï¼š
* **take a second**ï¼šèŠ±ä¸€ç‚¹æ—¶é—´
* **recreate** /ËŒriËkriËˆeÉªt/ï¼šåŠ¨è¯ï¼Œé‡ç°ã€å†åˆ›é€ 
* **scale** /skeÉªl/ï¼šåŠ¨è¯ï¼Œæ‰©å±•ã€ç¼©æ”¾

---

(33) [14:00-14:45] **And again this paper just goes on and on about this. There's also some interesting stuff that it tends to actually be cheaper too. And really I think this paper and this whole context window stuff is just where this whole space is going and you're seeing it all over the place. Things like the Ralph loop, things like GSD, it's all about context window management and how that changes your outputs. And right here, what we're seeing is we're seeing just sort of some of the code stuff that I talked about earlier. So, I will put a link to this paper inside the description. Definitely check it out. I think the stuff is just really interesting. Obviously, this field is changing by the day, but definitely read it, take a look at it. Wild stuff and really start to think about how you can integrate these sort of ideas into your own workflows.**

è¿™ç¯‡è®ºæ–‡ç»§ç»­è¯¦ç»†è®¨è®ºè¿™äº›å†…å®¹ã€‚è¿˜æœ‰ä¸€äº›æœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œå®ƒå®é™…ä¸Šå¾€å¾€æ›´ä¾¿å®œã€‚æˆ‘çœŸçš„è®¤ä¸ºè¿™ç¯‡è®ºæ–‡å’Œæ•´ä¸ªä¸Šä¸‹æ–‡çª—å£çš„å†…å®¹æ­£æ˜¯è¿™ä¸ªé¢†åŸŸçš„å‘å±•æ–¹å‘ï¼Œä½ åˆ°å¤„éƒ½èƒ½çœ‹åˆ°ã€‚åƒ **Ralph loop**ã€åƒ **GSD**ï¼Œéƒ½æ˜¯å…³äºä¸Šä¸‹æ–‡çª—å£ç®¡ç†ä»¥åŠå®ƒå¦‚ä½•æ”¹å˜ä½ çš„è¾“å‡ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°çš„å°±æ˜¯æˆ‘ä¹‹å‰è°ˆåˆ°çš„ä¸€äº›ä»£ç ç›¸å…³çš„å†…å®¹ã€‚æˆ‘ä¼šåœ¨æè¿°ä¸­æ”¾ä¸Šè¿™ç¯‡è®ºæ–‡çš„é“¾æ¥ï¼Œä¸€å®šè¦çœ‹çœ‹ã€‚æˆ‘è§‰å¾—è¿™äº›ä¸œè¥¿çœŸçš„å¾ˆæœ‰è¶£ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªé¢†åŸŸæ¯å¤©éƒ½åœ¨å˜åŒ–ï¼Œä½†ä¸€å®šè¦è¯»ä¸€è¯»ã€çœ‹ä¸€çœ‹ã€‚è¿™æ˜¯ç–¯ç‹‚çš„ä¸œè¥¿ï¼ŒçœŸçš„è¦å¼€å§‹æ€è€ƒå¦‚ä½•æŠŠè¿™äº›æƒ³æ³•æ•´åˆåˆ°ä½ è‡ªå·±çš„å·¥ä½œæµç¨‹ä¸­ã€‚

è§£æï¼š
* **go on and on**ï¼šä¸æ–­åœ°è®²ã€è¯¦ç»†è®¨è®º
* **tend to**ï¼šå€¾å‘äº
* **all over the place** ğŸ”¥ï¼šåˆ°å¤„ã€å„å¤„
* **by the day** ğŸ”¥ï¼šæ¯å¤©ã€æ—¥ç›Š
* **check it out** ğŸ”¥ï¼šçœ‹çœ‹ã€äº†è§£ä¸€ä¸‹ï¼ˆå£è¯­åŒ–ï¼‰
* **integrate** /ËˆÉªntÉªÉ¡reÉªt/ï¼šåŠ¨è¯ï¼Œæ•´åˆã€èå…¥
* **workflow** /ËˆwÉœËkflÉ™ÊŠ/ï¼šåè¯ï¼Œå·¥ä½œæµç¨‹

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ®µè§†é¢‘å®Œæ•´è®²è§£äº† **MIT** çš„ **RLM**ï¼ˆé€’å½’è¯­è¨€æ¨¡å‹ï¼‰çš„å·¥ä½œåŸç†ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå½“é¢å¯¹è¶…å¤§æ–‡æ¡£æ—¶ï¼Œä¸»æ¨¡å‹ **GPT5** ä¸ç›´æ¥å¸æ”¶å…¨éƒ¨å†…å®¹ï¼Œè€Œæ˜¯é€šè¿‡ **Python** ä»£ç å…ˆ"ä¾¦å¯Ÿ"æ–‡æ¡£ç»“æ„ï¼Œç„¶åå°†æ–‡æ¡£æ™ºèƒ½åœ°åˆ†å—ï¼Œæ¯ä¸ªå—äº¤ç»™ä¸€ä¸ªå­ä»£ç†ï¼ˆ**GPT5 mini**ï¼‰å¤„ç†ï¼Œæœ€åæ±‡æ€»ç»“æœã€‚è¿™å°±åƒ"è®©åˆ«äººå¹²æ´»ï¼Œè‡ªå·±é¢†åŠŸ"ä¸€æ ·ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤„ç†è¶…è¿‡ 1000 ä¸‡ tokens çš„æ•°æ®é›†ï¼Œè¿˜èƒ½åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œç”šè‡³æˆæœ¬æ›´ä½ã€‚ä½œè€…å»ºè®®å¤§å®¶å¯ä»¥å‚è€ƒè¿™ç§æ€è·¯ï¼Œåœ¨ **Claude Code** ç­‰å·¥å…·ä¸­ä½¿ç”¨å­ä»£ç†æ¥ç®¡ç†ä¸Šä¸‹æ–‡ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **context rot** | ä¸Šä¸‹æ–‡è…çƒ‚ï¼Œéšä¸Šä¸‹æ–‡å˜é•¿æ€§èƒ½ä¸‹é™ |
| **context window** | ä¸Šä¸‹æ–‡çª—å£ï¼Œæ¨¡å‹èƒ½å¤„ç†çš„æ–‡æœ¬é•¿åº¦ |
| **recursive** | é€’å½’çš„ |
| **deep dive** | æ·±å…¥ç ”ç©¶ |
| **needle in the haystack** | å¤§æµ·æé’ˆ |
| **crush it** | ç¢¾å‹ã€è½»æ¾æå®š |
| **ramp up** | åŠ é€Ÿã€æå‡ |
| **level off** | è¶‹äºå¹³ç¨³ |
| **beg the question** | å¼•å‡ºé—®é¢˜ |
| **light work** | å°èœä¸€ç¢Ÿ |
| **for all intents and purposes** | å®é™…ä¸Š |
| **the lay of the land** | æ•´ä½“æƒ…å†µã€å½¢åŠ¿ |
| **piecemeal** | é€æ­¥åœ°å¤„ç† |
| **chunk up** | åˆ†å—ã€åˆ‡å— |
| **spawn** | ç”Ÿæˆã€äº§ç”Ÿ |
| **sub-agent** | å­ä»£ç† |
| **tool call** | å·¥å…·è°ƒç”¨ |
| **offload** | å¸è½½ã€è½¬ç§»è´Ÿæ‹… |
| **in a nutshell** | ç®€è€Œè¨€ä¹‹ |
| **on your behalf** | ä»£æ›¿ä½ ã€ä»£è¡¨ä½  |
| **a fancy way of saying** | ä¸€ç§èŠ±å“¨çš„è¯´æ³• |
| **all the way down** | ä¸€ç›´å¾€ä¸‹ï¼ˆå¥—å¨ƒï¼‰ |
| **spread thin** | åˆ†æ‘Šå¾—å¾ˆè–„ |
| **by the day** | æ¯å¤©ã€æ—¥ç›Š |
| **check it out** | çœ‹çœ‹ã€äº†è§£ä¸€ä¸‹ |
| **workflow** | å·¥ä½œæµç¨‹ |
