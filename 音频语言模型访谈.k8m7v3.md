# ğŸ¯ éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸Moshi è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **25 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:00-0:25]

**{Neil, it's awesome [to have you].} {I'm super excited [to chat today].} {Yeah, thanks [for having me].} {Amazing.} {Neil is [the CEO and co-founder] (of Gradium) and also [the co-founder] (of Qoutai), [which is the creator of [some amazing language models] (across the board)].} {Also the inventor of [the specific models] [that takes (a different approach) (from diffusion)] and is more (around audio language models) (end to end).} {And they've done [some really incredible work] [both (in academic literature) as well as (deployed models) (with Gradium)].}**

Neilï¼Œå¾ˆé«˜å…´ä½ èƒ½æ¥ï¼æˆ‘ä»Šå¤©è¶…å…´å¥‹èƒ½è·Ÿä½ èŠã€‚â€”â€”è°¢è°¢ä½ ä»¬é‚€è¯·æˆ‘ã€‚â€”â€”å¤ªå¥½äº†ã€‚Neil æ˜¯ **Gradium** çš„ CEO å’Œè”åˆåˆ›å§‹äººï¼ŒåŒæ—¶ä¹Ÿæ˜¯ **Qoutai** çš„è”åˆåˆ›å§‹äººã€‚**Qoutai** åˆ›é€ äº†ä¸€ç³»åˆ—å‡ºè‰²çš„è¯­è¨€æ¨¡å‹ï¼ŒNeil æœ¬äººä¹Ÿæ˜¯ä¸€äº›ç‹¬ç‰¹æ¨¡å‹çš„å‘æ˜è€…â€”â€”è¿™äº›æ¨¡å‹é‡‡ç”¨äº†ä¸æ‰©æ•£æ¨¡å‹ä¸åŒçš„æ–¹æ³•ï¼Œæ›´å¤šæ˜¯ç«¯åˆ°ç«¯çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚ä»–ä»¬åœ¨å­¦æœ¯è®ºæ–‡å’Œé€šè¿‡ **Gradium** éƒ¨ç½²çš„æ¨¡å‹ä¸¤æ–¹é¢éƒ½åšå‡ºäº†éå¸¸å‡ºè‰²çš„å·¥ä½œã€‚

è§£æï¼š
* **co-founder** /ËŒkoÊŠËˆfaÊŠndÉ™r/ï¼šè”åˆåˆ›å§‹äºº
* **across the board** ğŸ”¥ï¼šå…¨é¢åœ°ã€å„æ–¹é¢
* **diffusion**ï¼šæ‰©æ•£ï¼ˆæŒ‡æ‰©æ•£æ¨¡å‹ï¼Œä¸€ç§ç”Ÿæˆå¼AIæŠ€æœ¯ï¼‰
* **end to end**ï¼šç«¯åˆ°ç«¯ï¼ˆç³»ç»Ÿä»è¾“å…¥åˆ°è¾“å‡ºä¸€ä½“åŒ–å¤„ç†ï¼‰
* **academic literature**ï¼šå­¦æœ¯æ–‡çŒ®/è®ºæ–‡
* **deployed models**ï¼šå·²éƒ¨ç½²çš„æ¨¡å‹ï¼ˆæŠ•å…¥å®é™…ä½¿ç”¨çš„æ¨¡å‹ï¼‰

---

(2) [0:25-0:50]

**{And so (super excited) [to hear more] [both about [the kind of novel approach] [that they're taking (at the technical level)] as well as [the novel approach] [that they're taking (from a startup level)] (of being both a nonprofit and building out these models also for application use)].} {Neil, thanks so much [for joining].} {Thanks.} {Thank you.} {I would love [to start off] with like [what is Gradium]?} {What is Qout?} {How do they â€” what is [the history of them] and [how did they come to be]?}**

æ‰€ä»¥éå¸¸æœŸå¾…èƒ½æ›´å¤šäº†è§£ä»–ä»¬åœ¨æŠ€æœ¯å±‚é¢é‡‡å–çš„åˆ›æ–°æ–¹æ³•ï¼Œä»¥åŠåœ¨åˆ›ä¸šå±‚é¢çš„åˆ›æ–°â€”â€”æ—¢æ˜¯éè¥åˆ©ç»„ç»‡ï¼ŒåŒæ—¶åˆæ„å»ºç”¨äºå®é™…åº”ç”¨çš„æ¨¡å‹ã€‚Neilï¼Œéå¸¸æ„Ÿè°¢ä½ åŠ å…¥ã€‚â€”â€”è°¢è°¢ã€‚â€”â€”æˆ‘æƒ³å…ˆä»è¿™ä¸ªé—®é¢˜å¼€å§‹ï¼š**Gradium** æ˜¯ä»€ä¹ˆï¼Ÿ**Qout** æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬çš„å†å²æ˜¯ä»€ä¹ˆï¼Œæ˜¯æ€ä¹ˆè¯ç”Ÿçš„ï¼Ÿ

è§£æï¼š
* **novel approach** ğŸ”¥ï¼šæ–°é¢–çš„æ–¹æ³•/åˆ›æ–°æ–¹æ³•
* **nonprofit**ï¼šéè¥åˆ©ç»„ç»‡
* **building out**ï¼šæ„å»ºã€æ‹“å±•
* **come to be**ï¼šå½¢æˆã€è¯ç”Ÿï¼ˆhow did they come to be = å®ƒä»¬æ˜¯æ€ä¹ˆè¯ç”Ÿçš„ï¼‰

---

(3) [0:50-1:16]

**{Yeah, so the history starts with Qoutai.} {So we created Qoutai [as a research lab] (in late 2023) and (back then) me and my co-founders (of Qout and Gradium) we were either (at Google or Meta) doing research [on generative models] (around audio) but also (around text and vision) and we wanted [to recreate a new lab] [that was (kind of) like [the early days of (Facebook Research) and (Google Brain) and (Google DeepMind)]] [that we knew].}**

å†å²è¦ä» **Qoutai** è¯´èµ·ã€‚æˆ‘ä»¬åœ¨ 2023 å¹´åº•åˆ›å»ºäº† **Qoutai** ä½œä¸ºä¸€ä¸ªç ”ç©¶å®éªŒå®¤ã€‚é‚£æ—¶å€™ï¼Œæˆ‘å’Œ **Qout** åŠ **Gradium** çš„è”åˆåˆ›å§‹äººä»¬åˆ†åˆ«åœ¨ **Google** æˆ– **Meta** åšå…³äºéŸ³é¢‘ä»¥åŠæ–‡æœ¬å’Œè§†è§‰çš„ç”Ÿæˆå¼æ¨¡å‹ç ”ç©¶ã€‚æˆ‘ä»¬æƒ³è¦é‡æ–°åˆ›å»ºä¸€ä¸ªå®éªŒå®¤ï¼Œé‚£ç§ç±»ä¼¼æˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„æ—©æœŸ **Facebook Research**ã€**Google Brain** å’Œ **Google DeepMind** çš„æ„Ÿè§‰ã€‚

è§£æï¼š
* **research lab**ï¼šç ”ç©¶å®éªŒå®¤
* **generative models**ï¼šç”Ÿæˆå¼æ¨¡å‹
* **recreate**ï¼šé‡æ–°åˆ›å»º
* **the early days of** ğŸ”¥ï¼šâ€¦â€¦çš„æ—©æœŸé˜¶æ®µ

---

(4) [1:16-1:40]

**{And so we had the choice of [either making a startup or making a nonprofit].} {And (you know) our experience was [that the most appropriate environment [to make breakthrough innovation] is a research environment].} {You can even look at LLMs.} {It was invented (at Google) but it was invented (at Google Brain) and not (in Google product divisions) [because (you know) only a researcher and a research team can take (enough risks) [to rethink everything (from first principles)]].}**

æ‰€ä»¥æˆ‘ä»¬é¢ä¸´é€‰æ‹©ï¼šè¦ä¹ˆåˆ›å»ºä¸€å®¶åˆåˆ›å…¬å¸ï¼Œè¦ä¹ˆåˆ›å»ºä¸€ä¸ªéè¥åˆ©ç»„ç»‡ã€‚æˆ‘ä»¬çš„ç»éªŒå‘Šè¯‰æˆ‘ä»¬ï¼Œåšå‡ºçªç ´æ€§åˆ›æ–°æœ€åˆé€‚çš„ç¯å¢ƒæ˜¯ç ”ç©¶ç¯å¢ƒã€‚çœ‹çœ‹å¤§è¯­è¨€æ¨¡å‹å°±çŸ¥é“äº†â€”â€”å®ƒæ˜¯åœ¨ **Google** å‘æ˜çš„ï¼Œä½†å…·ä½“æ˜¯åœ¨ **Google Brain** è€Œä¸æ˜¯ **Google** çš„äº§å“éƒ¨é—¨ï¼Œå› ä¸ºåªæœ‰ç ”ç©¶äººå‘˜å’Œç ”ç©¶å›¢é˜Ÿæ‰èƒ½æ‰¿æ‹…è¶³å¤Ÿçš„é£é™©ï¼Œä»ç¬¬ä¸€æ€§åŸç†å‡ºå‘é‡æ–°æ€è€ƒä¸€åˆ‡ã€‚

è§£æï¼š
* **breakthrough innovation** ğŸ”¥ï¼šçªç ´æ€§åˆ›æ–°
* **appropriate**ï¼šåˆé€‚çš„ã€æ°å½“çš„
* **first principles** ğŸ”¥ï¼šç¬¬ä¸€æ€§åŸç†ï¼ˆä»æœ€åŸºæœ¬å‡è®¾å‡ºå‘æ€è€ƒé—®é¢˜çš„æ–¹æ³•ï¼‰
* **product divisions**ï¼šäº§å“éƒ¨é—¨

---

(5) [1:40-2:05]

**{So since we really wanted [to make (significant) contribution], we wanted [to create an environment] [that will favor that] and the nonprofit was perfect (for that).} {And so we did (you know) two years of [open science publications], training PhD students and so on.} {It has a lot of success.} {Our models are downloaded [millions of times] (every month).} {I'll go into [the details of [what models I'm talking about]] later.}**

å› ä¸ºæˆ‘ä»¬çœŸçš„æƒ³åšå‡ºé‡å¤§è´¡çŒ®ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³åˆ›é€ ä¸€ä¸ªæœ‰åˆ©äºæ­¤çš„ç¯å¢ƒï¼Œè€Œéè¥åˆ©ç»„ç»‡å¯¹æ­¤æ˜¯å®Œç¾çš„é€‰æ‹©ã€‚äºæ˜¯æˆ‘ä»¬åšäº†ä¸¤å¹´çš„å¼€æ”¾ç§‘å­¦å‡ºç‰ˆç‰©ï¼ŒåŸ¹å…»åšå£«ç”Ÿç­‰ç­‰ã€‚æˆæœéå¸¸ä¸°å¯Œâ€”â€”æˆ‘ä»¬çš„æ¨¡å‹æ¯æœˆè¢«ä¸‹è½½æ•°ç™¾ä¸‡æ¬¡ã€‚æˆ‘ç¨åä¼šè¯¦ç»†ä»‹ç»æˆ‘è¯´çš„æ˜¯å“ªäº›æ¨¡å‹ã€‚

è§£æï¼š
* **significant contribution**ï¼šé‡å¤§è´¡çŒ®
* **favor**ï¼šæœ‰åˆ©äºã€åå‘
* **open science publications**ï¼šå¼€æ”¾ç§‘å­¦å‡ºç‰ˆç‰©ï¼ˆå…¬å¼€å‘è¡¨çš„ç ”ç©¶æˆæœï¼‰
* **PhD students**ï¼šåšå£«ç”Ÿ

---

(6) [2:05-2:29]

**{But what we then see is there was (a lot) also of market traction [around having strong models (for real-time voice) as a transcription, synthesis, translation and so on so forth].} {And (with our technical expertise) we could also provide such products.} {So we decided [to create Gradium].} {And Gradium and Qoutai are located [in the same offices].}**

ä½†æˆ‘ä»¬åæ¥å‘ç°ï¼Œå¸‚åœºä¸Šå¯¹å¼ºå¤§çš„å®æ—¶è¯­éŸ³æ¨¡å‹æœ‰å¾ˆå¤§çš„éœ€æ±‚â€”â€”åŒ…æ‹¬è½¬å½•ã€åˆæˆã€ç¿»è¯‘ç­‰ç­‰ã€‚å‡­å€Ÿæˆ‘ä»¬çš„æŠ€æœ¯ä¸“é•¿ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æä¾›è¿™æ ·çš„äº§å“ã€‚æ‰€ä»¥æˆ‘ä»¬å†³å®šåˆ›å»º **Gradium**ã€‚**Gradium** å’Œ **Qoutai** åœ¨åŒä¸€ä¸ªåŠå…¬å®¤é‡Œã€‚

è§£æï¼š
* **market traction** ğŸ”¥ï¼šå¸‚åœºå¸å¼•åŠ›/éœ€æ±‚
* **transcription**ï¼šè½¬å½•ï¼ˆå°†è¯­éŸ³è½¬ä¸ºæ–‡å­—ï¼‰
* **synthesis**ï¼šåˆæˆï¼ˆå¦‚è¯­éŸ³åˆæˆï¼‰
* **technical expertise**ï¼šæŠ€æœ¯ä¸“é•¿

---

(7) [2:29-2:55]

**{Qoutai is a shareholder (of Gradium).} {We have partnerships.} {So the idea is [to have [long-term fundamental research] [that is open]] and [much more concrete applied research (for products)].} {Yeah, that's awesome.} {I think the â€” how do you create [that creativity and the ability [to go in (more ambiguous) directions]] and [have that space] versus [doing (very applied) stuff]?} {Makes a lot of â€”}**

**Qoutai** æ˜¯ **Gradium** çš„è‚¡ä¸œï¼Œæˆ‘ä»¬æœ‰åˆä½œå…³ç³»ã€‚æ‰€ä»¥è¿™ä¸ªæ€è·¯å°±æ˜¯ï¼šä¸€æ–¹é¢æœ‰å¼€æ”¾çš„é•¿æœŸåŸºç¡€ç ”ç©¶ï¼Œå¦ä¸€æ–¹é¢æœ‰æ›´å…·ä½“çš„é¢å‘äº§å“çš„åº”ç”¨ç ”ç©¶ã€‚â€”â€”å¾ˆæ£’ã€‚æˆ‘è§‰å¾—å…³é”®é—®é¢˜æ˜¯ï¼Œä½ æ€æ ·æ‰èƒ½åˆ›é€ å‡ºé‚£ç§åˆ›é€ åŠ›å’Œæœæ›´æ¨¡ç³Šæ–¹å‘æ¢ç´¢çš„èƒ½åŠ›å’Œç©ºé—´ï¼Œè€Œä¸æ˜¯åªåšéå¸¸åº”ç”¨åŒ–çš„ä¸œè¥¿ï¼Ÿ

è§£æï¼š
* **shareholder**ï¼šè‚¡ä¸œ
* **fundamental research**ï¼šåŸºç¡€ç ”ç©¶
* **applied research**ï¼šåº”ç”¨ç ”ç©¶
* **ambiguous** /Ã¦mËˆbÉªÉ¡juÉ™s/ï¼šæ¨¡ç³Šçš„ã€ä¸æ˜ç¡®çš„
* **versus** ğŸ”¥ï¼šç›¸å¯¹äºã€ä¸â€¦â€¦å¯¹æ¯”

---

(8) [2:55-3:20]

**{Yeah, I can say a few words about that.} {So for example, the first project [we did (at Qoutai)] was Moshi.} {So the idea was, (you know,) (back then) the state of the art (in audio) was [text to speech] and [speech to text].} {There was no conversational AI whatsoever, right?} {It didn't even â€” you could start [building a cascaded system], but it didn't even really exist.} {And this is â€” what year â€” it was like 2023 [to have something [that works (really really) well]].}**

å—¯ï¼Œå…³äºè¿™ç‚¹æˆ‘å¯ä»¥è¯´å‡ å¥ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬åœ¨ **Qoutai** åšçš„ç¬¬ä¸€ä¸ªé¡¹ç›®å°±æ˜¯ **Moshi**ã€‚å½“æ—¶çš„æƒ…å†µæ˜¯ï¼ŒéŸ³é¢‘é¢†åŸŸæœ€å…ˆè¿›çš„æŠ€æœ¯å°±æ˜¯æ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³è½¬æ–‡æœ¬ã€‚æ ¹æœ¬æ²¡æœ‰å¯¹è¯å¼ AIâ€”â€”ä½ å¯ä»¥å¼€å§‹æ„å»ºä¸€ä¸ªçº§è”ç³»ç»Ÿï¼Œä½†å®ƒå®é™…ä¸Šå¹¶ä¸çœŸæ­£å­˜åœ¨ã€‚é‚£æ˜¯ 2023 å¹´ï¼Œè¦åšå‡ºä¸€ä¸ªçœŸæ­£å¥½ç”¨çš„ä¸œè¥¿ã€‚

è§£æï¼š
* **state of the art** ğŸ”¥ï¼šæœ€å…ˆè¿›çš„æŠ€æœ¯/æ°´å¹³
* **text to speech / speech to text**ï¼šæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆ**TTS**ï¼‰/ è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆ**STT**ï¼‰
* **conversational AI**ï¼šå¯¹è¯å¼AI
* **whatsoever**ï¼šä»»ä½•ï¼ˆå¼ºè°ƒå¦å®šï¼Œ= at allï¼‰
* **cascaded system**ï¼šçº§è”ç³»ç»Ÿï¼ˆå¤šä¸ªæ¨¡å—ä¸²è”å¤„ç†çš„ç³»ç»Ÿï¼‰

---

(9) [3:20-3:48]

**{So what we decided to do was to do something [that was (very) new] [that didn't need (too much) compute] because (you know) we were four [in the original team] â€” four people.} {We had 1,000 GPUs [which may sound like a lot] but it was already like [10 times or 20 times smaller] than (you know) a similar team (in a big tech company).} {So we could not go (like) for [the gigantic foundation reasoning model], right?}**

æ‰€ä»¥æˆ‘ä»¬å†³å®šåšä¸€äº›éå¸¸æ–°ã€åˆä¸éœ€è¦å¤ªå¤šç®—åŠ›çš„ä¸œè¥¿â€”â€”å› ä¸ºæˆ‘ä»¬æœ€åˆçš„å›¢é˜Ÿåªæœ‰ 4 ä¸ªäººã€‚æˆ‘ä»¬æœ‰ 1000 ä¸ª **GPU**ï¼Œå¬èµ·æ¥å¯èƒ½å¾ˆå¤šï¼Œä½†å·²ç»æ¯”å¤§å‹ç§‘æŠ€å…¬å¸ç±»ä¼¼å›¢é˜Ÿçš„è§„æ¨¡å°äº† 10 åˆ° 20 å€ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸å¯èƒ½å»æé‚£ç§å·¨å‹åŸºç¡€æ¨ç†æ¨¡å‹ã€‚

è§£æï¼š
* **compute**ï¼šç®—åŠ›ï¼ˆè®¡ç®—èµ„æºï¼‰
* **GPU**ï¼šå›¾å½¢å¤„ç†å™¨ï¼ˆç”¨äºAIè®­ç»ƒçš„ç¡¬ä»¶ï¼‰
* **gigantic** /dÊ’aÉªËˆÉ¡Ã¦ntÉªk/ï¼šå·¨å¤§çš„
* **foundation reasoning model**ï¼šåŸºç¡€æ¨ç†æ¨¡å‹

---

(10) [3:48-4:13]

**{And what I saw is [nobody had made (any) progress on this].} {We could have something [that would (mostly) build on our expertise] and (you know) [with smart architecture] could be trained [on a few dozens of GPUs] and [that would impress everyone] [to make this kind of real-time voice].} {So we went directly for training [a full duplex conversational model].} {So full duplex â€” it's not only [a conversational model] but [there is no turn taking whatsoever].}**

è€Œä¸”æˆ‘æ³¨æ„åˆ°ï¼Œè¿™ä¸ªæ–¹å‘ä¸Šæ²¡æœ‰äººå–å¾—ä»»ä½•è¿›å±•ã€‚æˆ‘ä»¬å¯ä»¥åšå‡ºä¸€ä¸ªä¸»è¦å»ºç«‹åœ¨æˆ‘ä»¬ä¸“ä¸šçŸ¥è¯†åŸºç¡€ä¸Šçš„ä¸œè¥¿ï¼Œé€šè¿‡å·§å¦™çš„æ¶æ„è®¾è®¡ï¼Œåªéœ€å‡ åä¸ª **GPU** å°±èƒ½è®­ç»ƒï¼Œè€Œä¸”èƒ½è®©æ‰€æœ‰äººéƒ½å°è±¡æ·±åˆ»â€”â€”åšå‡ºè¿™ç§å®æ—¶è¯­éŸ³ã€‚æ‰€ä»¥æˆ‘ä»¬ç›´æ¥å»è®­ç»ƒäº†ä¸€ä¸ªå…¨åŒå·¥å¯¹è¯æ¨¡å‹ã€‚å…¨åŒå·¥çš„æ„æ€æ˜¯â€”â€”å®ƒä¸ä»…æ˜¯ä¸€ä¸ªå¯¹è¯æ¨¡å‹ï¼Œè€Œä¸”å®Œå…¨æ²¡æœ‰è½®æ¬¡åˆ‡æ¢ã€‚

è§£æï¼š
* **build on** ğŸ”¥ï¼šå»ºç«‹åœ¨â€¦â€¦ä¹‹ä¸Š
* **expertise** /ËŒekspÉœËrËˆtiËz/ï¼šä¸“ä¸šçŸ¥è¯†/ä¸“é•¿
* **full duplex** ğŸ”¥ï¼šå…¨åŒå·¥ï¼ˆé€šä¿¡æœ¯è¯­ï¼ŒæŒ‡åŒæ–¹å¯åŒæ—¶å‘é€å’Œæ¥æ”¶ä¿¡å·ï¼‰
* **turn taking**ï¼šè½®æ¬¡åˆ‡æ¢ï¼ˆå¯¹è¯ä¸­ä¸€æ–¹è¯´å®Œå¦ä¸€æ–¹å†è¯´ï¼‰

---

(11) [4:13-4:37]

**{Like the model is listening (all the time) and speaking (all the time).} {When it doesn't speak, it still generates audio but it's silent audio.} {So it was really [bringing the world (from Alexa and Siri) to [a human-like conversation]].} {And we (in 6 months) were able [to ship that] [with a team (as I said) of (four people) (in the beginning) and (like six) (at the end)].} {So it was quite challenging but it was â€”}**

æ¨¡å‹ä¸€ç›´åœ¨å¬ï¼Œä¹Ÿä¸€ç›´åœ¨è¯´ã€‚å½“å®ƒä¸è¯´è¯çš„æ—¶å€™ï¼Œå®ƒä»ç„¶åœ¨ç”ŸæˆéŸ³é¢‘â€”â€”åªä¸è¿‡æ˜¯é™é»˜éŸ³é¢‘ã€‚æ‰€ä»¥è¿™çœŸçš„æ˜¯æŠŠä¸–ç•Œä» **Alexa** å’Œ **Siri** å¸¦åˆ°äº†ç±»äººå¯¹è¯çš„å±‚æ¬¡ã€‚æˆ‘ä»¬åœ¨ 6 ä¸ªæœˆå†…å°±äº¤ä»˜äº†è¿™ä¸ªäº§å“ï¼Œå›¢é˜Ÿä¸€å¼€å§‹æ˜¯ 4 ä¸ªäººï¼Œæœ€åå¤§æ¦‚ 6 ä¸ªäººã€‚è¿™ç¡®å®å¾ˆæœ‰æŒ‘æˆ˜æ€§â€”â€”

è§£æï¼š
* **silent audio**ï¼šé™é»˜éŸ³é¢‘ï¼ˆæ¨¡å‹è¾“å‡ºçš„ç©ºç™½/æ— å£°éŸ³é¢‘ï¼‰
* **human-like conversation**ï¼šç±»äººå¯¹è¯
* **ship** ğŸ”¥ï¼šäº¤ä»˜ã€å‘å¸ƒï¼ˆäº§å“ä¸Šçº¿ï¼Œåˆ›ä¸šåœˆé«˜é¢‘ç”¨è¯ï¼‰
* **challenging** /ËˆtÊƒÃ¦lÉªndÊ’ÉªÅ‹/ï¼šæœ‰æŒ‘æˆ˜æ€§çš„

---

(12) [4:37-5:02]

**{It is interesting â€” in voice, [a small team can make a big difference] [because since the models are (much smaller) (than large reasoning models)] it's really more about [how deep you are (into voice)] and [you understand [how to build these systems]] and you can be very competitive.} {Which is â€” that way I think [it makes it [a very good application (for startups)]], right?} {When you look at [reasoning model, video generation and so on], it requires [very large training infrastructure], [very large access to data] and so on.}**

æœ‰è¶£çš„æ˜¯â€”â€”åœ¨è¯­éŸ³é¢†åŸŸï¼Œå°å›¢é˜Ÿå¯ä»¥äº§ç”Ÿå·¨å¤§å½±å“ã€‚å› ä¸ºæ¨¡å‹æ¯”å¤§å‹æ¨ç†æ¨¡å‹å°å¾—å¤šï¼Œå…³é”®åœ¨äºä½ å¯¹è¯­éŸ³çš„ç†è§£æœ‰å¤šæ·±ã€ä½ æœ‰å¤šäº†è§£å¦‚ä½•æ„å»ºè¿™äº›ç³»ç»Ÿï¼Œä½ å°±èƒ½éå¸¸æœ‰ç«äº‰åŠ›ã€‚è¿™ä½¿å¾—è¯­éŸ³æˆä¸ºä¸€ä¸ªéå¸¸é€‚åˆåˆ›ä¸šå…¬å¸çš„åº”ç”¨æ–¹å‘ã€‚è€Œä½ çœ‹æ¨ç†æ¨¡å‹ã€è§†é¢‘ç”Ÿæˆç­‰ç­‰ï¼Œå®ƒä»¬éœ€è¦éå¸¸åºå¤§çš„è®­ç»ƒåŸºç¡€è®¾æ–½å’Œæµ·é‡æ•°æ®ã€‚

è§£æï¼š
* **make a big difference** ğŸ”¥ï¼šäº§ç”Ÿå·¨å¤§å½±å“
* **competitive**ï¼šæœ‰ç«äº‰åŠ›çš„
* **training infrastructure**ï¼šè®­ç»ƒåŸºç¡€è®¾æ–½
* **access to data**ï¼šæ•°æ®è·å–æ¸ é“

---

(13) [5:02-5:27]

**{So it's much more difficult (for a small actor) [to be meaningful].} {In voice there is [a lot of space to take].} {Yeah, that's really cool too because, yeah, I think [as you said] a lot of the labs aren't prioritizing this (compared to other spaces).} {And then there's also [interesting parts (of the speech models)] where like TTS is [a lot cheaper (to train)].} {So that really highlights [why there are [so many TTS models (out there)]] and [why [so many people] have come into [that part of the space]].}**

æ‰€ä»¥å°å›¢é˜Ÿè¦åœ¨é‚£äº›é¢†åŸŸæœ‰æ‰€ä½œä¸ºè¦å›°éš¾å¾—å¤šã€‚è€Œåœ¨è¯­éŸ³é¢†åŸŸæœ‰å¾ˆå¤šç©ºé—´å¯ä»¥å æ®ã€‚â€”â€”ç¡®å®å¾ˆé…·ï¼Œæ­£å¦‚ä½ æ‰€è¯´ï¼Œå¾ˆå¤šå®éªŒå®¤å¹¶æ²¡æœ‰ä¼˜å…ˆå…³æ³¨è¯­éŸ³é¢†åŸŸã€‚è€Œä¸”è¯­éŸ³æ¨¡å‹ä¸­è¿˜æœ‰ä¸€äº›æœ‰è¶£çš„æ–¹é¢ï¼Œæ¯”å¦‚ **TTS** çš„è®­ç»ƒæˆæœ¬ä½å¾—å¤šã€‚è¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆå¸‚é¢ä¸Šæœ‰è¿™ä¹ˆå¤š **TTS** æ¨¡å‹ï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆå¤šäººéƒ½æ¶Œå…¥äº†è¿™ä¸ªæ–¹å‘ã€‚

è§£æï¼š
* **small actor**ï¼šå°ç©å®¶ã€å°å‚ä¸è€…
* **prioritizing**ï¼šä¼˜å…ˆè€ƒè™‘
* **TTS (Text to Speech)**ï¼šæ–‡æœ¬è½¬è¯­éŸ³
* **highlights**ï¼šçªå‡ºã€å¼ºè°ƒ
* **out there** ğŸ”¥ï¼šåœ¨å¤–é¢ã€å¸‚é¢ä¸Šï¼ˆå£è¯­ä¸­æŒ‡å…¬å¼€å­˜åœ¨çš„ï¼‰

---

(14) [5:27-5:52]

**{In terms of â€” like Moshi â€” so when Moshi was released, like what were you â€” maybe give a description of [what the Moshi model was] (for everyone) and then also kind of [where it's come (since then)].} {Yeah.} {So the underlying (let's say) big principle is [audio language models].} {So basically (in audio generation) you have roughly [two families of model].} {One is diffusion models.}**

è¯´åˆ° **Moshi**â€”â€”å½“ **Moshi** å‘å¸ƒçš„æ—¶å€™ï¼Œèƒ½ä¸èƒ½ç»™å¤§å®¶æè¿°ä¸€ä¸‹ **Moshi** æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒåæ¥å‘å±•åˆ°äº†å“ªé‡Œï¼Ÿâ€”â€”å¥½çš„ã€‚åŸºæœ¬åŸç†å°±æ˜¯éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚åœ¨éŸ³é¢‘ç”Ÿæˆé¢†åŸŸï¼Œå¤§è‡´æœ‰ä¸¤å¤§ç±»æ¨¡å‹ã€‚ç¬¬ä¸€ç±»æ˜¯æ‰©æ•£æ¨¡å‹ã€‚

è§£æï¼š
* **in terms of** ğŸ”¥ï¼šå°±â€¦â€¦è€Œè¨€ã€è¯´åˆ°â€¦â€¦
* **underlying principle**ï¼šåŸºæœ¬åŸç†
* **roughly**ï¼šå¤§è‡´ã€å¤§çº¦
* **family of model**ï¼šæ¨¡å‹å®¶æ—/ç±»åˆ«
* **diffusion models**ï¼šæ‰©æ•£æ¨¡å‹

---

(15) [5:52-6:15]

**{And so typically [when you see an audio spectrogram], (you know,) like [a time-frequency representation (of sound)] [where you see [all the frequency bands and the energy]], so you can see it as an image and you can train [the same kind of algorithms [that are used (for image generation)]] but [to generate the spectrogram] and then you're going to reconstruct, (you know,) transform the spectrogram in audio.}**

é€šå¸¸ï¼Œå½“ä½ çœ‹ä¸€ä¸ªéŸ³é¢‘é¢‘è°±å›¾â€”â€”ä¹Ÿå°±æ˜¯å£°éŸ³çš„æ—¶é¢‘è¡¨ç¤ºï¼Œä½ èƒ½çœ‹åˆ°æ‰€æœ‰çš„é¢‘æ®µå’Œèƒ½é‡â€”â€”ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€å¼ å›¾åƒã€‚ç„¶åä½ å¯ä»¥ç”¨ä¸å›¾åƒç”Ÿæˆç›¸åŒçš„ç®—æ³•æ¥ç”Ÿæˆé¢‘è°±å›¾ï¼Œå†æŠŠé¢‘è°±å›¾é‡å»ºã€è½¬æ¢å›éŸ³é¢‘ã€‚

è§£æï¼š
* **spectrogram** /ËˆspektrÉ™É¡rÃ¦m/ï¼šé¢‘è°±å›¾
* **time-frequency representation**ï¼šæ—¶é¢‘è¡¨ç¤º
* **frequency bands**ï¼šé¢‘æ®µ/é¢‘å¸¦
* **reconstruct**ï¼šé‡å»ºã€è¿˜åŸ
* **algorithm** /ËˆÃ¦lÉ¡É™rÉªÃ°É™m/ï¼šç®—æ³•

---

(16) [6:15-6:40]

**{So that's diffusion models (for audio).} {The nice thing is [it can provide (very) high quality] but they do not process audio sequentially.} {So they are going to process [everything (at once)] and so they're incompatible (with real-time inference).} {What we proposed and introduced [(when I was at Google)] and [we have been working on (for now more than five years)] is [an audio language model].} {So it all started with SoundStream.}**

è¿™å°±æ˜¯éŸ³é¢‘é¢†åŸŸçš„æ‰©æ•£æ¨¡å‹ã€‚å¥½å¤„æ˜¯å®ƒèƒ½æä¾›éå¸¸é«˜çš„è´¨é‡ï¼Œä½†å®ƒä»¬ä¸æ˜¯æŒ‰é¡ºåºå¤„ç†éŸ³é¢‘çš„â€”â€”å®ƒä»¬ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å†…å®¹ï¼Œå› æ­¤ä¸å®æ—¶æ¨ç†ä¸å…¼å®¹ã€‚è€Œæˆ‘ä»¬æå‡ºå¹¶å¼•å…¥çš„â€”â€”è¿™æ˜¯æˆ‘åœ¨ **Google** æ—¶å¼€å§‹çš„ï¼Œæˆ‘ä»¬å·²ç»åšäº†äº”å¹´å¤šâ€”â€”æ˜¯éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚ä¸€åˆ‡éƒ½å§‹äº **SoundStream**ã€‚

è§£æï¼š
* **sequentially** /sÉªËˆkwenÊƒÉ™li/ï¼šæŒ‰é¡ºåºåœ°
* **at once**ï¼šä¸€æ¬¡æ€§ã€åŒæ—¶
* **incompatible** ğŸ”¥ï¼šä¸å…¼å®¹çš„
* **real-time inference**ï¼šå®æ—¶æ¨ç†
* **proposed and introduced**ï¼šæå‡ºå¹¶å¼•å…¥

---

(17) [6:40-7:04]

**{So it was a codec [that my team and I developed].} {So the idea was just [to create a new codec (for real-time communication) like (in particular) video conferencing].} {So that will be like [a neural network [that takes an audio and compresses it (so efficiently)] [that (even with a very bad network connectivity) you can have (high quality)]].} {So it was really (for Google Meet) [that we developed that].}**

å®ƒæ˜¯ä¸€ä¸ªç”±æˆ‘å’Œæˆ‘çš„å›¢é˜Ÿå¼€å‘çš„ç¼–è§£ç å™¨ã€‚æœ€åˆçš„æƒ³æ³•å°±æ˜¯ä¸ºå®æ—¶é€šä¿¡åˆ›å»ºä¸€ä¸ªæ–°çš„ç¼–è§£ç å™¨ï¼Œç‰¹åˆ«æ˜¯ç”¨äºè§†é¢‘ä¼šè®®ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œèƒ½æŠŠéŸ³é¢‘å‹ç¼©å¾—éå¸¸é«˜æ•ˆï¼Œå³ä½¿ç½‘ç»œè¿æ¥å¾ˆå·®ä¹Ÿèƒ½ä¿æŒé«˜è´¨é‡ã€‚æˆ‘ä»¬ç¡®å®æ˜¯ä¸º **Google Meet** å¼€å‘çš„ã€‚

è§£æï¼š
* **codec** /ËˆkoÊŠdek/ ğŸ”¥ï¼šç¼–è§£ç å™¨ï¼ˆç¼–ç +è§£ç çš„ç¼©å†™ï¼‰
* **real-time communication**ï¼šå®æ—¶é€šä¿¡
* **video conferencing**ï¼šè§†é¢‘ä¼šè®®
* **neural network**ï¼šç¥ç»ç½‘ç»œ
* **connectivity**ï¼šè¿æ¥æ€§ã€ç½‘ç»œè¿æ¥

---

(18) [7:04-7:29]

**{But then we realized [okay we have this very large audio waveform] and (at the time) everybody was generating audio [with generative adversarial networks].} {So it was great [if you want to generate (like 3 to 5 seconds of) audio].} {But [if you want to generate (one minute of audio)], it just doesn't work (at all).} {It doesn't scale [to this kind of long-form generation].} {And we were seeing (at the same time) [that the transformer architecture (for language modeling) was working (very well)].}**

ä½†åæ¥æˆ‘ä»¬æ„è¯†åˆ°ï¼Œæˆ‘ä»¬æœ‰è¿™ä¹ˆå¤§çš„éŸ³é¢‘æ³¢å½¢ï¼Œè€Œå½“æ—¶æ‰€æœ‰äººéƒ½åœ¨ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥ç”ŸæˆéŸ³é¢‘ã€‚å¦‚æœä½ æƒ³ç”Ÿæˆ 3 åˆ° 5 ç§’çš„éŸ³é¢‘ï¼Œé‚£å¾ˆæ£’ã€‚ä½†å¦‚æœä½ æƒ³ç”Ÿæˆä¸€åˆ†é’Ÿçš„éŸ³é¢‘ï¼Œå®ƒå®Œå…¨ä¸è¡Œâ€”â€”å®ƒæ— æ³•æ‰©å±•åˆ°è¿™ç§é•¿å½¢å¼çš„ç”Ÿæˆã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ° **Transformer** æ¶æ„åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°éå¸¸å¥½ã€‚

è§£æï¼š
* **waveform**ï¼šæ³¢å½¢
* **generative adversarial network (GAN)** ğŸ”¥ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
* **scale** ğŸ”¥ï¼šæ‰©å±•ã€å¯æ‰©å±•ï¼ˆscale to = æ‰©å±•åˆ°ï¼‰
* **long-form generation**ï¼šé•¿å½¢å¼ç”Ÿæˆ
* **transformer architecture**ï¼š**Transformer** æ¶æ„

---

(19) [7:29-7:55]

**{We tried [to adapt that (to our setting)].} {And so what we did is [we looked at our codec and realized] okay, we are compressing audio (so efficiently) that [the resulting codes (of the codec)] they are [a bit like text] â€” they are [very compressed representation].} {And so [what if we could train just [an audio language model] [which will be like [the exact same architecture (as a text language model)] but [instead of predicting (text tokens)] it predicts codes [that are like (audio tokens)]]]?} {And so it took (I don't know) like 5 days [to implement].}**

æˆ‘ä»¬è¯•ç€æŠŠå®ƒé€‚é…åˆ°æˆ‘ä»¬çš„åœºæ™¯ã€‚æˆ‘ä»¬ç ”ç©¶äº†è‡ªå·±çš„ç¼–è§£ç å™¨ï¼Œå‘ç°æˆ‘ä»¬æŠŠéŸ³é¢‘å‹ç¼©å¾—å¦‚æ­¤é«˜æ•ˆï¼Œä»¥è‡³äºç¼–è§£ç å™¨è¾“å‡ºçš„ç¼–ç æœ‰ç‚¹åƒæ–‡æœ¬â€”â€”å®ƒä»¬æ˜¯éå¸¸å‹ç¼©çš„è¡¨ç¤ºã€‚é‚£å¦‚æœæˆ‘ä»¬ç›´æ¥è®­ç»ƒä¸€ä¸ªéŸ³é¢‘è¯­è¨€æ¨¡å‹å‘¢ï¼Ÿå®ƒçš„æ¶æ„è·Ÿæ–‡æœ¬è¯­è¨€æ¨¡å‹å®Œå…¨ä¸€æ ·ï¼Œåªä¸è¿‡ä¸æ˜¯é¢„æµ‹æ–‡æœ¬ **token**ï¼Œè€Œæ˜¯é¢„æµ‹ç±»ä¼¼éŸ³é¢‘ **token** çš„ç¼–ç ã€‚å®ç°è¿™ä¸ªå¤§æ¦‚åªç”¨äº† 5 å¤©ã€‚

è§£æï¼š
* **adapt to** ğŸ”¥ï¼šé€‚é…åˆ°ã€è°ƒæ•´ä»¥é€‚åº”
* **compressed representation**ï¼šå‹ç¼©è¡¨ç¤º
* **token**ï¼šä»¤ç‰Œï¼ˆAIä¸­çš„åŸºæœ¬å¤„ç†å•å…ƒï¼‰
* **implement**ï¼šå®ç°ã€å®æ–½
* **what if** ğŸ”¥ï¼šå¦‚æœâ€¦â€¦ä¼šæ€æ ·ï¼ˆå‡è®¾æ€§æé—®ï¼‰

---

(20) [7:55-8:20]

**{We ran our first experiment and instantly we realized [we had tackled (instant) voice cloning].} {So (for the first time in the world) you would prefix â€” the same way [you prompt a text model] â€” you will prefix your model [with like three seconds (from someone's voice)] and you will keep speaking (in their voice).} {And so this became (extremely) easy (for us) [to then do [pretty much any task]].} {So we did music, then it gave NotebookLM [to generate a dialogue (for podcast)] â€” (you know) it was a very popular product (at Google) â€” translation and so on.}**

æˆ‘ä»¬è¿è¡Œäº†ç¬¬ä¸€ä¸ªå®éªŒï¼Œç«‹åˆ»å°±æ„è¯†åˆ°æˆ‘ä»¬è§£å†³äº†å³æ—¶è¯­éŸ³å…‹éš†çš„é—®é¢˜ã€‚ä¸–ç•Œä¸Šç¬¬ä¸€æ¬¡ï¼Œä½ å¯ä»¥åƒç»™æ–‡æœ¬æ¨¡å‹å†™æç¤ºè¯ä¸€æ ·ï¼Œç”¨æŸäººçš„ä¸‰ç§’é’Ÿè¯­éŸ³ä½œä¸ºå‰ç¼€ï¼Œç„¶åæ¨¡å‹å°±ä¼šç»§ç»­ç”¨é‚£ä¸ªäººçš„å£°éŸ³è¯´è¯ã€‚è¿™ä½¿å¾—æˆ‘ä»¬å‡ ä¹å¯ä»¥è½»æ¾å®Œæˆä»»ä½•ä»»åŠ¡â€”â€”æˆ‘ä»¬åšäº†éŸ³ä¹ç”Ÿæˆï¼Œç„¶åå®ƒå‚¬ç”Ÿäº† **NotebookLM** ç”¨æ¥ç”Ÿæˆæ’­å®¢å¯¹è¯â€”â€”è¿™åœ¨ **Google** æ˜¯ä¸€ä¸ªéå¸¸å—æ¬¢è¿çš„äº§å“â€”â€”è¿˜æœ‰ç¿»è¯‘ç­‰ç­‰ã€‚

è§£æï¼š
* **tackled** ğŸ”¥ï¼šè§£å†³äº†ã€æ”»å…‹äº†
* **voice cloning**ï¼šè¯­éŸ³å…‹éš†
* **prefix**ï¼šå‰ç¼€ï¼ˆè¿™é‡Œç”¨ä½œåŠ¨è¯ï¼Œç»™æ¨¡å‹åŠ å‰ç¼€è¾“å…¥ï¼‰
* **prompt**ï¼šæç¤ºï¼ˆç»™AIæ¨¡å‹çš„è¾“å…¥æŒ‡ä»¤ï¼‰
* **pretty much** ğŸ”¥ï¼šå‡ ä¹ã€å·®ä¸å¤š

---

(21) [8:20-8:50]

**{So it was (you know) just as versatile as [text LLMs], (you know) just pretty much doing [every task (at once)].} {So it was extremely interesting.} {And the very nice thing is [that we could always exploit [what the text community was doing]].} {So for example [when the text community distilled [large text models] (into small text models)], we could distill [large speech models] (into small ones).} {When they did [speculative decoding], we got [speculative decoding].} {When all the papers around [reasoning models] and so on, we were able [to integrate it (in our speech models)].}**

æ‰€ä»¥å®ƒå°±åƒæ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ä¸€æ ·å…¨èƒ½ï¼Œå‡ ä¹èƒ½åŒæ—¶åšæ‰€æœ‰ä»»åŠ¡ã€‚è¿™éå¸¸æœ‰è¶£ã€‚è€Œä¸”æœ€æ£’çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€ç›´å€Ÿé‰´æ–‡æœ¬ç¤¾åŒºåœ¨åšçš„äº‹æƒ…ã€‚æ¯”å¦‚ï¼Œå½“æ–‡æœ¬ç¤¾åŒºæŠŠå¤§å‹æ–‡æœ¬æ¨¡å‹è’¸é¦æˆå°å‹æ–‡æœ¬æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æŠŠå¤§å‹è¯­éŸ³æ¨¡å‹è’¸é¦æˆå°çš„ã€‚å½“ä»–ä»¬åšæ¨æµ‹æ€§è§£ç æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½ç”¨ä¸Šã€‚å½“å…³äºæ¨ç†æ¨¡å‹çš„è®ºæ–‡å‡ºæ¥æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æŠŠå®ƒæ•´åˆåˆ°æˆ‘ä»¬çš„è¯­éŸ³æ¨¡å‹ä¸­ã€‚

è§£æï¼š
* **versatile** /ËˆvÉœËrsÉ™tl/ ğŸ”¥ï¼šå¤šç”¨é€”çš„ã€å…¨èƒ½çš„
* **exploit**ï¼šåˆ©ç”¨ã€å€Ÿé‰´ï¼ˆä¸­æ€§ç”¨æ³•ï¼Œéè´¬ä¹‰ï¼‰
* **distill** ğŸ”¥ï¼šè’¸é¦ï¼ˆAIæœ¯è¯­ï¼Œå°†å¤§æ¨¡å‹çš„çŸ¥è¯†å‹ç¼©åˆ°å°æ¨¡å‹ä¸­ï¼‰
* **speculative decoding** ğŸ”¥ï¼šæ¨æµ‹æ€§è§£ç ï¼ˆä¸€ç§åŠ é€Ÿæ¨ç†çš„æŠ€æœ¯ï¼‰
* **integrate**ï¼šæ•´åˆã€é›†æˆ

---

(22) [8:50-9:16]

**{So it was just like also [you would see the progress (on the hardware) [being so tailored (for the transformer architecture and LLMs)]] [that we could just (you know) benefit from it as well].} {So we are like [riding (for free) the LLM wave] but just with audio.} {So this has been [a very very productive framework].} {And what we did with Moshi is [a very simple idea (conceptually)]: [instead of predicting [a single stream (of audio tokens)] you have two (in parallel)].}**

è€Œä¸”ç¡¬ä»¶æ–¹é¢çš„è¿›æ­¥ä¹Ÿæ˜¯å¦‚æ­¤â€”â€”ç¡¬ä»¶è¶Šæ¥è¶Šä¸º **Transformer** æ¶æ„å’Œå¤§è¯­è¨€æ¨¡å‹é‡èº«å®šåˆ¶ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½ä»ä¸­å—ç›Šã€‚æ‰€ä»¥æˆ‘ä»¬å°±åƒæ˜¯å…è´¹æ­ä¹˜äº†å¤§è¯­è¨€æ¨¡å‹çš„æµªæ½®ï¼Œåªä¸è¿‡æ˜¯ç”¨éŸ³é¢‘è€Œå·²ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é«˜äº§çš„æ¡†æ¶ã€‚æˆ‘ä»¬ç”¨ **Moshi** åšçš„äº‹æƒ…åœ¨æ¦‚å¿µä¸Šéå¸¸ç®€å•ï¼šä¸æ˜¯é¢„æµ‹å•ä¸€çš„éŸ³é¢‘ **token** æµï¼Œè€Œæ˜¯å¹¶è¡Œé¢„æµ‹ä¸¤ä¸ªã€‚

è§£æï¼š
* **tailored for** ğŸ”¥ï¼šä¸ºâ€¦â€¦é‡èº«å®šåˆ¶
* **riding the wave** ğŸ”¥ï¼šæ­ä¹˜æµªæ½®ï¼ˆåˆ©ç”¨è¶‹åŠ¿è·ç›Šï¼‰
* **productive framework**ï¼šé«˜äº§çš„æ¡†æ¶
* **in parallel**ï¼šå¹¶è¡Œåœ°
* **conceptually**ï¼šåœ¨æ¦‚å¿µä¸Š

---

(23) [9:16-9:46]

**{So one is going to be the user and one is going to be the AI.} {And since the two are run (in parallel), [instead of having [a strict delimitation (of speaker turns)] [where either you're speaking or you're listening]], both can speak (at the same time).} {Both can be silent (at the same time).} {You have [no voice activity detection whatsoever].} {Anyone [who has played (in our audience) with (voice activity detection parameters)] knows [how painful it is].} {So it was just [a very simple framework] [that would solve the challenging task] â€” [which (when you're a scientist) is exactly [what you're looking for]].}**

ä¸€ä¸ªä»£è¡¨ç”¨æˆ·ï¼Œä¸€ä¸ªä»£è¡¨ AIã€‚å› ä¸ºä¸¤è€…æ˜¯å¹¶è¡Œè¿è¡Œçš„ï¼Œæ‰€ä»¥ä¸éœ€è¦ä¸¥æ ¼åˆ’åˆ†è¯´è¯è€…çš„è½®æ¬¡â€”â€”ä¸æ˜¯"è¦ä¹ˆä½ åœ¨è¯´è¦ä¹ˆåœ¨å¬"â€”â€”åŒæ–¹å¯ä»¥åŒæ—¶è¯´è¯ï¼Œä¹Ÿå¯ä»¥åŒæ—¶æ²‰é»˜ã€‚ä½ å®Œå…¨ä¸éœ€è¦è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€‚åœ¨åº§ä»»ä½•è°ƒè¿‡è¯­éŸ³æ´»åŠ¨æ£€æµ‹å‚æ•°çš„äººéƒ½çŸ¥é“é‚£æœ‰å¤šç—›è‹¦ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æ¡†æ¶ï¼Œå´èƒ½è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡â€”â€”ä½œä¸ºç§‘å­¦å®¶ï¼Œè¿™æ­£æ˜¯ä½ æƒ³è¦çš„ã€‚

è§£æï¼š
* **delimitation**ï¼šç•Œå®šã€åˆ’åˆ†
* **speaker turns**ï¼šè¯´è¯è€…è½®æ¬¡
* **voice activity detection (VAD)** ğŸ”¥ï¼šè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆæ£€æµ‹ä½•æ—¶æœ‰äººåœ¨è¯´è¯ï¼‰
* **parameters**ï¼šå‚æ•°
* **painful** ğŸ”¥ï¼šç—›è‹¦çš„ï¼ˆå£è¯­ä¸­å¸¸ç”¨æ¥å½¢å®¹éš¾æçš„äº‹ï¼‰

---

(24) [9:46-10:11]

**{Yeah, I think this problem of [turn taking being interruptions or latency] â€” you're really constantly just flip-flopping between [either you're not acknowledging [when the user barges in]], you're not acknowledging interruptions, or you're interrupting the user, or you are causing [too much or too little latency] [so that it's unnatural]].} {And I think [that definitely is the promise (of speech-to-speech models)]: [that you have that full context] and you can model â€” it's just [a question of [how much data you're inputting]] â€” you can model back [that similar speech patterns].}**

æ˜¯çš„ï¼Œè½®æ¬¡åˆ‡æ¢å¸¦æ¥çš„æ‰“æ–­å’Œå»¶è¿Ÿé—®é¢˜â€”â€”ä½ çœŸçš„å°±æ˜¯åœ¨ä¸åœåœ°æ¥å›æ‘‡æ‘†ï¼šè¦ä¹ˆä½ æ²¡æœ‰åœ¨ç”¨æˆ·æ’å˜´çš„æ—¶å€™åšå‡ºå›åº”ï¼Œè¦ä¹ˆä½ æ‰“æ–­äº†ç”¨æˆ·ï¼Œè¦ä¹ˆä½ é€ æˆäº†å¤ªå¤šæˆ–å¤ªå°‘çš„å»¶è¿Ÿå¯¼è‡´ä¸è‡ªç„¶ã€‚è€Œæˆ‘è®¤ä¸ºè¿™å°±æ˜¯ç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹çš„æ„¿æ™¯ï¼šä½ æ‹¥æœ‰å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥å»ºæ¨¡â€”â€”è¿™åªæ˜¯ä½ è¾“å…¥å¤šå°‘æ•°æ®çš„é—®é¢˜â€”â€”ä½ å¯ä»¥å»ºæ¨¡å‡ºç±»ä¼¼çš„è¯­éŸ³æ¨¡å¼ã€‚

è§£æï¼š
* **flip-flopping** ğŸ”¥ï¼šæ¥å›æ‘‡æ‘†ã€åå¤ä¸å®š
* **acknowledge**ï¼šå›åº”ã€ç¡®è®¤
* **barge in** ğŸ”¥ï¼šæ’å˜´ã€é—¯å…¥
* **latency** /ËˆleÉªtÉ™nsi/ ğŸ”¥ï¼šå»¶è¿Ÿï¼ˆç³»ç»Ÿå“åº”æ—¶é—´ï¼‰
* **speech-to-speech models**ï¼šç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹
* **speech patterns**ï¼šè¯­éŸ³æ¨¡å¼/è¯´è¯æ¨¡å¼

---

(25) [10:11-10:37]

**{So I guess [why are speech-to-speech models not everywhere right now]?} {Like this sounds amazing.} {Like you have this supernatural speech pattern.} {You can clone voices.} {You can create like â€” NotebookLM was amazing [because you were able to have [a very podcast style]] or you could have [a more customer service â€” "I'm speaking on the phone" â€” [which sounds different (than a news broadcaster)]].}**

é‚£æˆ‘å°±æƒ³é—®äº†â€”â€”ä¸ºä»€ä¹ˆç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹ç°åœ¨è¿˜æ²¡æœ‰åˆ°å¤„æ™®åŠï¼Ÿå¬èµ·æ¥å¤ªå‰å®³äº†â€”â€”ä½ æœ‰è¿™ç§è¶…è‡ªç„¶çš„è¯­éŸ³æ¨¡å¼ï¼Œä½ å¯ä»¥å…‹éš†å£°éŸ³ã€‚ä½ å¯ä»¥åˆ›é€ å„ç§é£æ ¼â€”â€”**NotebookLM** ä¹‹æ‰€ä»¥æƒŠè‰³ï¼Œæ˜¯å› ä¸ºä½ å¯ä»¥å®ç°éå¸¸æ’­å®¢åŒ–çš„é£æ ¼ï¼Œä¹Ÿå¯ä»¥åšæˆå®¢æœåœ¨æ‰“ç”µè¯çš„æ„Ÿè§‰â€”â€”è€Œè¿™å¬èµ·æ¥è·Ÿæ–°é—»æ’­æŠ¥å‘˜å®Œå…¨ä¸åŒã€‚

è§£æï¼š
* **supernatural**ï¼šè¶…è‡ªç„¶çš„ï¼ˆè¿™é‡ŒæŒ‡éå¸¸é€¼çœŸã€è¶…è¶Šå¸¸è§„çš„ï¼‰
* **clone voices**ï¼šå…‹éš†å£°éŸ³
* **podcast style**ï¼šæ’­å®¢é£æ ¼
* **customer service**ï¼šå®¢æˆ·æœåŠ¡
* **news broadcaster**ï¼šæ–°é—»æ’­æŠ¥å‘˜

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ®µè®¿è°ˆä»‹ç»äº† **Neil** å’Œä»–åˆ›å»ºçš„ä¸¤å®¶ç»„ç»‡ï¼šéè¥åˆ©ç ”ç©¶å®éªŒå®¤ **Qoutai** å’Œå•†ä¸šå…¬å¸ **Gradium**ã€‚æ ¸å¿ƒæŠ€æœ¯æ˜¯éŸ³é¢‘è¯­è¨€æ¨¡å‹â€”â€”ä¸åŒäºæ‰©æ•£æ¨¡å‹ï¼Œå®ƒåŸºäº **SoundStream** ç¼–è§£ç å™¨å°†éŸ³é¢‘å‹ç¼©ä¸ºç±»ä¼¼æ–‡æœ¬çš„ tokenï¼Œç„¶åç”¨ä¸æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ç›¸åŒçš„ **Transformer** æ¶æ„æ¥ç”ŸæˆéŸ³é¢‘ã€‚ä»–ä»¬çš„çªç ´æ€§é¡¹ç›® **Moshi** æ˜¯ä¸€ä¸ªå…¨åŒå·¥å¯¹è¯æ¨¡å‹ï¼Œé€šè¿‡å¹¶è¡Œé¢„æµ‹ç”¨æˆ·å’Œ AI ä¸¤ä¸ªéŸ³é¢‘æµï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿè¯­éŸ³ç³»ç»Ÿä¸­çš„è½®æ¬¡åˆ‡æ¢å’Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹é—®é¢˜ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **co-founder** | è”åˆåˆ›å§‹äºº |
| **breakthrough innovation** | çªç ´æ€§åˆ›æ–° |
| **first principles** | ç¬¬ä¸€æ€§åŸç† |
| **state of the art** | æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ |
| **cascaded system** | çº§è”ç³»ç»Ÿ |
| **full duplex** | å…¨åŒå·¥ï¼ˆåŒå‘åŒæ—¶é€šä¿¡ï¼‰ |
| **codec** | ç¼–è§£ç å™¨ |
| **spectrogram** | é¢‘è°±å›¾ |
| **GAN (Generative Adversarial Network)** | ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ |
| **distill** | è’¸é¦ï¼ˆæ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼‰ |
| **speculative decoding** | æ¨æµ‹æ€§è§£ç  |
| **voice activity detection (VAD)** | è¯­éŸ³æ´»åŠ¨æ£€æµ‹ |
| **latency** | å»¶è¿Ÿ |
| **market traction** | å¸‚åœºå¸å¼•åŠ›/éœ€æ±‚ |
| **riding the wave** | æ­ä¹˜æµªæ½® |
| **flip-flopping** | æ¥å›æ‘‡æ‘† |
| **barge in** | æ’å˜´ã€é—¯å…¥ |
| **scale** | æ‰©å±•ã€å¯æ‰©å±• |
| **tailored for** | ä¸ºâ€¦â€¦é‡èº«å®šåˆ¶ |
| **versatile** | å¤šç”¨é€”çš„ã€å…¨èƒ½çš„ |
