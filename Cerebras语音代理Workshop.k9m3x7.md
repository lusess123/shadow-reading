# ğŸ¯ Cerebras è¯­éŸ³é”€å”®ä»£ç† Workshop è‹±è¯­æ®µè½ç¿»è¯‘

æœ¬æ–‡å…± **20 ä¸ªè¯­ä¹‰å•å…ƒ**ï¼Œå°†å…¨éƒ¨ç¿»è¯‘ã€‚

---

(1) [0:21-0:40] **Hi everyone, we're about to start the next session. Thank you guys so much for coming out today. Um, this is going to be a build your own sales agent workshop. So, we're going to be walking through everything you need to know to build your own voice agent. My name is Sarah Chang from Cerebras and I am excited to be joined by Genway. Um, and we are both part of the DevX team at Cerebras.**

å¤§å®¶å¥½ï¼Œæˆ‘ä»¬é©¬ä¸Šè¦å¼€å§‹ä¸‹ä¸€åœº session äº†ã€‚éå¸¸æ„Ÿè°¢å¤§å®¶ä»Šå¤©çš„åˆ°æ¥ã€‚è¿™å°†æ˜¯ä¸€ä¸ªã€Œæ„å»ºä½ è‡ªå·±çš„é”€å”®ä»£ç†ã€çš„ workshopã€‚æˆ‘ä»¬ä¼šå¸¦å¤§å®¶äº†è§£æ„å»ºè¯­éŸ³ä»£ç†æ‰€éœ€çš„ä¸€åˆ‡çŸ¥è¯†ã€‚æˆ‘æ˜¯æ¥è‡ª **Cerebras** çš„ **Sarah Chang**ï¼Œå¾ˆé«˜å…´ **Genway** ä¹ŸåŠ å…¥äº†æˆ‘ä»¬ã€‚æˆ‘ä»¬éƒ½æ˜¯ **Cerebras** çš„ **DevX** å›¢é˜Ÿæˆå‘˜ã€‚

è§£æï¼š
* **be about to**ï¼šçŸ­è¯­ï¼Œå³å°†ã€æ­£è¦åšæŸäº‹
* **coming out**ï¼šçŸ­è¯­ï¼Œå‡ºå¸­ã€åˆ°åœºï¼ˆå£è¯­è¡¨è¾¾ï¼‰
* **walk through** ğŸ”¥ï¼šçŸ­è¯­ï¼Œé€æ­¥è®²è§£ã€å¸¦é¢†äº†è§£
* **DevX**ï¼šDeveloper Experience çš„ç¼©å†™ï¼Œå¼€å‘è€…ä½“éªŒå›¢é˜Ÿ

---

(2) [0:41-1:15] **Yeah, thanks Sarah. Um, so today we're going to walk through how to build a voice sales agent that can actually have natural conversations with customers and our sales agents will pull product contacts from an external source to respond in real time. So, we're going to be building an AI agent that can speak, listen, and respond intelligently um to your company's sales materials. And we have the full code for you to follow along with. We have a notebook that you can scan later um to step ghost and we'll walk you through it step by step in just a moment.**

è°¢è°¢ **Sarah**ã€‚ä»Šå¤©æˆ‘ä»¬è¦è®²çš„æ˜¯å¦‚ä½•æ„å»ºä¸€ä¸ªè¯­éŸ³é”€å”®ä»£ç†ï¼Œå®ƒèƒ½å¤Ÿä¸å®¢æˆ·è¿›è¡ŒçœŸæ­£è‡ªç„¶çš„å¯¹è¯ï¼Œæˆ‘ä»¬çš„é”€å”®ä»£ç†ä¼šä»å¤–éƒ¨æ•°æ®æºæ‹‰å–äº§å“ä¿¡æ¯æ¥å®æ—¶å“åº”ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ„å»ºçš„æ˜¯ä¸€ä¸ªèƒ½è¯´ã€èƒ½å¬ã€èƒ½æ™ºèƒ½å“åº”çš„ AI ä»£ç†ï¼Œå®ƒå¯ä»¥åˆ©ç”¨ä½ å…¬å¸çš„é”€å”®èµ„æ–™ã€‚æˆ‘ä»¬æœ‰å®Œæ•´çš„ä»£ç ä¾›å¤§å®¶è·Ÿç€åšï¼Œæœ‰ä¸€ä¸ª notebook å¯ä»¥ç¨åæ‰«ç è·å–ï¼Œæˆ‘ä»¬é©¬ä¸Šä¼šä¸€æ­¥ä¸€æ­¥å¸¦å¤§å®¶è¿‡ä¸€éã€‚

è§£æï¼š
* **pull from**ï¼šçŸ­è¯­ï¼Œä»...æ‹‰å–/è·å–ï¼ˆæ•°æ®ï¼‰
* **in real time** ğŸ”¥ï¼šçŸ­è¯­ï¼Œå®æ—¶åœ°
* **follow along with**ï¼šçŸ­è¯­ï¼Œè·Ÿç€ä¸€èµ·åš
* **step by step**ï¼šçŸ­è¯­ï¼Œä¸€æ­¥ä¸€æ­¥åœ°

---

(3) [1:18-1:41] **So, before we get started, let's go through what you will get out of this workshop. So you will get free API credits for Cerebras livekit cartisia. You will have the quick start. We'll have again have a full code notebook for you to follow along with and at the end you will have your very own sales agent that you can hook up to your company's materials so that you can you know implement this in production.**

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè®²è®²è¿™æ¬¡ workshop ä½ èƒ½æ”¶è·ä»€ä¹ˆã€‚ä½ ä¼šè·å¾— **Cerebras**ã€**LiveKit**ã€**Cartesia** çš„å…è´¹ API é¢åº¦ï¼Œä¼šæœ‰ä¸€ä¸ªå¿«é€Ÿå…¥é—¨æŒ‡å—ï¼Œè¿˜æœ‰å®Œæ•´çš„ä»£ç  notebook ä¾›ä½ è·Ÿç€åšã€‚æœ€åä½ ä¼šæ‹¥æœ‰ä¸€ä¸ªå±äºä½ è‡ªå·±çš„é”€å”®ä»£ç†ï¼Œå¯ä»¥æ¥å…¥ä½ å…¬å¸çš„èµ„æ–™ï¼Œç„¶åéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚

è§£æï¼š
* **get out of** ğŸ”¥ï¼šçŸ­è¯­ï¼Œä»...ä¸­è·å¾—ï¼ˆæ”¶ç›Š/ä»·å€¼ï¼‰
* **API credits**ï¼šAPI è°ƒç”¨é¢åº¦/ç§¯åˆ†
* **hook up to**ï¼šçŸ­è¯­ï¼Œè¿æ¥åˆ°ã€æ¥å…¥
* **implement in production**ï¼šéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

---

(4) [1:41-2:03] **So here's the starter code that I would recommend scanning just so you can follow along. Um, again, this is what we'll be walking through step by step today. And there will be individual modules that you'll be able to just run and see some good outputs. So, I'll give you a few seconds for that. We'll have the QR code later as well, so not to worry.**

è¿™æ˜¯æˆ‘æ¨èå¤§å®¶æ‰«ç è·å–çš„ starter codeï¼Œè¿™æ ·ä½ å¯ä»¥è·Ÿç€ä¸€èµ·åšã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©ä¼šä¸€æ­¥æ­¥è®²è§£çš„å†…å®¹ï¼Œä¼šæœ‰ä¸€äº›ç‹¬ç«‹çš„æ¨¡å—ï¼Œä½ å¯ä»¥ç›´æ¥è¿è¡Œå¹¶çœ‹åˆ°æ•ˆæœã€‚æˆ‘ç»™å¤§å®¶å‡ ç§’é’Ÿæ‰«ç ã€‚äºŒç»´ç ç¨åè¿˜ä¼šå†å‡ºç°ï¼Œæ‰€ä»¥ä¸ç”¨æ‹…å¿ƒã€‚

è§£æï¼š
* **starter code**ï¼šèµ·æ­¥ä»£ç ã€åˆå§‹ä»£ç æ¨¡æ¿
* **individual modules**ï¼šç‹¬ç«‹æ¨¡å—
* **not to worry**ï¼šå£è¯­è¡¨è¾¾ï¼Œä¸ç”¨æ‹…å¿ƒ

---

(5) [2:03-2:39] **So, before we get started, I wanted to talk a little bit about Cerebras and, you know, Cerebras inference's secret sauce. So, for those of you who are unfamiliar, we are a hardware company. We are building an AI processor that is much larger and much faster than what you are probably familiar with with Nvidia GPUs. So out of curiosity, I'm wondering how many people here have heard about Cerebras hardware. Not bad. Okay. Higher than last year.**

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æƒ³å…ˆèŠèŠ **Cerebras** ä»¥åŠ **Cerebras** æ¨ç†çš„ç§˜å¯†æ­¦å™¨ã€‚å¯¹äºä¸ç†Ÿæ‚‰çš„æœ‹å‹ï¼Œæˆ‘ä»¬æ˜¯ä¸€å®¶ç¡¬ä»¶å…¬å¸ï¼Œæˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€æ¬¾ AI å¤„ç†å™¨ï¼Œå®ƒæ¯”ä½ å¯èƒ½ç†Ÿæ‚‰çš„ **Nvidia GPU** è¦å¤§å¾—å¤šã€å¿«å¾—å¤šã€‚å‡ºäºå¥½å¥‡ï¼Œæˆ‘æƒ³é—®é—®åœ¨åº§æœ‰å¤šå°‘äººå¬è¯´è¿‡ **Cerebras** çš„ç¡¬ä»¶ï¼Ÿè¿˜ä¸é”™å˜›ã€‚æ¯”å»å¹´å¤šäº†ã€‚

è§£æï¼š
* **secret sauce** ğŸ”¥ï¼šåè¯çŸ­è¯­ï¼Œç§˜å¯†æ­¦å™¨ã€ç‹¬é—¨ç»æŠ€ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰
* **for those of you who**ï¼šå¯¹äºä½ ä»¬å½“ä¸­...çš„äºº
* **out of curiosity**ï¼šå‡ºäºå¥½å¥‡

---

(6) [2:33-3:12] **Okay. So before we do go, I want to share um I want to show everyone the speed of what we're talking about here. So this is just a chat. It's running on Cerebras. You can choose any. So, we can host any different model on our hardware. So, I'm going to choose an example model like a llama model. And I'm going to give it a prompt. So, I'm going to give it a prompt that it's intentionally asking it to respond something a little longer. This go funny dad jokes, but make each joke a couple sentences. And that's how fast it generates.**

å¥½çš„ï¼Œåœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘æƒ³ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹æˆ‘ä»¬è¯´çš„é€Ÿåº¦åˆ°åº•æ˜¯ä»€ä¹ˆæ ·çš„ã€‚è¿™åªæ˜¯ä¸€ä¸ªèŠå¤©ç•Œé¢ï¼Œè¿è¡Œåœ¨ **Cerebras** ä¸Šã€‚ä½ å¯ä»¥é€‰æ‹©ä»»ä½•æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç¡¬ä»¶å¯ä»¥æ‰˜ç®¡å„ç§ä¸åŒçš„æ¨¡å‹ã€‚æˆ‘æ¥é€‰ä¸€ä¸ªç¤ºä¾‹æ¨¡å‹ï¼Œæ¯”å¦‚ **Llama** æ¨¡å‹ã€‚ç„¶åç»™å®ƒä¸€ä¸ª promptï¼Œæˆ‘ä¼šæ•…æ„è®©å®ƒç”Ÿæˆé•¿ä¸€ç‚¹çš„å†…å®¹ã€‚ã€Œè®²å‡ ä¸ªæç¬‘çš„çˆ¸çˆ¸ç¬‘è¯ï¼Œæ¯ä¸ªç¬‘è¯å†™å‡ å¥è¯ã€ã€‚çœ‹ï¼Œè¿™å°±æ˜¯å®ƒç”Ÿæˆçš„é€Ÿåº¦ã€‚

è§£æï¼š
* **host**ï¼šåŠ¨è¯ï¼Œæ‰˜ç®¡ã€è¿è¡Œï¼ˆæ¨¡å‹/æœåŠ¡ï¼‰
* **prompt**ï¼šåè¯ï¼Œæç¤ºè¯ï¼ˆAI æœ¯è¯­ï¼‰
* **intentionally**ï¼šå‰¯è¯ï¼Œæ•…æ„åœ°
* **dad jokes**ï¼šçˆ¸çˆ¸ç¬‘è¯ï¼ˆä¸€ç§å†·å¹½é»˜é£æ ¼çš„ç¬‘è¯ï¼‰

---

(7) [3:12-3:51] **Does anyone else have a prompt you want to try? A longer prompt. Amazing. There you go. So, really quickly before we get started, I know we have a lot of software geeks here, but I do want to for a second talk about hardware. And I want to talk a little bit about what hardware innovations um make such fast inference possible especially as we build a new generation of AI products.**

æœ‰æ²¡æœ‰äººæƒ³è¯•è¯•å…¶ä»– promptï¼Ÿé•¿ä¸€ç‚¹çš„ï¼Ÿå¤ªæ£’äº†ï¼Œå°±æ˜¯è¿™æ ·ã€‚å¥½çš„ï¼Œåœ¨æ­£å¼å¼€å§‹ä¹‹å‰ï¼Œæˆ‘çŸ¥é“åœ¨åº§æœ‰å¾ˆå¤šè½¯ä»¶æå®¢ï¼Œä½†æˆ‘æƒ³èŠ±ä¸€ç‚¹æ—¶é—´èŠèŠç¡¬ä»¶ã€‚æˆ‘æƒ³è®²è®²æ˜¯ä»€ä¹ˆæ ·çš„ç¡¬ä»¶åˆ›æ–°è®©å¦‚æ­¤å¿«é€Ÿçš„æ¨ç†æˆä¸ºå¯èƒ½ï¼Œå°¤å…¶æ˜¯å½“æˆ‘ä»¬åœ¨æ„å»ºæ–°ä¸€ä»£ AI äº§å“çš„æ—¶å€™ã€‚

è§£æï¼š
* **geek** ğŸ”¥ï¼šåè¯ï¼Œæå®¢ã€æŠ€æœ¯çˆ±å¥½è€…
* **for a second**ï¼šçŸ­è¯­ï¼ŒçŸ­æš‚åœ°ã€ä¸€ä¼šå„¿
* **innovation**ï¼šåè¯ï¼Œåˆ›æ–°
* **inference**ï¼šåè¯ï¼Œæ¨ç†ï¼ˆAI æœ¯è¯­ï¼ŒæŒ‡æ¨¡å‹è¿è¡Œç”Ÿæˆè¾“å‡ºçš„è¿‡ç¨‹ï¼‰

---

(8) [3:54-4:14] **And so we're going to a little bit of a hardware segment, but one of the main secret sauces for Cerebras is that Cerebras chips do not have memory bandwidth issues. And I don't know how familiar you guys are with, you know, GPU architecture, but we're actually gonna deep dive really quickly into how GPU architecture works and how it compares to what people are doing today.**

æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªç®€çŸ­çš„ç¡¬ä»¶ç¯èŠ‚ã€‚**Cerebras** çš„ä¸€ä¸ªæ ¸å¿ƒç§˜å¯†æ­¦å™¨æ˜¯ï¼Œ**Cerebras** çš„èŠ¯ç‰‡æ²¡æœ‰å†…å­˜å¸¦å®½é—®é¢˜ã€‚æˆ‘ä¸çŸ¥é“å¤§å®¶å¯¹ **GPU** æ¶æ„æœ‰å¤šç†Ÿæ‚‰ï¼Œä½†æˆ‘ä»¬ä¼šå¿«é€Ÿæ·±å…¥è®²è§£ä¸€ä¸‹ **GPU** æ¶æ„æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå®ƒä¸å½“å‰ä¸šç•Œåšæ³•çš„å¯¹æ¯”ã€‚

è§£æï¼š
* **segment**ï¼šåè¯ï¼Œæ®µè½ã€ç¯èŠ‚
* **memory bandwidth**ï¼šå†…å­˜å¸¦å®½ï¼ˆç¡¬ä»¶æœ¯è¯­ï¼‰
* **deep dive** ğŸ”¥ï¼šçŸ­è¯­ï¼Œæ·±å…¥æ¢è®¨
* **architecture**ï¼šåè¯ï¼Œæ¶æ„

---

(9) [4:17-4:43] **And so for context, this is the hardware that, you know, all of our inference runs on. It's the wafer scale engine 3. It is quite literally the size of a dinner plate. And this has 4 trillion transistors, 900,000 cores, and very significant amounts of onchip memory. And so this is the comparison of what our hardware looks like next to the NVIDIA GPU. So you can see some of those metrics line up. So significantly more transistors.**

ä½œä¸ºèƒŒæ™¯ä»‹ç»ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ‰€æœ‰æ¨ç†è¿è¡Œçš„ç¡¬ä»¶ï¼Œå«åš **Wafer Scale Engine 3**ï¼ˆæ™¶åœ†çº§å¼•æ“ 3ï¼‰ã€‚å®ƒçœŸçš„æœ‰ä¸€ä¸ªé¤ç›˜é‚£ä¹ˆå¤§ã€‚å®ƒæœ‰ 4 ä¸‡äº¿ä¸ªæ™¶ä½“ç®¡ã€90 ä¸‡ä¸ªæ ¸å¿ƒï¼Œä»¥åŠéå¸¸å¤§é‡çš„ç‰‡ä¸Šå†…å­˜ã€‚è¿™æ˜¯æˆ‘ä»¬çš„ç¡¬ä»¶ä¸ **Nvidia GPU** çš„å¯¹æ¯”å›¾ï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸€äº›æŒ‡æ ‡çš„å¯¹æ¯”ï¼Œæ™¶ä½“ç®¡æ•°é‡æ˜æ˜¾å¤šå¾ˆå¤šã€‚

è§£æï¼š
* **for context**ï¼šçŸ­è¯­ï¼Œä½œä¸ºèƒŒæ™¯/ä¸Šä¸‹æ–‡
* **wafer scale**ï¼šæ™¶åœ†çº§ï¼ˆåŠå¯¼ä½“æœ¯è¯­ï¼‰
* **quite literally**ï¼šçŸ­è¯­ï¼Œç¡®å®ã€çœŸçš„ï¼ˆå­—é¢æ„ä¹‰ä¸Šçš„ï¼‰
* **transistor**ï¼šåè¯ï¼Œæ™¶ä½“ç®¡
* **onchip memory**ï¼šç‰‡ä¸Šå†…å­˜
* **metrics line up**ï¼šæŒ‡æ ‡å¯¹é½/å¯¹æ¯”

---

(10) [4:45-5:25] **But to actually understand what Cerebras did with their hardware that makes it able to achieve 20x 30x 70x faster speeds than inference on Nvidia GPUs. We're going to actually start by taking a look at the Nvidia GPU. So this is a diagram of an H100. And if you look at the red rectangle, that is a core. And so on the H100 there's about 17,000 cores and each of these cores is what is actually doing all of the mathematical computations needed in training or inference or whatever computation you need to do. So every core has a subset of the computations um that is assigned.**

ä½†è¦çœŸæ­£ç†è§£ **Cerebras** åœ¨ç¡¬ä»¶ä¸Šåšäº†ä»€ä¹ˆï¼Œä½¿å…¶èƒ½å¤Ÿæ¯” **Nvidia GPU** æ¨ç†å¿« 20 å€ã€30 å€ç”šè‡³ 70 å€ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ **Nvidia GPU**ã€‚è¿™æ˜¯ä¸€å¼  **H100** çš„ç¤ºæ„å›¾ã€‚çœ‹é‚£ä¸ªçº¢è‰²çŸ©å½¢ï¼Œé‚£å°±æ˜¯ä¸€ä¸ªæ ¸å¿ƒã€‚**H100** å¤§çº¦æœ‰ 17,000 ä¸ªæ ¸å¿ƒï¼Œæ¯ä¸ªæ ¸å¿ƒéƒ½åœ¨æ‰§è¡Œè®­ç»ƒæˆ–æ¨ç†æˆ–å…¶ä»–è®¡ç®—æ‰€éœ€çš„æ•°å­¦è¿ç®—ã€‚æ¯ä¸ªæ ¸å¿ƒè¢«åˆ†é…äº†ä¸€éƒ¨åˆ†è®¡ç®—ä»»åŠ¡ã€‚

è§£æï¼š
* **diagram**ï¼šåè¯ï¼Œå›¾è¡¨ã€ç¤ºæ„å›¾
* **core**ï¼šåè¯ï¼Œæ ¸å¿ƒï¼ˆCPU/GPU çš„è®¡ç®—å•å…ƒï¼‰
* **mathematical computations**ï¼šæ•°å­¦è®¡ç®—
* **a subset of**ï¼š...çš„å­é›†/ä¸€éƒ¨åˆ†
* **assigned**ï¼šåŠ¨è¯è¿‡å»åˆ†è¯ï¼Œè¢«åˆ†é…çš„

---

(11) [5:29-6:09] **So when you run inference what are some of the types of things that a core will need access to to do its computation? It needs its weight, activations, KV cache, etc. On the H100, all of these values are stored offchip. So, they're stored in an offchip memory. And so, as you can imagine, during inference, each of these cores, there's thousands of computations happening constantly. And each core is needing to constantly load and offload the KV cache, activation, weights, etc. from an off-memory location. And as you can imagine this creates a very significant memory bandwidth bottleneck.**

é‚£ä¹ˆå½“ä½ è¿è¡Œæ¨ç†æ—¶ï¼Œä¸€ä¸ªæ ¸å¿ƒéœ€è¦è®¿é—®å“ªäº›ä¸œè¥¿æ¥å®Œæˆè®¡ç®—å‘¢ï¼Ÿå®ƒéœ€è¦æƒé‡ã€æ¿€æ´»å€¼ã€**KV cache** ç­‰ç­‰ã€‚åœ¨ **H100** ä¸Šï¼Œæ‰€æœ‰è¿™äº›å€¼éƒ½å­˜å‚¨åœ¨ç‰‡å¤–ã€‚ä¹Ÿå°±æ˜¯å­˜å‚¨åœ¨ç‰‡å¤–å†…å­˜ä¸­ã€‚ä½ å¯ä»¥æƒ³è±¡ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ ¸å¿ƒéƒ½åœ¨ä¸æ–­è¿›è¡Œæˆåƒä¸Šä¸‡æ¬¡è®¡ç®—ï¼Œæ¯ä¸ªæ ¸å¿ƒéœ€è¦ä¸æ–­åœ°ä»ç‰‡å¤–å†…å­˜ä½ç½®åŠ è½½å’Œå¸è½½ **KV cache**ã€æ¿€æ´»å€¼ã€æƒé‡ç­‰ã€‚å¯ä»¥æƒ³è±¡ï¼Œè¿™ä¼šé€ æˆéå¸¸ä¸¥é‡çš„å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚

è§£æï¼š
* **weight**ï¼šæƒé‡ï¼ˆç¥ç»ç½‘ç»œå‚æ•°ï¼‰
* **activation**ï¼šæ¿€æ´»å€¼ï¼ˆç¥ç»ç½‘ç»œæœ¯è¯­ï¼‰
* **KV cache**ï¼šKey-Value ç¼“å­˜ï¼ˆTransformer æ¨ç†ä¼˜åŒ–æœ¯è¯­ï¼‰
* **offchip**ï¼šç‰‡å¤–çš„ï¼ˆä¸ onchip ç‰‡ä¸Šç›¸å¯¹ï¼‰
* **load and offload**ï¼šåŠ è½½å’Œå¸è½½
* **bottleneck** ğŸ”¥ï¼šåè¯ï¼Œç“¶é¢ˆ

---

(12) [6:11-6:49] **What Cerebras has done instead is that instead of storing all these values offchip every single core on the Cerebras hardware the WSC3 there's 900,000 cores which in comparison to 17,000 is already a lot larger. Um every single core has direct its own direct onchip memory. So its own SRAM. So every single core on this wafer has a memory right next to it. And what that means is that all of the values that every single core needs for computations like weights, KV cache, etc. is directly accessible and much faster to accessible and it's right there.**

**Cerebras** çš„åšæ³•ä¸åŒã€‚å®ƒä¸æ˜¯æŠŠæ‰€æœ‰è¿™äº›å€¼å­˜å‚¨åœ¨ç‰‡å¤–ï¼Œè€Œæ˜¯åœ¨ **Cerebras** ç¡¬ä»¶ï¼ˆ**WSC3**ï¼‰ä¸Šçš„æ¯ä¸€ä¸ªæ ¸å¿ƒâ€”â€”å…±æœ‰ 90 ä¸‡ä¸ªæ ¸å¿ƒï¼Œç›¸æ¯” 17,000 å·²ç»å¤šå¾ˆå¤šäº†â€”â€”æ¯ä¸ªæ ¸å¿ƒéƒ½æœ‰è‡ªå·±ä¸“å±çš„ç‰‡ä¸Šå†…å­˜ï¼Œä¹Ÿå°±æ˜¯è‡ªå·±çš„ **SRAM**ã€‚æ‰€ä»¥è¿™ä¸ªæ™¶åœ†ä¸Šçš„æ¯ä¸€ä¸ªæ ¸å¿ƒæ—è¾¹éƒ½æœ‰ä¸€å—å†…å­˜ã€‚è¿™æ„å‘³ç€æ¯ä¸ªæ ¸å¿ƒè®¡ç®—æ‰€éœ€çš„æ‰€æœ‰å€¼ï¼Œæ¯”å¦‚æƒé‡ã€**KV cache** ç­‰ï¼Œéƒ½å¯ä»¥ç›´æ¥è®¿é—®ï¼Œè®¿é—®é€Ÿåº¦å¿«å¾—å¤šï¼Œå°±åœ¨é‚£é‡Œã€‚

è§£æï¼š
* **instead of**ï¼šè€Œä¸æ˜¯
* **in comparison to**ï¼šä¸...ç›¸æ¯”
* **SRAM**ï¼šStatic Random Access Memoryï¼Œé™æ€éšæœºå­˜å–å­˜å‚¨å™¨
* **directly accessible**ï¼šå¯ç›´æ¥è®¿é—®çš„
* **right next to**ï¼šç´§æŒ¨ç€

---

(13) [6:50-7:22] **And so as you the other and so that's a little bit that's one example of what Cerebras has done on the hardware side. Um, but going back to software, I also want to talk about really quickly one thing that Cerebras implements on the software side to accelerate inference. And so one way that you can accelerate inference is through a technique called spec um standard decode or speculative decoding. So in standard decoding you have one model generate every single token one at a time. And this is sequential, right? You have to wait for the previous token to be generated to generate the next token.**

è¿™å°±æ˜¯ **Cerebras** åœ¨ç¡¬ä»¶æ–¹é¢åšçš„ä¸€ä¸ªä¾‹å­ã€‚ä½†å›åˆ°è½¯ä»¶å±‚é¢ï¼Œæˆ‘ä¹Ÿæƒ³å¿«é€Ÿè®²ä¸€ä¸‹ **Cerebras** åœ¨è½¯ä»¶æ–¹é¢ç”¨æ¥åŠ é€Ÿæ¨ç†çš„ä¸€ä¸ªæŠ€æœ¯ã€‚åŠ é€Ÿæ¨ç†çš„ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡ä¸€ç§å«åšæ¨æµ‹è§£ç ï¼ˆ**speculative decoding**ï¼‰çš„æŠ€æœ¯ã€‚åœ¨æ ‡å‡†è§£ç ä¸­ï¼Œä½ è®©ä¸€ä¸ªæ¨¡å‹ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ª tokenï¼Œè¿™æ˜¯é¡ºåºæ‰§è¡Œçš„ï¼Œå¯¹å§ï¼Ÿä½ å¿…é¡»ç­‰å‰ä¸€ä¸ª token ç”Ÿæˆå®Œæ‰èƒ½ç”Ÿæˆä¸‹ä¸€ä¸ª tokenã€‚

è§£æï¼š
* **on the hardware/software side**ï¼šåœ¨ç¡¬ä»¶/è½¯ä»¶æ–¹é¢
* **accelerate**ï¼šåŠ¨è¯ï¼ŒåŠ é€Ÿ
* **speculative decoding** ğŸ”¥ï¼šæ¨æµ‹è§£ç ï¼ˆä¸€ç§æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼‰
* **token**ï¼štoken/æ ‡è®°ï¼ˆNLP æœ¯è¯­ï¼Œæ–‡æœ¬çš„æœ€å°å¤„ç†å•å…ƒï¼‰
* **sequential**ï¼šå½¢å®¹è¯ï¼Œé¡ºåºçš„ã€åºåˆ—çš„

---

(14) [7:22-8:06] **So in speculative decoding, you combine two models. And what you're doing is you use a smaller model that's like a draft model that can generate all of the tokens very quickly. And then you use your larger model to go back and verify that the output of the smaller model is correct. And by combining these two models, you're able to get the speed of the smaller model and the accuracy of the larger model. And if you think about it, your speed is capped by this uh your like this the speed um is capped by the speed of the larger model. So you will up to the large like the speed will be up to the larger model um but it will never go beyond it. So it will only ever be faster.**

åœ¨æ¨æµ‹è§£ç ä¸­ï¼Œä½ ä¼šç»„åˆä¸¤ä¸ªæ¨¡å‹ã€‚ä½ ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ä½œä¸ºè‰ç¨¿æ¨¡å‹ï¼Œå®ƒå¯ä»¥éå¸¸å¿«é€Ÿåœ°ç”Ÿæˆæ‰€æœ‰ tokenã€‚ç„¶åä½¿ç”¨è¾ƒå¤§çš„æ¨¡å‹å›è¿‡å¤´æ¥éªŒè¯å°æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦æ­£ç¡®ã€‚é€šè¿‡ç»„åˆè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œä½ å¯ä»¥è·å¾—å°æ¨¡å‹çš„é€Ÿåº¦å’Œå¤§æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œä½ çš„é€Ÿåº¦ä¸Šé™æ˜¯ç”±å¤§æ¨¡å‹å†³å®šçš„ï¼Œé€Ÿåº¦æœ€å¤šåªèƒ½è¾¾åˆ°å¤§æ¨¡å‹çš„é€Ÿåº¦ï¼Œä¸ä¼šè¶…è¿‡å®ƒï¼Œåªä¼šæ›´å¿«ã€‚

è§£æï¼š
* **draft model**ï¼šè‰ç¨¿æ¨¡å‹
* **verify**ï¼šåŠ¨è¯ï¼ŒéªŒè¯
* **accuracy**ï¼šåè¯ï¼Œå‡†ç¡®æ€§
* **be capped by** ğŸ”¥ï¼šè¢«...é™åˆ¶/å°é¡¶
* **go beyond**ï¼šè¶…è¶Š

---

(15) [8:10-8:25] **So as a kind of a short recap, hardware, memory, bandwidth, we talked through that software, speculative decoding, but that was a little side moment and I want to go and now back to the workshop. Now that you have all the context that you need. Awesome job.**

ç®€å•å›é¡¾ä¸€ä¸‹ï¼šç¡¬ä»¶æ–¹é¢è®²äº†å†…å­˜å¸¦å®½ï¼Œè½¯ä»¶æ–¹é¢è®²äº†æ¨æµ‹è§£ç ã€‚è¿™æ˜¯ä¸€ä¸ªå°æ’æ›²ï¼Œç°åœ¨è®©æˆ‘ä»¬å›åˆ° workshopã€‚ä½ ä»¬å·²ç»æœ‰äº†æ‰€éœ€çš„æ‰€æœ‰èƒŒæ™¯çŸ¥è¯†äº†ã€‚å¤ªæ£’äº†ï¼

è§£æï¼š
* **recap** ğŸ”¥ï¼šåè¯/åŠ¨è¯ï¼Œå›é¡¾ã€æ€»ç»“
* **side moment**ï¼šå°æ’æ›²ã€é¢˜å¤–è¯
* **context**ï¼šåè¯ï¼ŒèƒŒæ™¯ã€ä¸Šä¸‹æ–‡

---

(16) [8:26-8:56] **Yeah, thanks Sarah. Um, for those who folks who join in late, you guys can scan the QR code to get the starter code. We had it in the early slide, but um since we'll be teaching you guys how to build these sales agents, you can follow along with our code. Um yeah, so I think in the future, most customer interactions will probably be AI powered, but you know, instead of just typing back and forth with the chatbot, what the best way to kind of really have these customer interactions is really through real conversations, which is why voice agents are so powerful.**

è°¢è°¢ **Sarah**ã€‚å¯¹äºè¿Ÿåˆ°çš„æœ‹å‹ï¼Œä½ ä»¬å¯ä»¥æ‰«äºŒç»´ç è·å– starter codeï¼Œæˆ‘ä»¬ä¹‹å‰çš„å¹»ç¯ç‰‡ä¸Šæœ‰ã€‚æ—¢ç„¶æˆ‘ä»¬è¦æ•™å¤§å®¶å¦‚ä½•æ„å»ºè¿™äº›é”€å”®ä»£ç†ï¼Œä½ å¯ä»¥è·Ÿç€æˆ‘ä»¬çš„ä»£ç ä¸€èµ·åšã€‚æˆ‘è®¤ä¸ºåœ¨æœªæ¥ï¼Œå¤§å¤šæ•°å®¢æˆ·äº¤äº’å¯èƒ½éƒ½ä¼šç”± AI é©±åŠ¨ã€‚ä½†ä¸å…¶åªæ˜¯å’ŒèŠå¤©æœºå™¨äººæ¥å›æ‰“å­—ï¼ŒçœŸæ­£è¿›è¡Œå®¢æˆ·äº¤äº’çš„æœ€ä½³æ–¹å¼å…¶å®æ˜¯é€šè¿‡çœŸå®çš„å¯¹è¯ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¯­éŸ³ä»£ç†å¦‚æ­¤å¼ºå¤§ã€‚

è§£æï¼š
* **join in late**ï¼šè¿Ÿåˆ°åŠ å…¥
* **AI powered**ï¼šAI é©±åŠ¨çš„
* **typing back and forth**ï¼šæ¥å›æ‰“å­—
* **chatbot**ï¼šèŠå¤©æœºå™¨äºº

---

(17) [9:00-9:31] **So before we dive deep into it, what exactly is a voice agent? Absolutely. Um so voice agents are stateful intelligent systems that can simultaneously run inference while constantly listening to you when you're speaking and they can actually engage in real and very natural conversations. Um I would like to highlight four key uh capabilities. First, they understand and respond to spoken language. um they don't just spit out answers based on string matching or keywords but rather they can actually understand the meaning behind what people are saying.**

é‚£ä¹ˆåœ¨æ·±å…¥ä¹‹å‰ï¼Œä»€ä¹ˆæ˜¯è¯­éŸ³ä»£ç†ï¼Ÿå¥½é—®é¢˜ã€‚è¯­éŸ³ä»£ç†æ˜¯æœ‰çŠ¶æ€çš„æ™ºèƒ½ç³»ç»Ÿï¼Œå¯ä»¥åœ¨ä½ è¯´è¯æ—¶åŒæ—¶è¿è¡Œæ¨ç†å¹¶æŒç»­ç›‘å¬ä½ çš„å£°éŸ³ï¼Œå®ƒä»¬èƒ½å¤Ÿè¿›è¡ŒçœŸå®ä¸”éå¸¸è‡ªç„¶çš„å¯¹è¯ã€‚æˆ‘æƒ³å¼ºè°ƒå››ä¸ªå…³é”®èƒ½åŠ›ã€‚é¦–å…ˆï¼Œå®ƒä»¬èƒ½ç†è§£å¹¶å“åº”å£è¯­ã€‚å®ƒä»¬ä¸æ˜¯ç®€å•åœ°åŸºäºå­—ç¬¦ä¸²åŒ¹é…æˆ–å…³é”®è¯æ¥è¾“å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯èƒ½çœŸæ­£ç†è§£äººä»¬è¯è¯­èƒŒåçš„å«ä¹‰ã€‚

è§£æï¼š
* **dive deep into** ğŸ”¥ï¼šæ·±å…¥æ¢è®¨
* **stateful**ï¼šå½¢å®¹è¯ï¼Œæœ‰çŠ¶æ€çš„ï¼ˆç¼–ç¨‹æœ¯è¯­ï¼‰
* **simultaneously**ï¼šå‰¯è¯ï¼ŒåŒæ—¶åœ°
* **engage in**ï¼šå‚ä¸ã€è¿›è¡Œ
* **spit out**ï¼šçŸ­è¯­ï¼Œï¼ˆå¿«é€Ÿï¼‰è¾“å‡ºã€åå‡º
* **string matching**ï¼šå­—ç¬¦ä¸²åŒ¹é…

---

(18) [9:34-9:49] **Um this also means that they can handle a lot of complex tasks. So someone might ask like I'm looking for a product recommendation and the agent can subsequently kind of look into the user's purchase history, the shop's current stock levels and recommend something that they actually like.**

è¿™ä¹Ÿæ„å‘³ç€å®ƒä»¬å¯ä»¥å¤„ç†å¾ˆå¤šå¤æ‚ä»»åŠ¡ã€‚æ¯”å¦‚æœ‰äººå¯èƒ½ä¼šè¯´ã€Œæˆ‘æƒ³è¦äº§å“æ¨èã€ï¼Œä»£ç†å¯ä»¥éšåæŸ¥çœ‹ç”¨æˆ·çš„è´­ä¹°å†å²ã€å•†åº—å½“å‰çš„åº“å­˜æ°´å¹³ï¼Œç„¶åæ¨èä»–ä»¬çœŸæ­£å–œæ¬¢çš„ä¸œè¥¿ã€‚

è§£æï¼š
* **handle**ï¼šåŠ¨è¯ï¼Œå¤„ç†
* **subsequently**ï¼šå‰¯è¯ï¼Œéšåã€æ¥ä¸‹æ¥
* **purchase history**ï¼šè´­ä¹°å†å²
* **stock levels**ï¼šåº“å­˜æ°´å¹³

---

(19) [9:49-10:06] **And you actually might see this referred in some places called multi-agent or workflows. Um speech is obviously the fastest way to communicate your intent in any system. We're speaking now I guess but you can just say what you want. There's like no typing, no clicking through menus and no learning curves.**

ä½ å¯èƒ½åœ¨ä¸€äº›åœ°æ–¹çœ‹åˆ°è¿™è¢«ç§°ä¸ºå¤šä»£ç†ï¼ˆ**multi-agent**ï¼‰æˆ–å·¥ä½œæµï¼ˆ**workflows**ï¼‰ã€‚è¯­éŸ³æ˜¾ç„¶æ˜¯åœ¨ä»»ä½•ç³»ç»Ÿä¸­ä¼ è¾¾æ„å›¾çš„æœ€å¿«æ–¹å¼ã€‚æˆ‘ä»¬ç°åœ¨å°±åœ¨è¯´è¯ï¼Œä½ åªéœ€è¦è¯´å‡ºä½ æƒ³è¦çš„ï¼Œä¸ç”¨æ‰“å­—ã€ä¸ç”¨ç‚¹å‡»èœå•ã€æ²¡æœ‰å­¦ä¹ æ›²çº¿ã€‚

è§£æï¼š
* **multi-agent**ï¼šå¤šä»£ç†ï¼ˆAI ç³»ç»Ÿæ¶æ„æœ¯è¯­ï¼‰
* **workflows**ï¼šå·¥ä½œæµ
* **intent**ï¼šåè¯ï¼Œæ„å›¾
* **learning curves**ï¼šå­¦ä¹ æ›²çº¿

---

(20) [10:06-10:13] **And lastly um none of this would be possible unless the agent can keep track of the state of the conversation.**

æœ€åï¼Œé™¤éä»£ç†èƒ½å¤Ÿè·Ÿè¸ªå¯¹è¯çš„çŠ¶æ€ï¼Œå¦åˆ™è¿™ä¸€åˆ‡éƒ½ä¸å¯èƒ½å®ç°ã€‚

è§£æï¼š
* **lastly**ï¼šå‰¯è¯ï¼Œæœ€å
* **keep track of** ğŸ”¥ï¼šè·Ÿè¸ªã€è®°å½•
* **state of the conversation**ï¼šå¯¹è¯çŠ¶æ€

---

## ğŸ“š æ®µè½å°ç»“

è¿™æ˜¯ **Cerebras** å…¬å¸ä¸¾åŠçš„ä¸€ä¸ªè¯­éŸ³é”€å”®ä»£ç† workshop çš„å¼€åœºéƒ¨åˆ†ã€‚æ¼”è®²è€… **Sarah** å’Œ **Genway** é¦–å…ˆä»‹ç»äº† workshop çš„ç›®æ ‡â€”â€”å¸®åŠ©å‚ä¸è€…æ„å»ºè‡ªå·±çš„è¯­éŸ³é”€å”®ä»£ç†ã€‚æ¥ç€è¯¦ç»†è®²è§£äº† **Cerebras** ç¡¬ä»¶çš„æ ¸å¿ƒä¼˜åŠ¿ï¼šé€šè¿‡ç‰‡ä¸Šå†…å­˜æ¶ˆé™¤å†…å­˜å¸¦å®½ç“¶é¢ˆï¼Œä»¥åŠè½¯ä»¶å±‚é¢çš„æ¨æµ‹è§£ç æŠ€æœ¯æ¥åŠ é€Ÿæ¨ç†ã€‚æœ€åå¼•å…¥äº†è¯­éŸ³ä»£ç†çš„æ¦‚å¿µï¼Œå¼ºè°ƒå…¶æœ‰çŠ¶æ€ã€èƒ½ç†è§£è¯­ä¹‰ã€å¤„ç†å¤æ‚ä»»åŠ¡ç­‰å…³é”®èƒ½åŠ›ã€‚

### ğŸ”¥ æ ¸å¿ƒè¯æ±‡è¡¨

| è¯æ±‡/çŸ­è¯­ | å«ä¹‰ |
|---------|------|
| **walk through** | é€æ­¥è®²è§£ã€å¸¦é¢†äº†è§£ |
| **secret sauce** | ç§˜å¯†æ­¦å™¨ã€ç‹¬é—¨ç»æŠ€ |
| **in real time** | å®æ—¶åœ° |
| **deep dive** | æ·±å…¥æ¢è®¨ |
| **bottleneck** | ç“¶é¢ˆ |
| **speculative decoding** | æ¨æµ‹è§£ç ï¼ˆæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼‰ |
| **be capped by** | è¢«...é™åˆ¶/å°é¡¶ |
| **stateful** | æœ‰çŠ¶æ€çš„ |
| **keep track of** | è·Ÿè¸ªã€è®°å½• |
| **inference** | æ¨ç†ï¼ˆAI æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„è¿‡ç¨‹ï¼‰ |
| **onchip/offchip memory** | ç‰‡ä¸Š/ç‰‡å¤–å†…å­˜ |
| **feel free to** | å°½ç®¡...ï¼ˆç¤¼è²Œè¡¨è¾¾ï¼‰ |
